{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[download this notebook here](https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/5_train_preference_comparisons.ipynb)\n",
    "# We want to compare pairwise comparison with groupwise comparison\n",
    "\n",
    "We will use synthetic feedback based on the true reward function of the reacher environment to evaluate and compare pairwise comparison and pairwise group comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up the preference comparisons algorithm, we first need to set up a lot of its internals beforehand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from imitation.algorithms import preference_comparisons\n",
    "from imitation.rewards.reward_nets import BasicRewardNet, RewardEnsemble\n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.util.util import make_vec_env\n",
    "from imitation.policies.base import FeedForward32Policy, NormalizeFeaturesExtractor\n",
    "from imitation.regularization.regularizers import LpRegularizer\n",
    "from imitation.regularization.updaters import IntervalParamScaler\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "import numpy as np\n",
    "from imitation.util import logger\n",
    "import stable_baselines3.common.logger as sb_logger\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "def intantiate_and_train(pairwise):\n",
    "    venv = make_vec_env(\"Reacher-v4\", rng=rng, render_mode='rgb_array', n_envs=8)\n",
    "\n",
    "    reward_net_members = [BasicRewardNet(venv.observation_space, venv.action_space, normalize_input_layer=RunningNorm) for _ in range(5)]\n",
    "    reward_net = RewardEnsemble(venv.observation_space, venv.action_space, reward_net_members)\n",
    "\n",
    "    preference_model = preference_comparisons.PreferenceModel(reward_net)\n",
    "    # reward_trainer = preference_comparisons.BasicRewardTrainer(\n",
    "    #     preference_model=preference_model,\n",
    "    #     loss=preference_comparisons.CrossEntropyRewardLoss(),\n",
    "    #     epochs=3,\n",
    "    #     rng=rng,\n",
    "    # )\n",
    "\n",
    "\n",
    "    # Create a lambda updater\n",
    "    scaling_factor = 0.1\n",
    "    tolerable_interval = (0.9, 1.1) \n",
    "    lambda_updater = IntervalParamScaler(scaling_factor, tolerable_interval)\n",
    "    # Create a RegularizerFactory\n",
    "    regularizer_factory = LpRegularizer.create(initial_lambda=0.1, lambda_updater=lambda_updater, p=2, val_split=0.1)\n",
    "\n",
    "    reward_trainer = preference_comparisons.EnsembleTrainer(\n",
    "        preference_model,\n",
    "        loss=preference_comparisons.CrossEntropyRewardLoss(),\n",
    "        rng=rng,\n",
    "        epochs=5,\n",
    "        batch_size = 4,\n",
    "        minibatch_size = 2,\n",
    "        # lr: float = 1e-3,\n",
    "        # custom_logger: Optional[imit_logger.HierarchicalLogger] = None,\n",
    "        regularizer_factory = regularizer_factory,\n",
    "    )\n",
    "    if pairwise:\n",
    "        base_fragmenter = preference_comparisons.RandomFragmenter(\n",
    "            warning_threshold=0,\n",
    "            rng=rng,\n",
    "        )\n",
    "        fragmenter = preference_comparisons.ActiveSelectionFragmenter(\n",
    "                preference_model,\n",
    "                base_fragmenter,\n",
    "                2.0,\n",
    "        )\n",
    "        gatherer = preference_comparisons.SyntheticGatherer(rng=rng)\n",
    "    else:\n",
    "        fragmenter = preference_comparisons.AbsoluteUncertaintyFragmenter(\n",
    "            preference_model,\n",
    "            2.0,\n",
    "            rng=rng,\n",
    "        )\n",
    "        gatherer = preference_comparisons.SyntheticGathererForGroupComparisons(rng=rng)\n",
    "    # Several hyperparameters (reward_epochs, ppo_clip_range, ppo_ent_coef,\n",
    "    # ppo_gae_lambda, ppo_n_epochs, discount_factor, use_sde, sde_sample_freq,\n",
    "    # ppo_lr, exploration_frac, num_iterations, initial_comparison_frac,\n",
    "    # initial_epoch_multiplier, query_schedule) used in this example have been\n",
    "    # approximately fine-tuned to reach a reasonable level of performance.\n",
    "    agent = PPO(\n",
    "        policy=FeedForward32Policy,\n",
    "        policy_kwargs=dict(\n",
    "            features_extractor_class=NormalizeFeaturesExtractor,\n",
    "            features_extractor_kwargs=dict(normalize_class=RunningNorm),\n",
    "        ),\n",
    "        env=venv,\n",
    "        seed=0,\n",
    "        n_steps=2048 // venv.num_envs,\n",
    "        batch_size=64,\n",
    "        ent_coef=0.01,\n",
    "        learning_rate=2e-3,\n",
    "        clip_range=0.1,\n",
    "        gae_lambda=0.95,\n",
    "        gamma=0.97,\n",
    "        n_epochs=10,\n",
    "        tensorboard_log=\"tensorboard_logs/\",\n",
    "    )\n",
    "\n",
    "    trajectory_generator = preference_comparisons.AgentTrainer(\n",
    "        algorithm=agent,\n",
    "        reward_fn=reward_net,\n",
    "        venv=venv,\n",
    "        rng=rng,\n",
    "        exploration_frac=0.05,\n",
    "    )\n",
    "\n",
    "    default_logger = sb_logger.Logger(folder='/logs', output_formats='stdout,log,csv,tensorboard')\n",
    "    custom_logger = logger.HierarchicalLogger(default_logger=default_logger)\n",
    "\n",
    "    pref_comparisons = preference_comparisons.PreferenceComparisons(\n",
    "        trajectory_generator,\n",
    "        reward_net,\n",
    "        num_iterations=5,  # Set to 60 for better performance\n",
    "        fragmenter=fragmenter,\n",
    "        preference_gatherer=gatherer,\n",
    "        reward_trainer=reward_trainer,\n",
    "        fragment_length=50,\n",
    "        transition_oversampling=1,\n",
    "        initial_comparison_frac=0.1,\n",
    "        allow_variable_horizon=False,\n",
    "        initial_epoch_multiplier=4,\n",
    "        query_schedule=\"hyperbolic\",\n",
    "        custom_logger=custom_logger,\n",
    "    )\n",
    "\n",
    "    return pref_comparisons.train(\n",
    "        total_timesteps=100_000,\n",
    "        total_comparisons=1000,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classical pairwise comparison (baseline):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pairwise_comparison_result = intantiate_and_train(True)    \n",
    "\n",
    "print(pairwise_comparison_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pairwise Group comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query schedule: [50, 127, 102, 85, 73, 63]\n",
      "Collecting 100 fragments (5000 transitions)\n",
      "Requested 4750 transitions but only 0 in buffer. Sampling 4750 additional transitions.\n",
      "Sampling 250 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 50 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09fc6081fdf84eedb671b4395044531c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "---------------------------------------------------\n",
      "| raw/                                 |          |\n",
      "|    agent/rollout/ep_len_mean         | 50       |\n",
      "|    agent/rollout/ep_rew_mean         | -60.2    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -26.4    |\n",
      "|    agent/time/fps                    | 1081     |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 1        |\n",
      "|    agent/time/total_timesteps        | 2048     |\n",
      "---------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -60.8        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -17.2        |\n",
      "|    agent/time/fps                    | 905          |\n",
      "|    agent/time/iterations             | 2            |\n",
      "|    agent/time/time_elapsed           | 4            |\n",
      "|    agent/time/total_timesteps        | 4096         |\n",
      "|    agent/train/approx_kl             | 0.0035414556 |\n",
      "|    agent/train/clip_fraction         | 0.173        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.85        |\n",
      "|    agent/train/explained_variance    | -0.18        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.0165       |\n",
      "|    agent/train/n_updates             | 10           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0056      |\n",
      "|    agent/train/std                   | 1            |\n",
      "|    agent/train/value_loss            | 0.227        |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61.3        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -9.21        |\n",
      "|    agent/time/fps                    | 857          |\n",
      "|    agent/time/iterations             | 3            |\n",
      "|    agent/time/time_elapsed           | 7            |\n",
      "|    agent/time/total_timesteps        | 6144         |\n",
      "|    agent/train/approx_kl             | 0.0054649087 |\n",
      "|    agent/train/clip_fraction         | 0.23         |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.87        |\n",
      "|    agent/train/explained_variance    | 0.255        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.0131       |\n",
      "|    agent/train/n_updates             | 20           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0118      |\n",
      "|    agent/train/std                   | 1.02         |\n",
      "|    agent/train/value_loss            | 0.142        |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61.4        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.23        |\n",
      "|    agent/time/fps                    | 836          |\n",
      "|    agent/time/iterations             | 4            |\n",
      "|    agent/time/time_elapsed           | 9            |\n",
      "|    agent/time/total_timesteps        | 8192         |\n",
      "|    agent/train/approx_kl             | 0.0051557478 |\n",
      "|    agent/train/clip_fraction         | 0.222        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.88        |\n",
      "|    agent/train/explained_variance    | 0.625        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0147      |\n",
      "|    agent/train/n_updates             | 30           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0115      |\n",
      "|    agent/train/std                   | 1.02         |\n",
      "|    agent/train/value_loss            | 0.0941       |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61.1        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.06        |\n",
      "|    agent/time/fps                    | 823          |\n",
      "|    agent/time/iterations             | 5            |\n",
      "|    agent/time/time_elapsed           | 12           |\n",
      "|    agent/time/total_timesteps        | 10240        |\n",
      "|    agent/train/approx_kl             | 0.0067371675 |\n",
      "|    agent/train/clip_fraction         | 0.24         |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.85        |\n",
      "|    agent/train/explained_variance    | 0.645        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.00762     |\n",
      "|    agent/train/n_updates             | 40           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0134      |\n",
      "|    agent/train/std                   | 1.01         |\n",
      "|    agent/train/value_loss            | 0.0825       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                   |          |\n",
      "|    agent/rollout/ep_len_mean            | 50       |\n",
      "|    agent/rollout/ep_rew_mean            | -61      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean    | -12.6    |\n",
      "|    agent/time/fps                       | 900      |\n",
      "|    agent/time/iterations                | 3        |\n",
      "|    agent/time/time_elapsed              | 6.6      |\n",
      "|    agent/time/total_timesteps           | 6.14e+03 |\n",
      "|    agent/train/approx_kl                | 0.00564  |\n",
      "|    agent/train/clip_fraction            | 0.23     |\n",
      "|    agent/train/clip_range               | 0.1      |\n",
      "|    agent/train/entropy_loss             | -2.85    |\n",
      "|    agent/train/explained_variance       | 0.408    |\n",
      "|    agent/train/learning_rate            | 0.002    |\n",
      "|    agent/train/loss                     | -0.00424 |\n",
      "|    agent/train/n_updates                | 30       |\n",
      "|    agent/train/policy_gradient_loss     | -0.0111  |\n",
      "|    agent/train/std                      | 1.01     |\n",
      "|    agent/train/value_loss               | 0.122    |\n",
      "|    preferences/entropy                  | 0.095    |\n",
      "|    reward/epoch-0/train/accuracy        | 0.622    |\n",
      "|    reward/epoch-0/train/gt_reward_loss  | 0.0586   |\n",
      "|    reward/epoch-0/train/loss            | 0.746    |\n",
      "|    reward/epoch-1/train/accuracy        | 0.693    |\n",
      "|    reward/epoch-1/train/gt_reward_loss  | 0.0535   |\n",
      "|    reward/epoch-1/train/loss            | 0.601    |\n",
      "|    reward/epoch-10/train/accuracy       | 0.866    |\n",
      "|    reward/epoch-10/train/gt_reward_loss | 0.0582   |\n",
      "|    reward/epoch-10/train/loss           | 0.398    |\n",
      "|    reward/epoch-11/train/accuracy       | 0.854    |\n",
      "|    reward/epoch-11/train/gt_reward_loss | 0.0649   |\n",
      "|    reward/epoch-11/train/loss           | 0.388    |\n",
      "|    reward/epoch-2/train/accuracy        | 0.717    |\n",
      "|    reward/epoch-2/train/gt_reward_loss  | 0.0527   |\n",
      "|    reward/epoch-2/train/loss            | 0.509    |\n",
      "|    reward/epoch-3/train/accuracy        | 0.693    |\n",
      "|    reward/epoch-3/train/gt_reward_loss  | 0.0546   |\n",
      "|    reward/epoch-3/train/loss            | 0.466    |\n",
      "|    reward/epoch-4/train/accuracy        | 0.752    |\n",
      "|    reward/epoch-4/train/gt_reward_loss  | 0.0551   |\n",
      "|    reward/epoch-4/train/loss            | 0.479    |\n",
      "|    reward/epoch-5/train/accuracy        | 0.835    |\n",
      "|    reward/epoch-5/train/gt_reward_loss  | 0.051    |\n",
      "|    reward/epoch-5/train/loss            | 0.441    |\n",
      "|    reward/epoch-6/train/accuracy        | 0.851    |\n",
      "|    reward/epoch-6/train/gt_reward_loss  | 0.065    |\n",
      "|    reward/epoch-6/train/loss            | 0.415    |\n",
      "|    reward/epoch-7/train/accuracy        | 0.839    |\n",
      "|    reward/epoch-7/train/gt_reward_loss  | 0.0655   |\n",
      "|    reward/epoch-7/train/loss            | 0.434    |\n",
      "|    reward/epoch-8/train/accuracy        | 0.835    |\n",
      "|    reward/epoch-8/train/gt_reward_loss  | 0.051    |\n",
      "|    reward/epoch-8/train/loss            | 0.389    |\n",
      "|    reward/epoch-9/train/accuracy        | 0.814    |\n",
      "|    reward/epoch-9/train/gt_reward_loss  | 0.0614   |\n",
      "|    reward/epoch-9/train/loss            | 0.422    |\n",
      "| reward/                                 |          |\n",
      "|    final/train/accuracy                 | 0.854    |\n",
      "|    final/train/gt_reward_loss           | 0.0649   |\n",
      "|    final/train/loss                     | 0.388    |\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting 254 fragments (12700 transitions)\n",
      "Requested 12065 transitions but only 10200 in buffer. Sampling 1865 additional transitions.\n",
      "Sampling 635 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 180 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b69a0d972334abb907d3b5c10bd2580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -60.7        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -4.38        |\n",
      "|    agent/time/fps                    | 1103         |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 1            |\n",
      "|    agent/time/total_timesteps        | 12288        |\n",
      "|    agent/train/approx_kl             | 0.0073185386 |\n",
      "|    agent/train/clip_fraction         | 0.284        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.83        |\n",
      "|    agent/train/explained_variance    | 0.694        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0284      |\n",
      "|    agent/train/n_updates             | 50           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0131      |\n",
      "|    agent/train/std                   | 0.989        |\n",
      "|    agent/train/value_loss            | 0.0647       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.2       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -4.24       |\n",
      "|    agent/time/fps                    | 903         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 4           |\n",
      "|    agent/time/total_timesteps        | 14336       |\n",
      "|    agent/train/approx_kl             | 0.007421621 |\n",
      "|    agent/train/clip_fraction         | 0.313       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.79       |\n",
      "|    agent/train/explained_variance    | 0.693       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0193     |\n",
      "|    agent/train/n_updates             | 60          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0152     |\n",
      "|    agent/train/std                   | 0.973       |\n",
      "|    agent/train/value_loss            | 0.0556      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.3       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -4.36       |\n",
      "|    agent/time/fps                    | 855         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 7           |\n",
      "|    agent/time/total_timesteps        | 16384       |\n",
      "|    agent/train/approx_kl             | 0.008447725 |\n",
      "|    agent/train/clip_fraction         | 0.305       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.78       |\n",
      "|    agent/train/explained_variance    | 0.758       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0325     |\n",
      "|    agent/train/n_updates             | 70          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0131     |\n",
      "|    agent/train/std                   | 0.973       |\n",
      "|    agent/train/value_loss            | 0.0442      |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61.4        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -4.28        |\n",
      "|    agent/time/fps                    | 835          |\n",
      "|    agent/time/iterations             | 4            |\n",
      "|    agent/time/time_elapsed           | 9            |\n",
      "|    agent/time/total_timesteps        | 18432        |\n",
      "|    agent/train/approx_kl             | 0.0076092146 |\n",
      "|    agent/train/clip_fraction         | 0.323        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.76        |\n",
      "|    agent/train/explained_variance    | 0.764        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0393      |\n",
      "|    agent/train/n_updates             | 80           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0145      |\n",
      "|    agent/train/std                   | 0.957        |\n",
      "|    agent/train/value_loss            | 0.0442       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -60.9       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -4.16       |\n",
      "|    agent/time/fps                    | 822         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 12          |\n",
      "|    agent/time/total_timesteps        | 20480       |\n",
      "|    agent/train/approx_kl             | 0.008634226 |\n",
      "|    agent/train/clip_fraction         | 0.305       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.72       |\n",
      "|    agent/train/explained_variance    | 0.789       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0309     |\n",
      "|    agent/train/n_updates             | 90          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0137     |\n",
      "|    agent/train/std                   | 0.944       |\n",
      "|    agent/train/value_loss            | 0.0396      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -61.1    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -4.28    |\n",
      "|    agent/time/fps                      | 904      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 6.6      |\n",
      "|    agent/time/total_timesteps          | 1.64e+04 |\n",
      "|    agent/train/approx_kl               | 0.00818  |\n",
      "|    agent/train/clip_fraction           | 0.31     |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.75    |\n",
      "|    agent/train/explained_variance      | 0.759    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0285  |\n",
      "|    agent/train/n_updates               | 80       |\n",
      "|    agent/train/policy_gradient_loss    | -0.0138  |\n",
      "|    agent/train/std                     | 0.956    |\n",
      "|    agent/train/value_loss              | 0.042    |\n",
      "|    preferences/entropy                 | 0.198    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.652    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.142    |\n",
      "|    reward/epoch-0/train/loss           | 0.67     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.675    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.144    |\n",
      "|    reward/epoch-1/train/loss           | 0.608    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.653    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.15     |\n",
      "|    reward/epoch-2/train/loss           | 0.589    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.653    |\n",
      "|    final/train/gt_reward_loss          | 0.15     |\n",
      "|    final/train/loss                    | 0.589    |\n",
      "-----------------------------------------------------\n",
      "Collecting 204 fragments (10200 transitions)\n",
      "Sampling 510 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 285 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e7f35b3f9e54554b570ddee5b21e600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -60         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.77       |\n",
      "|    agent/time/fps                    | 1102        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 1           |\n",
      "|    agent/time/total_timesteps        | 22528       |\n",
      "|    agent/train/approx_kl             | 0.008780796 |\n",
      "|    agent/train/clip_fraction         | 0.305       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.71       |\n",
      "|    agent/train/explained_variance    | 0.793       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0203     |\n",
      "|    agent/train/n_updates             | 100         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0127     |\n",
      "|    agent/train/std                   | 0.933       |\n",
      "|    agent/train/value_loss            | 0.0264      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -59.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.88       |\n",
      "|    agent/time/fps                    | 903         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 4           |\n",
      "|    agent/time/total_timesteps        | 24576       |\n",
      "|    agent/train/approx_kl             | 0.007207038 |\n",
      "|    agent/train/clip_fraction         | 0.295       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.65       |\n",
      "|    agent/train/explained_variance    | 0.587       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0102     |\n",
      "|    agent/train/n_updates             | 110         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00974    |\n",
      "|    agent/train/std                   | 0.913       |\n",
      "|    agent/train/value_loss            | 0.025       |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -59.8        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -4.16        |\n",
      "|    agent/time/fps                    | 853          |\n",
      "|    agent/time/iterations             | 3            |\n",
      "|    agent/time/time_elapsed           | 7            |\n",
      "|    agent/time/total_timesteps        | 26624        |\n",
      "|    agent/train/approx_kl             | 0.0083042085 |\n",
      "|    agent/train/clip_fraction         | 0.31         |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.64        |\n",
      "|    agent/train/explained_variance    | 0.725        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0286      |\n",
      "|    agent/train/n_updates             | 120          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0104      |\n",
      "|    agent/train/std                   | 0.902        |\n",
      "|    agent/train/value_loss            | 0.021        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -58.9       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -4.18       |\n",
      "|    agent/time/fps                    | 829         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 9           |\n",
      "|    agent/time/total_timesteps        | 28672       |\n",
      "|    agent/train/approx_kl             | 0.011023119 |\n",
      "|    agent/train/clip_fraction         | 0.327       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.58       |\n",
      "|    agent/train/explained_variance    | 0.775       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0189     |\n",
      "|    agent/train/n_updates             | 130         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0137     |\n",
      "|    agent/train/std                   | 0.874       |\n",
      "|    agent/train/value_loss            | 0.0182      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -59.1       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.9        |\n",
      "|    agent/time/fps                    | 815         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 12          |\n",
      "|    agent/time/total_timesteps        | 30720       |\n",
      "|    agent/train/approx_kl             | 0.009869597 |\n",
      "|    agent/train/clip_fraction         | 0.326       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.53       |\n",
      "|    agent/train/explained_variance    | 0.749       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0369     |\n",
      "|    agent/train/n_updates             | 140         |\n",
      "|    agent/train/policy_gradient_loss  | -0.011      |\n",
      "|    agent/train/std                   | 0.859       |\n",
      "|    agent/train/value_loss            | 0.0182      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -59.5    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -3.98    |\n",
      "|    agent/time/fps                      | 900      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 6.6      |\n",
      "|    agent/time/total_timesteps          | 2.66e+04 |\n",
      "|    agent/train/approx_kl               | 0.00917  |\n",
      "|    agent/train/clip_fraction           | 0.318    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.58    |\n",
      "|    agent/train/explained_variance      | 0.734    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0232  |\n",
      "|    agent/train/n_updates               | 130      |\n",
      "|    agent/train/policy_gradient_loss    | -0.0114  |\n",
      "|    agent/train/std                     | 0.879    |\n",
      "|    agent/train/value_loss              | 0.0203   |\n",
      "|    preferences/entropy                 | 0.188    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.625    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.163    |\n",
      "|    reward/epoch-0/train/loss           | 0.672    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.679    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.161    |\n",
      "|    reward/epoch-1/train/loss           | 0.601    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.73     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.162    |\n",
      "|    reward/epoch-2/train/loss           | 0.557    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.73     |\n",
      "|    final/train/gt_reward_loss          | 0.162    |\n",
      "|    final/train/loss                    | 0.557    |\n",
      "-----------------------------------------------------\n",
      "Collecting 170 fragments (8500 transitions)\n",
      "Sampling 425 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 370 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e325d82f2fc4ff8aff83062c8324f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -58.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.75       |\n",
      "|    agent/time/fps                    | 1098        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 1           |\n",
      "|    agent/time/total_timesteps        | 32768       |\n",
      "|    agent/train/approx_kl             | 0.009421687 |\n",
      "|    agent/train/clip_fraction         | 0.33        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.52       |\n",
      "|    agent/train/explained_variance    | 0.833       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0213     |\n",
      "|    agent/train/n_updates             | 150         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0119     |\n",
      "|    agent/train/std                   | 0.848       |\n",
      "|    agent/train/value_loss            | 0.019       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -58.5       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.89       |\n",
      "|    agent/time/fps                    | 913         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 4           |\n",
      "|    agent/time/total_timesteps        | 34816       |\n",
      "|    agent/train/approx_kl             | 0.008673655 |\n",
      "|    agent/train/clip_fraction         | 0.305       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.47       |\n",
      "|    agent/train/explained_variance    | 0.875       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0128     |\n",
      "|    agent/train/n_updates             | 160         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00966    |\n",
      "|    agent/train/std                   | 0.829       |\n",
      "|    agent/train/value_loss            | 0.0119      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -58.2       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.88       |\n",
      "|    agent/time/fps                    | 862         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 7           |\n",
      "|    agent/time/total_timesteps        | 36864       |\n",
      "|    agent/train/approx_kl             | 0.008224179 |\n",
      "|    agent/train/clip_fraction         | 0.312       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.44       |\n",
      "|    agent/train/explained_variance    | 0.881       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.035      |\n",
      "|    agent/train/n_updates             | 170         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00976    |\n",
      "|    agent/train/std                   | 0.819       |\n",
      "|    agent/train/value_loss            | 0.0164      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -57.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.75       |\n",
      "|    agent/time/fps                    | 840         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 9           |\n",
      "|    agent/time/total_timesteps        | 38912       |\n",
      "|    agent/train/approx_kl             | 0.010192422 |\n",
      "|    agent/train/clip_fraction         | 0.338       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.39       |\n",
      "|    agent/train/explained_variance    | 0.901       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0149     |\n",
      "|    agent/train/n_updates             | 180         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0127     |\n",
      "|    agent/train/std                   | 0.796       |\n",
      "|    agent/train/value_loss            | 0.0161      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -58.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.57       |\n",
      "|    agent/time/fps                    | 828         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 12          |\n",
      "|    agent/time/total_timesteps        | 40960       |\n",
      "|    agent/train/approx_kl             | 0.009951457 |\n",
      "|    agent/train/clip_fraction         | 0.331       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.34       |\n",
      "|    agent/train/explained_variance    | 0.923       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0375     |\n",
      "|    agent/train/n_updates             | 190         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0111     |\n",
      "|    agent/train/std                   | 0.783       |\n",
      "|    agent/train/value_loss            | 0.0168      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -58.4    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -3.77    |\n",
      "|    agent/time/fps                      | 908      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 6.6      |\n",
      "|    agent/time/total_timesteps          | 3.69e+04 |\n",
      "|    agent/train/approx_kl               | 0.00923  |\n",
      "|    agent/train/clip_fraction           | 0.323    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.39    |\n",
      "|    agent/train/explained_variance      | 0.903    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0275  |\n",
      "|    agent/train/n_updates               | 180      |\n",
      "|    agent/train/policy_gradient_loss    | -0.0107  |\n",
      "|    agent/train/std                     | 0.801    |\n",
      "|    agent/train/value_loss              | 0.0149   |\n",
      "|    preferences/entropy                 | 0.188    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.711    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.163    |\n",
      "|    reward/epoch-0/train/loss           | 0.574    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.707    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.157    |\n",
      "|    reward/epoch-1/train/loss           | 0.555    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.725    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.162    |\n",
      "|    reward/epoch-2/train/loss           | 0.546    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.725    |\n",
      "|    final/train/gt_reward_loss          | 0.162    |\n",
      "|    final/train/loss                    | 0.546    |\n",
      "-----------------------------------------------------\n",
      "Collecting 146 fragments (7300 transitions)\n",
      "Sampling 365 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 445 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dff4fe4915d44408113995d05dda9e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -59.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.54       |\n",
      "|    agent/time/fps                    | 1086        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 1           |\n",
      "|    agent/time/total_timesteps        | 43008       |\n",
      "|    agent/train/approx_kl             | 0.009126718 |\n",
      "|    agent/train/clip_fraction         | 0.329       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.32       |\n",
      "|    agent/train/explained_variance    | 0.936       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0375     |\n",
      "|    agent/train/n_updates             | 200         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0103     |\n",
      "|    agent/train/std                   | 0.778       |\n",
      "|    agent/train/value_loss            | 0.0133      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -60.1       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.64       |\n",
      "|    agent/time/fps                    | 898         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 4           |\n",
      "|    agent/time/total_timesteps        | 45056       |\n",
      "|    agent/train/approx_kl             | 0.009602737 |\n",
      "|    agent/train/clip_fraction         | 0.309       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.31       |\n",
      "|    agent/train/explained_variance    | 0.939       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.028      |\n",
      "|    agent/train/n_updates             | 210         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00977    |\n",
      "|    agent/train/std                   | 0.767       |\n",
      "|    agent/train/value_loss            | 0.0126      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -60.2       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.68       |\n",
      "|    agent/time/fps                    | 850         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 7           |\n",
      "|    agent/time/total_timesteps        | 47104       |\n",
      "|    agent/train/approx_kl             | 0.011066822 |\n",
      "|    agent/train/clip_fraction         | 0.354       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.27       |\n",
      "|    agent/train/explained_variance    | 0.945       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.026      |\n",
      "|    agent/train/n_updates             | 220         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00983    |\n",
      "|    agent/train/std                   | 0.756       |\n",
      "|    agent/train/value_loss            | 0.0152      |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -59.1        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.45        |\n",
      "|    agent/time/fps                    | 826          |\n",
      "|    agent/time/iterations             | 4            |\n",
      "|    agent/time/time_elapsed           | 9            |\n",
      "|    agent/time/total_timesteps        | 49152        |\n",
      "|    agent/train/approx_kl             | 0.0136333145 |\n",
      "|    agent/train/clip_fraction         | 0.374        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.24        |\n",
      "|    agent/train/explained_variance    | 0.92         |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0246      |\n",
      "|    agent/train/n_updates             | 230          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00904     |\n",
      "|    agent/train/std                   | 0.741        |\n",
      "|    agent/train/value_loss            | 0.0148       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -58.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.3        |\n",
      "|    agent/time/fps                    | 808         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 12          |\n",
      "|    agent/time/total_timesteps        | 51200       |\n",
      "|    agent/train/approx_kl             | 0.014298737 |\n",
      "|    agent/train/clip_fraction         | 0.388       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.17       |\n",
      "|    agent/train/explained_variance    | 0.951       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0473     |\n",
      "|    agent/train/n_updates             | 240         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0114     |\n",
      "|    agent/train/std                   | 0.717       |\n",
      "|    agent/train/value_loss            | 0.0131      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -59.6    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -3.52    |\n",
      "|    agent/time/fps                      | 894      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 6.6      |\n",
      "|    agent/time/total_timesteps          | 4.71e+04 |\n",
      "|    agent/train/approx_kl               | 0.0123   |\n",
      "|    agent/train/clip_fraction           | 0.363    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.22    |\n",
      "|    agent/train/explained_variance      | 0.943    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0307  |\n",
      "|    agent/train/n_updates               | 230      |\n",
      "|    agent/train/policy_gradient_loss    | -0.0105  |\n",
      "|    agent/train/std                     | 0.736    |\n",
      "|    agent/train/value_loss              | 0.0129   |\n",
      "|    preferences/entropy                 | 0.167    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.715    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.16     |\n",
      "|    reward/epoch-0/train/loss           | 0.565    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.732    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.158    |\n",
      "|    reward/epoch-1/train/loss           | 0.54     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.75     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.159    |\n",
      "|    reward/epoch-2/train/loss           | 0.527    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.75     |\n",
      "|    final/train/gt_reward_loss          | 0.159    |\n",
      "|    final/train/loss                    | 0.527    |\n",
      "-----------------------------------------------------\n",
      "Collecting 126 fragments (6300 transitions)\n",
      "Sampling 315 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 510 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94fc2cc36d864bf3867d39f32edf1f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -59         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.39       |\n",
      "|    agent/time/fps                    | 990         |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 2           |\n",
      "|    agent/time/total_timesteps        | 53248       |\n",
      "|    agent/train/approx_kl             | 0.013037488 |\n",
      "|    agent/train/clip_fraction         | 0.392       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.11       |\n",
      "|    agent/train/explained_variance    | 0.962       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0275     |\n",
      "|    agent/train/n_updates             | 250         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0126     |\n",
      "|    agent/train/std                   | 0.698       |\n",
      "|    agent/train/value_loss            | 0.00896     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -58.9       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.51       |\n",
      "|    agent/time/fps                    | 841         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 4           |\n",
      "|    agent/time/total_timesteps        | 55296       |\n",
      "|    agent/train/approx_kl             | 0.011891743 |\n",
      "|    agent/train/clip_fraction         | 0.327       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.09       |\n",
      "|    agent/train/explained_variance    | 0.906       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0213     |\n",
      "|    agent/train/n_updates             | 260         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00499    |\n",
      "|    agent/train/std                   | 0.688       |\n",
      "|    agent/train/value_loss            | 0.00733     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -57.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.46       |\n",
      "|    agent/time/fps                    | 801         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 7           |\n",
      "|    agent/time/total_timesteps        | 57344       |\n",
      "|    agent/train/approx_kl             | 0.011656096 |\n",
      "|    agent/train/clip_fraction         | 0.367       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.06       |\n",
      "|    agent/train/explained_variance    | 0.952       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0172     |\n",
      "|    agent/train/n_updates             | 270         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0105     |\n",
      "|    agent/train/std                   | 0.68        |\n",
      "|    agent/train/value_loss            | 0.00533     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -56.9       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.23       |\n",
      "|    agent/time/fps                    | 792         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 10          |\n",
      "|    agent/time/total_timesteps        | 59392       |\n",
      "|    agent/train/approx_kl             | 0.009418229 |\n",
      "|    agent/train/clip_fraction         | 0.304       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.05       |\n",
      "|    agent/train/explained_variance    | 0.948       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0384     |\n",
      "|    agent/train/n_updates             | 280         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00626    |\n",
      "|    agent/train/std                   | 0.682       |\n",
      "|    agent/train/value_loss            | 0.00581     |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 50         |\n",
      "|    agent/rollout/ep_rew_mean         | -56.3      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.08      |\n",
      "|    agent/time/fps                    | 777        |\n",
      "|    agent/time/iterations             | 5          |\n",
      "|    agent/time/time_elapsed           | 13         |\n",
      "|    agent/time/total_timesteps        | 61440      |\n",
      "|    agent/train/approx_kl             | 0.01019304 |\n",
      "|    agent/train/clip_fraction         | 0.351      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -2.01      |\n",
      "|    agent/train/explained_variance    | 0.964      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | -0.046     |\n",
      "|    agent/train/n_updates             | 290        |\n",
      "|    agent/train/policy_gradient_loss  | -0.0107    |\n",
      "|    agent/train/std                   | 0.669      |\n",
      "|    agent/train/value_loss            | 0.00536    |\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -57.7    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -3.33    |\n",
      "|    agent/time/fps                      | 840      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 7.2      |\n",
      "|    agent/time/total_timesteps          | 5.73e+04 |\n",
      "|    agent/train/approx_kl               | 0.0113   |\n",
      "|    agent/train/clip_fraction           | 0.341    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.04    |\n",
      "|    agent/train/explained_variance      | 0.948    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0323  |\n",
      "|    agent/train/n_updates               | 280      |\n",
      "|    agent/train/policy_gradient_loss    | -0.00782 |\n",
      "|    agent/train/std                     | 0.675    |\n",
      "|    agent/train/value_loss              | 0.00564  |\n",
      "|    preferences/entropy                 | 0.127    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.717    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.163    |\n",
      "|    reward/epoch-0/train/loss           | 0.583    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.725    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.163    |\n",
      "|    reward/epoch-1/train/loss           | 0.558    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.739    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.162    |\n",
      "|    reward/epoch-2/train/loss           | 0.531    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.739    |\n",
      "|    final/train/gt_reward_loss          | 0.162    |\n",
      "|    final/train/loss                    | 0.531    |\n",
      "-----------------------------------------------------\n",
      "Query schedule: [50, 127, 102, 85, 73, 63]\n",
      "Collecting 100 fragments (5000 transitions)\n",
      "Requested 4750 transitions but only 0 in buffer. Sampling 4750 additional transitions.\n",
      "Sampling 250 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 50 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba5131139e9450a91638d4cd4639b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "---------------------------------------------------\n",
      "| raw/                                 |          |\n",
      "|    agent/rollout/ep_len_mean         | 50       |\n",
      "|    agent/rollout/ep_rew_mean         | -60.2    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -28.7    |\n",
      "|    agent/time/fps                    | 1062     |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 1        |\n",
      "|    agent/time/total_timesteps        | 2048     |\n",
      "---------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -18.4       |\n",
      "|    agent/time/fps                    | 812         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 5           |\n",
      "|    agent/time/total_timesteps        | 4096        |\n",
      "|    agent/train/approx_kl             | 0.002738751 |\n",
      "|    agent/train/clip_fraction         | 0.111       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.85       |\n",
      "|    agent/train/explained_variance    | -0.253      |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.000328    |\n",
      "|    agent/train/n_updates             | 10          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00269    |\n",
      "|    agent/train/std                   | 1.01        |\n",
      "|    agent/train/value_loss            | 0.29        |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.5       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -10.2       |\n",
      "|    agent/time/fps                    | 692         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 8           |\n",
      "|    agent/time/total_timesteps        | 6144        |\n",
      "|    agent/train/approx_kl             | 0.003535796 |\n",
      "|    agent/train/clip_fraction         | 0.156       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.85       |\n",
      "|    agent/train/explained_variance    | -0.0599     |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0014     |\n",
      "|    agent/train/n_updates             | 20          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00729    |\n",
      "|    agent/train/std                   | 1.01        |\n",
      "|    agent/train/value_loss            | 0.0959      |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61.2        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.18        |\n",
      "|    agent/time/fps                    | 690          |\n",
      "|    agent/time/iterations             | 4            |\n",
      "|    agent/time/time_elapsed           | 11           |\n",
      "|    agent/time/total_timesteps        | 8192         |\n",
      "|    agent/train/approx_kl             | 0.0040204283 |\n",
      "|    agent/train/clip_fraction         | 0.182        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.84        |\n",
      "|    agent/train/explained_variance    | 0.461        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0264      |\n",
      "|    agent/train/n_updates             | 30           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0086      |\n",
      "|    agent/train/std                   | 0.995        |\n",
      "|    agent/train/value_loss            | 0.0575       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.3       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.18       |\n",
      "|    agent/time/fps                    | 628         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 16          |\n",
      "|    agent/time/total_timesteps        | 10240       |\n",
      "|    agent/train/approx_kl             | 0.005356969 |\n",
      "|    agent/train/clip_fraction         | 0.193       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.83       |\n",
      "|    agent/train/explained_variance    | 0.433       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0184     |\n",
      "|    agent/train/n_updates             | 40          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0105     |\n",
      "|    agent/train/std                   | 0.995       |\n",
      "|    agent/train/value_loss            | 0.0484      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                   |          |\n",
      "|    agent/rollout/ep_len_mean            | 50       |\n",
      "|    agent/rollout/ep_rew_mean            | -61      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean    | -13.9    |\n",
      "|    agent/time/fps                       | 777      |\n",
      "|    agent/time/iterations                | 3        |\n",
      "|    agent/time/time_elapsed              | 8.2      |\n",
      "|    agent/time/total_timesteps           | 6.14e+03 |\n",
      "|    agent/train/approx_kl                | 0.00434  |\n",
      "|    agent/train/clip_fraction            | 0.177    |\n",
      "|    agent/train/clip_range               | 0.1      |\n",
      "|    agent/train/entropy_loss             | -2.84    |\n",
      "|    agent/train/explained_variance       | 0.168    |\n",
      "|    agent/train/learning_rate            | 0.002    |\n",
      "|    agent/train/loss                     | -0.0152  |\n",
      "|    agent/train/n_updates                | 30       |\n",
      "|    agent/train/policy_gradient_loss     | -0.00827 |\n",
      "|    agent/train/std                      | 1        |\n",
      "|    agent/train/value_loss               | 0.106    |\n",
      "|    preferences/entropy                  | 0.233    |\n",
      "|    reward/epoch-0/train/accuracy        | 0.472    |\n",
      "|    reward/epoch-0/train/gt_reward_loss  | 0.27     |\n",
      "|    reward/epoch-0/train/loss            | 1.75     |\n",
      "|    reward/epoch-1/train/accuracy        | 0.448    |\n",
      "|    reward/epoch-1/train/gt_reward_loss  | 0.241    |\n",
      "|    reward/epoch-1/train/loss            | 1.48     |\n",
      "|    reward/epoch-10/train/accuracy       | 0.752    |\n",
      "|    reward/epoch-10/train/gt_reward_loss | 0.238    |\n",
      "|    reward/epoch-10/train/loss           | 0.545    |\n",
      "|    reward/epoch-11/train/accuracy       | 0.835    |\n",
      "|    reward/epoch-11/train/gt_reward_loss | 0.276    |\n",
      "|    reward/epoch-11/train/loss           | 0.488    |\n",
      "|    reward/epoch-2/train/accuracy        | 0.587    |\n",
      "|    reward/epoch-2/train/gt_reward_loss  | 0.281    |\n",
      "|    reward/epoch-2/train/loss            | 1.12     |\n",
      "|    reward/epoch-3/train/accuracy        | 0.602    |\n",
      "|    reward/epoch-3/train/gt_reward_loss  | 0.233    |\n",
      "|    reward/epoch-3/train/loss            | 0.86     |\n",
      "|    reward/epoch-4/train/accuracy        | 0.661    |\n",
      "|    reward/epoch-4/train/gt_reward_loss  | 0.251    |\n",
      "|    reward/epoch-4/train/loss            | 0.717    |\n",
      "|    reward/epoch-5/train/accuracy        | 0.665    |\n",
      "|    reward/epoch-5/train/gt_reward_loss  | 0.246    |\n",
      "|    reward/epoch-5/train/loss            | 0.649    |\n",
      "|    reward/epoch-6/train/accuracy        | 0.622    |\n",
      "|    reward/epoch-6/train/gt_reward_loss  | 0.241    |\n",
      "|    reward/epoch-6/train/loss            | 0.618    |\n",
      "|    reward/epoch-7/train/accuracy        | 0.714    |\n",
      "|    reward/epoch-7/train/gt_reward_loss  | 0.287    |\n",
      "|    reward/epoch-7/train/loss            | 0.584    |\n",
      "|    reward/epoch-8/train/accuracy        | 0.724    |\n",
      "|    reward/epoch-8/train/gt_reward_loss  | 0.235    |\n",
      "|    reward/epoch-8/train/loss            | 0.611    |\n",
      "|    reward/epoch-9/train/accuracy        | 0.752    |\n",
      "|    reward/epoch-9/train/gt_reward_loss  | 0.249    |\n",
      "|    reward/epoch-9/train/loss            | 0.575    |\n",
      "| reward/                                 |          |\n",
      "|    final/train/accuracy                 | 0.835    |\n",
      "|    final/train/gt_reward_loss           | 0.276    |\n",
      "|    final/train/loss                     | 0.488    |\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting 254 fragments (12700 transitions)\n",
      "Requested 12065 transitions but only 10200 in buffer. Sampling 1865 additional transitions.\n",
      "Sampling 635 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 180 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3444572ede5a463da8b34b0a7aa734be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61.6        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.97        |\n",
      "|    agent/time/fps                    | 902          |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 2            |\n",
      "|    agent/time/total_timesteps        | 12288        |\n",
      "|    agent/train/approx_kl             | 0.0060605095 |\n",
      "|    agent/train/clip_fraction         | 0.244        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.84        |\n",
      "|    agent/train/explained_variance    | 0.259        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.03        |\n",
      "|    agent/train/n_updates             | 50           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0123      |\n",
      "|    agent/train/std                   | 1            |\n",
      "|    agent/train/value_loss            | 0.039        |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -62.2        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.04        |\n",
      "|    agent/time/fps                    | 790          |\n",
      "|    agent/time/iterations             | 2            |\n",
      "|    agent/time/time_elapsed           | 5            |\n",
      "|    agent/time/total_timesteps        | 14336        |\n",
      "|    agent/train/approx_kl             | 0.0055453354 |\n",
      "|    agent/train/clip_fraction         | 0.231        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.83        |\n",
      "|    agent/train/explained_variance    | 0.459        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0397      |\n",
      "|    agent/train/n_updates             | 60           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00987     |\n",
      "|    agent/train/std                   | 0.996        |\n",
      "|    agent/train/value_loss            | 0.0258       |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -62.2        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.21        |\n",
      "|    agent/time/fps                    | 757          |\n",
      "|    agent/time/iterations             | 3            |\n",
      "|    agent/time/time_elapsed           | 8            |\n",
      "|    agent/time/total_timesteps        | 16384        |\n",
      "|    agent/train/approx_kl             | 0.0065446813 |\n",
      "|    agent/train/clip_fraction         | 0.266        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.83        |\n",
      "|    agent/train/explained_variance    | 0.629        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0402      |\n",
      "|    agent/train/n_updates             | 70           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0109      |\n",
      "|    agent/train/std                   | 0.992        |\n",
      "|    agent/train/value_loss            | 0.0288       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -62.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.19       |\n",
      "|    agent/time/fps                    | 733         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 11          |\n",
      "|    agent/time/total_timesteps        | 18432       |\n",
      "|    agent/train/approx_kl             | 0.006503021 |\n",
      "|    agent/train/clip_fraction         | 0.256       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.81       |\n",
      "|    agent/train/explained_variance    | 0.724       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0499     |\n",
      "|    agent/train/n_updates             | 80          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0127     |\n",
      "|    agent/train/std                   | 0.989       |\n",
      "|    agent/train/value_loss            | 0.021       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -62.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.2        |\n",
      "|    agent/time/fps                    | 709         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 14          |\n",
      "|    agent/time/total_timesteps        | 20480       |\n",
      "|    agent/train/approx_kl             | 0.008835153 |\n",
      "|    agent/train/clip_fraction         | 0.31        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.8        |\n",
      "|    agent/train/explained_variance    | 0.721       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.033      |\n",
      "|    agent/train/n_updates             | 90          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0149     |\n",
      "|    agent/train/std                   | 0.985       |\n",
      "|    agent/train/value_loss            | 0.0179      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -62.2    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -6.12    |\n",
      "|    agent/time/fps                      | 778      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 8        |\n",
      "|    agent/time/total_timesteps          | 1.64e+04 |\n",
      "|    agent/train/approx_kl               | 0.00727  |\n",
      "|    agent/train/clip_fraction           | 0.271    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.81    |\n",
      "|    agent/train/explained_variance      | 0.659    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0386  |\n",
      "|    agent/train/n_updates               | 80       |\n",
      "|    agent/train/policy_gradient_loss    | -0.0123  |\n",
      "|    agent/train/std                     | 0.987    |\n",
      "|    agent/train/value_loss              | 0.0215   |\n",
      "|    preferences/entropy                 | 0.167    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.575    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.17     |\n",
      "|    reward/epoch-0/train/loss           | 0.73     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.592    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.174    |\n",
      "|    reward/epoch-1/train/loss           | 0.708    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.625    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.17     |\n",
      "|    reward/epoch-2/train/loss           | 0.645    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.625    |\n",
      "|    final/train/gt_reward_loss          | 0.17     |\n",
      "|    final/train/loss                    | 0.645    |\n",
      "-----------------------------------------------------\n",
      "Collecting 204 fragments (10200 transitions)\n",
      "Sampling 510 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 285 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b99bd23a4449759799ac36815bac88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.06       |\n",
      "|    agent/time/fps                    | 682         |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 3           |\n",
      "|    agent/time/total_timesteps        | 22528       |\n",
      "|    agent/train/approx_kl             | 0.008941688 |\n",
      "|    agent/train/clip_fraction         | 0.294       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.78       |\n",
      "|    agent/train/explained_variance    | 0.76        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0301     |\n",
      "|    agent/train/n_updates             | 100         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0134     |\n",
      "|    agent/train/std                   | 0.975       |\n",
      "|    agent/train/value_loss            | 0.0138      |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61.9        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.16        |\n",
      "|    agent/time/fps                    | 673          |\n",
      "|    agent/time/iterations             | 2            |\n",
      "|    agent/time/time_elapsed           | 6            |\n",
      "|    agent/time/total_timesteps        | 24576        |\n",
      "|    agent/train/approx_kl             | 0.0061217835 |\n",
      "|    agent/train/clip_fraction         | 0.242        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.77        |\n",
      "|    agent/train/explained_variance    | 0.663        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0488      |\n",
      "|    agent/train/n_updates             | 110          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00821     |\n",
      "|    agent/train/std                   | 0.965        |\n",
      "|    agent/train/value_loss            | 0.0123       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -62.2       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.36       |\n",
      "|    agent/time/fps                    | 675         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 9           |\n",
      "|    agent/time/total_timesteps        | 26624       |\n",
      "|    agent/train/approx_kl             | 0.008259151 |\n",
      "|    agent/train/clip_fraction         | 0.275       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.76       |\n",
      "|    agent/train/explained_variance    | 0.835       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0225     |\n",
      "|    agent/train/n_updates             | 120         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00984    |\n",
      "|    agent/train/std                   | 0.962       |\n",
      "|    agent/train/value_loss            | 0.0114      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.33       |\n",
      "|    agent/time/fps                    | 636         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 12          |\n",
      "|    agent/time/total_timesteps        | 28672       |\n",
      "|    agent/train/approx_kl             | 0.008941652 |\n",
      "|    agent/train/clip_fraction         | 0.314       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.75       |\n",
      "|    agent/train/explained_variance    | 0.878       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0582     |\n",
      "|    agent/train/n_updates             | 130         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0159     |\n",
      "|    agent/train/std                   | 0.955       |\n",
      "|    agent/train/value_loss            | 0.00896     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.2       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.15       |\n",
      "|    agent/time/fps                    | 647         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 15          |\n",
      "|    agent/time/total_timesteps        | 30720       |\n",
      "|    agent/train/approx_kl             | 0.010086689 |\n",
      "|    agent/train/clip_fraction         | 0.343       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.72       |\n",
      "|    agent/train/explained_variance    | 0.885       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0492     |\n",
      "|    agent/train/n_updates             | 140         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0157     |\n",
      "|    agent/train/std                   | 0.941       |\n",
      "|    agent/train/value_loss            | 0.0104      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -61.7    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -6.21    |\n",
      "|    agent/time/fps                      | 663      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 9        |\n",
      "|    agent/time/total_timesteps          | 2.66e+04 |\n",
      "|    agent/train/approx_kl               | 0.00843  |\n",
      "|    agent/train/clip_fraction           | 0.299    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.74    |\n",
      "|    agent/train/explained_variance      | 0.832    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0446  |\n",
      "|    agent/train/n_updates               | 130      |\n",
      "|    agent/train/policy_gradient_loss    | -0.0125  |\n",
      "|    agent/train/std                     | 0.95     |\n",
      "|    agent/train/value_loss              | 0.0103   |\n",
      "|    preferences/entropy                 | 0.2      |\n",
      "|    reward/epoch-0/train/accuracy       | 0.638    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.177    |\n",
      "|    reward/epoch-0/train/loss           | 0.624    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.716    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.178    |\n",
      "|    reward/epoch-1/train/loss           | 0.584    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.712    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.178    |\n",
      "|    reward/epoch-2/train/loss           | 0.574    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.712    |\n",
      "|    final/train/gt_reward_loss          | 0.178    |\n",
      "|    final/train/loss                    | 0.574    |\n",
      "-----------------------------------------------------\n",
      "Collecting 170 fragments (8500 transitions)\n",
      "Sampling 425 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 370 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "300ee358e83e4095a91f212f5f91dfc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 50         |\n",
      "|    agent/rollout/ep_rew_mean         | -61.3      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.11      |\n",
      "|    agent/time/fps                    | 993        |\n",
      "|    agent/time/iterations             | 1          |\n",
      "|    agent/time/time_elapsed           | 2          |\n",
      "|    agent/time/total_timesteps        | 32768      |\n",
      "|    agent/train/approx_kl             | 0.00873569 |\n",
      "|    agent/train/clip_fraction         | 0.32       |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -2.69      |\n",
      "|    agent/train/explained_variance    | 0.901      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | -0.0445    |\n",
      "|    agent/train/n_updates             | 150        |\n",
      "|    agent/train/policy_gradient_loss  | -0.0129    |\n",
      "|    agent/train/std                   | 0.926      |\n",
      "|    agent/train/value_loss            | 0.00845    |\n",
      "-----------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -60.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.28       |\n",
      "|    agent/time/fps                    | 836         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 4           |\n",
      "|    agent/time/total_timesteps        | 34816       |\n",
      "|    agent/train/approx_kl             | 0.009306194 |\n",
      "|    agent/train/clip_fraction         | 0.323       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.65       |\n",
      "|    agent/train/explained_variance    | 0.867       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0299     |\n",
      "|    agent/train/n_updates             | 160         |\n",
      "|    agent/train/policy_gradient_loss  | -0.013      |\n",
      "|    agent/train/std                   | 0.909       |\n",
      "|    agent/train/value_loss            | 0.00785     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -59.9       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.43       |\n",
      "|    agent/time/fps                    | 782         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 7           |\n",
      "|    agent/time/total_timesteps        | 36864       |\n",
      "|    agent/train/approx_kl             | 0.011548715 |\n",
      "|    agent/train/clip_fraction         | 0.33        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.61       |\n",
      "|    agent/train/explained_variance    | 0.888       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0381     |\n",
      "|    agent/train/n_updates             | 170         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0132     |\n",
      "|    agent/train/std                   | 0.892       |\n",
      "|    agent/train/value_loss            | 0.0101      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -58.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.41       |\n",
      "|    agent/time/fps                    | 755         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 10          |\n",
      "|    agent/time/total_timesteps        | 38912       |\n",
      "|    agent/train/approx_kl             | 0.012369854 |\n",
      "|    agent/train/clip_fraction         | 0.365       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.56       |\n",
      "|    agent/train/explained_variance    | 0.917       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0292     |\n",
      "|    agent/train/n_updates             | 180         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0144     |\n",
      "|    agent/train/std                   | 0.868       |\n",
      "|    agent/train/value_loss            | 0.00762     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -58.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.25       |\n",
      "|    agent/time/fps                    | 710         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 14          |\n",
      "|    agent/time/total_timesteps        | 40960       |\n",
      "|    agent/train/approx_kl             | 0.011113337 |\n",
      "|    agent/train/clip_fraction         | 0.359       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.5        |\n",
      "|    agent/train/explained_variance    | 0.948       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0403     |\n",
      "|    agent/train/n_updates             | 190         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0151     |\n",
      "|    agent/train/std                   | 0.842       |\n",
      "|    agent/train/value_loss            | 0.00723     |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -59.7    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -6.29    |\n",
      "|    agent/time/fps                      | 815      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 7.4      |\n",
      "|    agent/time/total_timesteps          | 3.69e+04 |\n",
      "|    agent/train/approx_kl               | 0.0114   |\n",
      "|    agent/train/clip_fraction           | 0.348    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.56    |\n",
      "|    agent/train/explained_variance      | 0.904    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.039   |\n",
      "|    agent/train/n_updates               | 180      |\n",
      "|    agent/train/policy_gradient_loss    | -0.0131  |\n",
      "|    agent/train/std                     | 0.867    |\n",
      "|    agent/train/value_loss              | 0.00791  |\n",
      "|    preferences/entropy                 | 0.179    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.701    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.197    |\n",
      "|    reward/epoch-0/train/loss           | 0.589    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.709    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.197    |\n",
      "|    reward/epoch-1/train/loss           | 0.579    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.731    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.186    |\n",
      "|    reward/epoch-2/train/loss           | 0.552    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.731    |\n",
      "|    final/train/gt_reward_loss          | 0.186    |\n",
      "|    final/train/loss                    | 0.552    |\n",
      "-----------------------------------------------------\n",
      "Collecting 146 fragments (7300 transitions)\n",
      "Sampling 365 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 445 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de8f531397634b60b2603013104ca719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -58.2       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.28       |\n",
      "|    agent/time/fps                    | 993         |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 2           |\n",
      "|    agent/time/total_timesteps        | 43008       |\n",
      "|    agent/train/approx_kl             | 0.012673536 |\n",
      "|    agent/train/clip_fraction         | 0.363       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.46       |\n",
      "|    agent/train/explained_variance    | 0.898       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0577     |\n",
      "|    agent/train/n_updates             | 200         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00976    |\n",
      "|    agent/train/std                   | 0.823       |\n",
      "|    agent/train/value_loss            | 0.00675     |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 50         |\n",
      "|    agent/rollout/ep_rew_mean         | -58.9      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.43      |\n",
      "|    agent/time/fps                    | 811        |\n",
      "|    agent/time/iterations             | 2          |\n",
      "|    agent/time/time_elapsed           | 5          |\n",
      "|    agent/time/total_timesteps        | 45056      |\n",
      "|    agent/train/approx_kl             | 0.01036169 |\n",
      "|    agent/train/clip_fraction         | 0.342      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -2.42      |\n",
      "|    agent/train/explained_variance    | 0.924      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | -0.0454    |\n",
      "|    agent/train/n_updates             | 210        |\n",
      "|    agent/train/policy_gradient_loss  | -0.0105    |\n",
      "|    agent/train/std                   | 0.807      |\n",
      "|    agent/train/value_loss            | 0.007      |\n",
      "-----------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -59.2       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.6        |\n",
      "|    agent/time/fps                    | 776         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 7           |\n",
      "|    agent/time/total_timesteps        | 47104       |\n",
      "|    agent/train/approx_kl             | 0.011365764 |\n",
      "|    agent/train/clip_fraction         | 0.369       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.39       |\n",
      "|    agent/train/explained_variance    | 0.879       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0356     |\n",
      "|    agent/train/n_updates             | 220         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00864    |\n",
      "|    agent/train/std                   | 0.798       |\n",
      "|    agent/train/value_loss            | 0.0095      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -58.3       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.49       |\n",
      "|    agent/time/fps                    | 671         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 12          |\n",
      "|    agent/time/total_timesteps        | 49152       |\n",
      "|    agent/train/approx_kl             | 0.012793833 |\n",
      "|    agent/train/clip_fraction         | 0.384       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.36       |\n",
      "|    agent/train/explained_variance    | 0.903       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0356     |\n",
      "|    agent/train/n_updates             | 230         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0117     |\n",
      "|    agent/train/std                   | 0.785       |\n",
      "|    agent/train/value_loss            | 0.0123      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -57         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.39       |\n",
      "|    agent/time/fps                    | 660         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 15          |\n",
      "|    agent/time/total_timesteps        | 51200       |\n",
      "|    agent/train/approx_kl             | 0.010862654 |\n",
      "|    agent/train/clip_fraction         | 0.368       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.3        |\n",
      "|    agent/train/explained_variance    | 0.908       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0289     |\n",
      "|    agent/train/n_updates             | 240         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0118     |\n",
      "|    agent/train/std                   | 0.761       |\n",
      "|    agent/train/value_loss            | 0.0104      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -58.3    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -6.44    |\n",
      "|    agent/time/fps                      | 782      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 8.2      |\n",
      "|    agent/time/total_timesteps          | 4.71e+04 |\n",
      "|    agent/train/approx_kl               | 0.0115   |\n",
      "|    agent/train/clip_fraction           | 0.365    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.35    |\n",
      "|    agent/train/explained_variance      | 0.908    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0363  |\n",
      "|    agent/train/n_updates               | 230      |\n",
      "|    agent/train/policy_gradient_loss    | -0.0105  |\n",
      "|    agent/train/std                     | 0.78     |\n",
      "|    agent/train/value_loss              | 0.00945  |\n",
      "|    preferences/entropy                 | 0.191    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.719    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.192    |\n",
      "|    reward/epoch-0/train/loss           | 0.576    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.728    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.192    |\n",
      "|    reward/epoch-1/train/loss           | 0.557    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.716    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.191    |\n",
      "|    reward/epoch-2/train/loss           | 0.562    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.716    |\n",
      "|    final/train/gt_reward_loss          | 0.191    |\n",
      "|    final/train/loss                    | 0.562    |\n",
      "-----------------------------------------------------\n",
      "Collecting 126 fragments (6300 transitions)\n",
      "Sampling 315 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 510 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23dba74bf8c406ba177b1dea6dabe8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -57.1       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.48       |\n",
      "|    agent/time/fps                    | 935         |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 2           |\n",
      "|    agent/time/total_timesteps        | 53248       |\n",
      "|    agent/train/approx_kl             | 0.012068499 |\n",
      "|    agent/train/clip_fraction         | 0.364       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.26       |\n",
      "|    agent/train/explained_variance    | 0.928       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0361     |\n",
      "|    agent/train/n_updates             | 250         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0101     |\n",
      "|    agent/train/std                   | 0.747       |\n",
      "|    agent/train/value_loss            | 0.0081      |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -56          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.92        |\n",
      "|    agent/time/fps                    | 792          |\n",
      "|    agent/time/iterations             | 2            |\n",
      "|    agent/time/time_elapsed           | 5            |\n",
      "|    agent/time/total_timesteps        | 55296        |\n",
      "|    agent/train/approx_kl             | 0.0115339635 |\n",
      "|    agent/train/clip_fraction         | 0.33         |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.26        |\n",
      "|    agent/train/explained_variance    | 0.875        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0328      |\n",
      "|    agent/train/n_updates             | 260          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00576     |\n",
      "|    agent/train/std                   | 0.75         |\n",
      "|    agent/train/value_loss            | 0.0104       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -55.5       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.24       |\n",
      "|    agent/time/fps                    | 706         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 8           |\n",
      "|    agent/time/total_timesteps        | 57344       |\n",
      "|    agent/train/approx_kl             | 0.013944184 |\n",
      "|    agent/train/clip_fraction         | 0.366       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.24       |\n",
      "|    agent/train/explained_variance    | 0.877       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0126     |\n",
      "|    agent/train/n_updates             | 270         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00712    |\n",
      "|    agent/train/std                   | 0.734       |\n",
      "|    agent/train/value_loss            | 0.01        |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 50         |\n",
      "|    agent/rollout/ep_rew_mean         | -54.2      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.21      |\n",
      "|    agent/time/fps                    | 669        |\n",
      "|    agent/time/iterations             | 4          |\n",
      "|    agent/time/time_elapsed           | 12         |\n",
      "|    agent/time/total_timesteps        | 59392      |\n",
      "|    agent/train/approx_kl             | 0.01252081 |\n",
      "|    agent/train/clip_fraction         | 0.354      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -2.2       |\n",
      "|    agent/train/explained_variance    | 0.909      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | -0.0406    |\n",
      "|    agent/train/n_updates             | 280        |\n",
      "|    agent/train/policy_gradient_loss  | -0.00768   |\n",
      "|    agent/train/std                   | 0.72       |\n",
      "|    agent/train/value_loss            | 0.00758    |\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 50         |\n",
      "|    agent/rollout/ep_rew_mean         | -54.7      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.16      |\n",
      "|    agent/time/fps                    | 678        |\n",
      "|    agent/time/iterations             | 5          |\n",
      "|    agent/time/time_elapsed           | 15         |\n",
      "|    agent/time/total_timesteps        | 61440      |\n",
      "|    agent/train/approx_kl             | 0.01595094 |\n",
      "|    agent/train/clip_fraction         | 0.368      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -2.16      |\n",
      "|    agent/train/explained_variance    | 0.879      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | -0.00404   |\n",
      "|    agent/train/n_updates             | 290        |\n",
      "|    agent/train/policy_gradient_loss  | -0.00934   |\n",
      "|    agent/train/std                   | 0.71       |\n",
      "|    agent/train/value_loss            | 0.0128     |\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -55.5    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -7       |\n",
      "|    agent/time/fps                      | 756      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 8.4      |\n",
      "|    agent/time/total_timesteps          | 5.73e+04 |\n",
      "|    agent/train/approx_kl               | 0.0133   |\n",
      "|    agent/train/clip_fraction           | 0.353    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.19    |\n",
      "|    agent/train/explained_variance      | 0.882    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0275  |\n",
      "|    agent/train/n_updates               | 280      |\n",
      "|    agent/train/policy_gradient_loss    | -0.00769 |\n",
      "|    agent/train/std                     | 0.722    |\n",
      "|    agent/train/value_loss              | 0.01     |\n",
      "|    preferences/entropy                 | 0.0704   |\n",
      "|    reward/epoch-0/train/accuracy       | 0.719    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.172    |\n",
      "|    reward/epoch-0/train/loss           | 0.556    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.729    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.173    |\n",
      "|    reward/epoch-1/train/loss           | 0.543    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.743    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.172    |\n",
      "|    reward/epoch-2/train/loss           | 0.526    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.743    |\n",
      "|    final/train/gt_reward_loss          | 0.172    |\n",
      "|    final/train/loss                    | 0.526    |\n",
      "-----------------------------------------------------\n",
      "Query schedule: [50, 127, 102, 85, 73, 63]\n",
      "Collecting 100 fragments (5000 transitions)\n",
      "Requested 4750 transitions but only 0 in buffer. Sampling 4750 additional transitions.\n",
      "Sampling 250 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 50 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a86112e129594daa875d89eceb2d2945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "---------------------------------------------------\n",
      "| raw/                                 |          |\n",
      "|    agent/rollout/ep_len_mean         | 50       |\n",
      "|    agent/rollout/ep_rew_mean         | -60.2    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -28.7    |\n",
      "|    agent/time/fps                    | 1066     |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 1        |\n",
      "|    agent/time/total_timesteps        | 2048     |\n",
      "---------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -60.7        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -18.2        |\n",
      "|    agent/time/fps                    | 867          |\n",
      "|    agent/time/iterations             | 2            |\n",
      "|    agent/time/time_elapsed           | 4            |\n",
      "|    agent/time/total_timesteps        | 4096         |\n",
      "|    agent/train/approx_kl             | 0.0027847397 |\n",
      "|    agent/train/clip_fraction         | 0.123        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.85        |\n",
      "|    agent/train/explained_variance    | 0.0137       |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.00544      |\n",
      "|    agent/train/n_updates             | 10           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00343     |\n",
      "|    agent/train/std                   | 1.01         |\n",
      "|    agent/train/value_loss            | 0.225        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.2       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -9.96       |\n",
      "|    agent/time/fps                    | 808         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 7           |\n",
      "|    agent/time/total_timesteps        | 6144        |\n",
      "|    agent/train/approx_kl             | 0.005646358 |\n",
      "|    agent/train/clip_fraction         | 0.229       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.87       |\n",
      "|    agent/train/explained_variance    | 0.33        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.00298    |\n",
      "|    agent/train/n_updates             | 20          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0116     |\n",
      "|    agent/train/std                   | 1.02        |\n",
      "|    agent/train/value_loss            | 0.11        |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61.3        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.88        |\n",
      "|    agent/time/fps                    | 781          |\n",
      "|    agent/time/iterations             | 4            |\n",
      "|    agent/time/time_elapsed           | 10           |\n",
      "|    agent/time/total_timesteps        | 8192         |\n",
      "|    agent/train/approx_kl             | 0.0054920474 |\n",
      "|    agent/train/clip_fraction         | 0.231        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.86        |\n",
      "|    agent/train/explained_variance    | 0.675        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.029       |\n",
      "|    agent/train/n_updates             | 30           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0109      |\n",
      "|    agent/train/std                   | 1.01         |\n",
      "|    agent/train/value_loss            | 0.063        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.3       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.79       |\n",
      "|    agent/time/fps                    | 767         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 13          |\n",
      "|    agent/time/total_timesteps        | 10240       |\n",
      "|    agent/train/approx_kl             | 0.005364392 |\n",
      "|    agent/train/clip_fraction         | 0.234       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.85       |\n",
      "|    agent/train/explained_variance    | 0.495       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0248     |\n",
      "|    agent/train/n_updates             | 40          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0114     |\n",
      "|    agent/train/std                   | 1           |\n",
      "|    agent/train/value_loss            | 0.0561      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                   |          |\n",
      "|    agent/rollout/ep_len_mean            | 50       |\n",
      "|    agent/rollout/ep_rew_mean            | -60.9    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean    | -13.7    |\n",
      "|    agent/time/fps                       | 858      |\n",
      "|    agent/time/iterations                | 3        |\n",
      "|    agent/time/time_elapsed              | 7        |\n",
      "|    agent/time/total_timesteps           | 6.14e+03 |\n",
      "|    agent/train/approx_kl                | 0.00501  |\n",
      "|    agent/train/clip_fraction            | 0.219    |\n",
      "|    agent/train/clip_range               | 0.1      |\n",
      "|    agent/train/entropy_loss             | -2.85    |\n",
      "|    agent/train/explained_variance       | 0.418    |\n",
      "|    agent/train/learning_rate            | 0.002    |\n",
      "|    agent/train/loss                     | -0.0165  |\n",
      "|    agent/train/n_updates                | 30       |\n",
      "|    agent/train/policy_gradient_loss     | -0.0103  |\n",
      "|    agent/train/std                      | 1.01     |\n",
      "|    agent/train/value_loss               | 0.0978   |\n",
      "|    preferences/entropy                  | 0.218    |\n",
      "|    reward/epoch-0/train/accuracy        | 0.587    |\n",
      "|    reward/epoch-0/train/gt_reward_loss  | 0.15     |\n",
      "|    reward/epoch-0/train/loss            | 1.13     |\n",
      "|    reward/epoch-1/train/accuracy        | 0.655    |\n",
      "|    reward/epoch-1/train/gt_reward_loss  | 0.141    |\n",
      "|    reward/epoch-1/train/loss            | 0.937    |\n",
      "|    reward/epoch-10/train/accuracy       | 0.764    |\n",
      "|    reward/epoch-10/train/gt_reward_loss | 0.159    |\n",
      "|    reward/epoch-10/train/loss           | 0.517    |\n",
      "|    reward/epoch-11/train/accuracy       | 0.807    |\n",
      "|    reward/epoch-11/train/gt_reward_loss | 0.146    |\n",
      "|    reward/epoch-11/train/loss           | 0.494    |\n",
      "|    reward/epoch-2/train/accuracy        | 0.618    |\n",
      "|    reward/epoch-2/train/gt_reward_loss  | 0.152    |\n",
      "|    reward/epoch-2/train/loss            | 0.899    |\n",
      "|    reward/epoch-3/train/accuracy        | 0.622    |\n",
      "|    reward/epoch-3/train/gt_reward_loss  | 0.169    |\n",
      "|    reward/epoch-3/train/loss            | 0.816    |\n",
      "|    reward/epoch-4/train/accuracy        | 0.665    |\n",
      "|    reward/epoch-4/train/gt_reward_loss  | 0.166    |\n",
      "|    reward/epoch-4/train/loss            | 0.687    |\n",
      "|    reward/epoch-5/train/accuracy        | 0.661    |\n",
      "|    reward/epoch-5/train/gt_reward_loss  | 0.141    |\n",
      "|    reward/epoch-5/train/loss            | 0.634    |\n",
      "|    reward/epoch-6/train/accuracy        | 0.646    |\n",
      "|    reward/epoch-6/train/gt_reward_loss  | 0.158    |\n",
      "|    reward/epoch-6/train/loss            | 0.602    |\n",
      "|    reward/epoch-7/train/accuracy        | 0.661    |\n",
      "|    reward/epoch-7/train/gt_reward_loss  | 0.17     |\n",
      "|    reward/epoch-7/train/loss            | 0.566    |\n",
      "|    reward/epoch-8/train/accuracy        | 0.748    |\n",
      "|    reward/epoch-8/train/gt_reward_loss  | 0.153    |\n",
      "|    reward/epoch-8/train/loss            | 0.564    |\n",
      "|    reward/epoch-9/train/accuracy        | 0.792    |\n",
      "|    reward/epoch-9/train/gt_reward_loss  | 0.149    |\n",
      "|    reward/epoch-9/train/loss            | 0.527    |\n",
      "| reward/                                 |          |\n",
      "|    final/train/accuracy                 | 0.807    |\n",
      "|    final/train/gt_reward_loss           | 0.146    |\n",
      "|    final/train/loss                     | 0.494    |\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting 254 fragments (12700 transitions)\n",
      "Requested 12065 transitions but only 10200 in buffer. Sampling 1865 additional transitions.\n",
      "Sampling 635 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 180 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868086c7204f4eb4b193c6a6ce9b6233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -60.9        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.37        |\n",
      "|    agent/time/fps                    | 1087         |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 1            |\n",
      "|    agent/time/total_timesteps        | 12288        |\n",
      "|    agent/train/approx_kl             | 0.0057599708 |\n",
      "|    agent/train/clip_fraction         | 0.277        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.83        |\n",
      "|    agent/train/explained_variance    | 0.574        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0313      |\n",
      "|    agent/train/n_updates             | 50           |\n",
      "|    agent/train/policy_gradient_loss  | -0.014       |\n",
      "|    agent/train/std                   | 1            |\n",
      "|    agent/train/value_loss            | 0.0348       |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -62          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.66        |\n",
      "|    agent/time/fps                    | 867          |\n",
      "|    agent/time/iterations             | 2            |\n",
      "|    agent/time/time_elapsed           | 4            |\n",
      "|    agent/time/total_timesteps        | 14336        |\n",
      "|    agent/train/approx_kl             | 0.0058816858 |\n",
      "|    agent/train/clip_fraction         | 0.223        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.85        |\n",
      "|    agent/train/explained_variance    | 0.466        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0309      |\n",
      "|    agent/train/n_updates             | 60           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00919     |\n",
      "|    agent/train/std                   | 1.01         |\n",
      "|    agent/train/value_loss            | 0.0372       |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -62.4        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.1         |\n",
      "|    agent/time/fps                    | 810          |\n",
      "|    agent/time/iterations             | 3            |\n",
      "|    agent/time/time_elapsed           | 7            |\n",
      "|    agent/time/total_timesteps        | 16384        |\n",
      "|    agent/train/approx_kl             | 0.0077629886 |\n",
      "|    agent/train/clip_fraction         | 0.265        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.85        |\n",
      "|    agent/train/explained_variance    | 0.739        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0256      |\n",
      "|    agent/train/n_updates             | 70           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0118      |\n",
      "|    agent/train/std                   | 1            |\n",
      "|    agent/train/value_loss            | 0.0277       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -62.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.14       |\n",
      "|    agent/time/fps                    | 784         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 10          |\n",
      "|    agent/time/total_timesteps        | 18432       |\n",
      "|    agent/train/approx_kl             | 0.007975528 |\n",
      "|    agent/train/clip_fraction         | 0.279       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.82       |\n",
      "|    agent/train/explained_variance    | 0.817       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0491     |\n",
      "|    agent/train/n_updates             | 80          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0136     |\n",
      "|    agent/train/std                   | 0.991       |\n",
      "|    agent/train/value_loss            | 0.0234      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -62.1       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.08       |\n",
      "|    agent/time/fps                    | 752         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 13          |\n",
      "|    agent/time/total_timesteps        | 20480       |\n",
      "|    agent/train/approx_kl             | 0.008250463 |\n",
      "|    agent/train/clip_fraction         | 0.318       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.8        |\n",
      "|    agent/train/explained_variance    | 0.736       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0385     |\n",
      "|    agent/train/n_updates             | 90          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0151     |\n",
      "|    agent/train/std                   | 0.983       |\n",
      "|    agent/train/value_loss            | 0.0225      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -62      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -5.87    |\n",
      "|    agent/time/fps                      | 860      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 7        |\n",
      "|    agent/time/total_timesteps          | 1.64e+04 |\n",
      "|    agent/train/approx_kl               | 0.00744  |\n",
      "|    agent/train/clip_fraction           | 0.274    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.82    |\n",
      "|    agent/train/explained_variance      | 0.718    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0356  |\n",
      "|    agent/train/n_updates               | 80       |\n",
      "|    agent/train/policy_gradient_loss    | -0.0125  |\n",
      "|    agent/train/std                     | 0.993    |\n",
      "|    agent/train/value_loss              | 0.0261   |\n",
      "|    preferences/entropy                 | 0.179    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.619    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.214    |\n",
      "|    reward/epoch-0/train/loss           | 0.739    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.631    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.202    |\n",
      "|    reward/epoch-1/train/loss           | 0.712    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.655    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.217    |\n",
      "|    reward/epoch-2/train/loss           | 0.654    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.655    |\n",
      "|    final/train/gt_reward_loss          | 0.217    |\n",
      "|    final/train/loss                    | 0.654    |\n",
      "-----------------------------------------------------\n",
      "Collecting 204 fragments (10200 transitions)\n",
      "Sampling 510 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 285 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1645cd33f9304570ae3a8b480b00fa58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.97       |\n",
      "|    agent/time/fps                    | 962         |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 2           |\n",
      "|    agent/time/total_timesteps        | 22528       |\n",
      "|    agent/train/approx_kl             | 0.007324839 |\n",
      "|    agent/train/clip_fraction         | 0.286       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.79       |\n",
      "|    agent/train/explained_variance    | 0.83        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0342     |\n",
      "|    agent/train/n_updates             | 100         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0129     |\n",
      "|    agent/train/std                   | 0.977       |\n",
      "|    agent/train/value_loss            | 0.0197      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.9       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.17       |\n",
      "|    agent/time/fps                    | 813         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 5           |\n",
      "|    agent/time/total_timesteps        | 24576       |\n",
      "|    agent/train/approx_kl             | 0.007893221 |\n",
      "|    agent/train/clip_fraction         | 0.283       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.77       |\n",
      "|    agent/train/explained_variance    | 0.815       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0544     |\n",
      "|    agent/train/n_updates             | 110         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0111     |\n",
      "|    agent/train/std                   | 0.971       |\n",
      "|    agent/train/value_loss            | 0.0223      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -62.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.31       |\n",
      "|    agent/time/fps                    | 742         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 8           |\n",
      "|    agent/time/total_timesteps        | 26624       |\n",
      "|    agent/train/approx_kl             | 0.008302388 |\n",
      "|    agent/train/clip_fraction         | 0.333       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.78       |\n",
      "|    agent/train/explained_variance    | 0.853       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0417     |\n",
      "|    agent/train/n_updates             | 120         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0128     |\n",
      "|    agent/train/std                   | 0.971       |\n",
      "|    agent/train/value_loss            | 0.0164      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.41       |\n",
      "|    agent/time/fps                    | 728         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 11          |\n",
      "|    agent/time/total_timesteps        | 28672       |\n",
      "|    agent/train/approx_kl             | 0.011200734 |\n",
      "|    agent/train/clip_fraction         | 0.34        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.77       |\n",
      "|    agent/train/explained_variance    | 0.881       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0484     |\n",
      "|    agent/train/n_updates             | 130         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0139     |\n",
      "|    agent/train/std                   | 0.961       |\n",
      "|    agent/train/value_loss            | 0.0141      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.33       |\n",
      "|    agent/time/fps                    | 717         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 14          |\n",
      "|    agent/time/total_timesteps        | 30720       |\n",
      "|    agent/train/approx_kl             | 0.010952804 |\n",
      "|    agent/train/clip_fraction         | 0.331       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.74       |\n",
      "|    agent/train/explained_variance    | 0.898       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0556     |\n",
      "|    agent/train/n_updates             | 140         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0132     |\n",
      "|    agent/train/std                   | 0.945       |\n",
      "|    agent/train/value_loss            | 0.011       |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -61.9    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -6.24    |\n",
      "|    agent/time/fps                      | 792      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 8        |\n",
      "|    agent/time/total_timesteps          | 2.66e+04 |\n",
      "|    agent/train/approx_kl               | 0.00947  |\n",
      "|    agent/train/clip_fraction           | 0.319    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.75    |\n",
      "|    agent/train/explained_variance      | 0.864    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0455  |\n",
      "|    agent/train/n_updates               | 130      |\n",
      "|    agent/train/policy_gradient_loss    | -0.0125  |\n",
      "|    agent/train/std                     | 0.957    |\n",
      "|    agent/train/value_loss              | 0.0155   |\n",
      "|    preferences/entropy                 | 0.194    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.656    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.182    |\n",
      "|    reward/epoch-0/train/loss           | 0.631    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.691    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.18     |\n",
      "|    reward/epoch-1/train/loss           | 0.597    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.699    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.18     |\n",
      "|    reward/epoch-2/train/loss           | 0.57     |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.699    |\n",
      "|    final/train/gt_reward_loss          | 0.18     |\n",
      "|    final/train/loss                    | 0.57     |\n",
      "-----------------------------------------------------\n",
      "Collecting 170 fragments (8500 transitions)\n",
      "Sampling 425 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 370 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "986bdc0d674641f7948b7f75cf0182f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.2        |\n",
      "|    agent/time/fps                    | 1069        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 1           |\n",
      "|    agent/time/total_timesteps        | 32768       |\n",
      "|    agent/train/approx_kl             | 0.009024402 |\n",
      "|    agent/train/clip_fraction         | 0.307       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.71       |\n",
      "|    agent/train/explained_variance    | 0.873       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0274     |\n",
      "|    agent/train/n_updates             | 150         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0113     |\n",
      "|    agent/train/std                   | 0.937       |\n",
      "|    agent/train/value_loss            | 0.0136      |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61.8        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.33        |\n",
      "|    agent/time/fps                    | 847          |\n",
      "|    agent/time/iterations             | 2            |\n",
      "|    agent/time/time_elapsed           | 4            |\n",
      "|    agent/time/total_timesteps        | 34816        |\n",
      "|    agent/train/approx_kl             | 0.0072448896 |\n",
      "|    agent/train/clip_fraction         | 0.293        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.68        |\n",
      "|    agent/train/explained_variance    | 0.855        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0375      |\n",
      "|    agent/train/n_updates             | 160          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00982     |\n",
      "|    agent/train/std                   | 0.921        |\n",
      "|    agent/train/value_loss            | 0.0127       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.5       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.5        |\n",
      "|    agent/time/fps                    | 769         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 7           |\n",
      "|    agent/time/total_timesteps        | 36864       |\n",
      "|    agent/train/approx_kl             | 0.010130976 |\n",
      "|    agent/train/clip_fraction         | 0.323       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.66       |\n",
      "|    agent/train/explained_variance    | 0.88        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0621     |\n",
      "|    agent/train/n_updates             | 170         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0128     |\n",
      "|    agent/train/std                   | 0.912       |\n",
      "|    agent/train/value_loss            | 0.0135      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 50         |\n",
      "|    agent/rollout/ep_rew_mean         | -60.7      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.47      |\n",
      "|    agent/time/fps                    | 701        |\n",
      "|    agent/time/iterations             | 4          |\n",
      "|    agent/time/time_elapsed           | 11         |\n",
      "|    agent/time/total_timesteps        | 38912      |\n",
      "|    agent/train/approx_kl             | 0.01252255 |\n",
      "|    agent/train/clip_fraction         | 0.354      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -2.61      |\n",
      "|    agent/train/explained_variance    | 0.8        |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | -0.0282    |\n",
      "|    agent/train/n_updates             | 180        |\n",
      "|    agent/train/policy_gradient_loss  | -0.0131    |\n",
      "|    agent/train/std                   | 0.885      |\n",
      "|    agent/train/value_loss            | 0.0185     |\n",
      "-----------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -60.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.3        |\n",
      "|    agent/time/fps                    | 660         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 15          |\n",
      "|    agent/time/total_timesteps        | 40960       |\n",
      "|    agent/train/approx_kl             | 0.011094799 |\n",
      "|    agent/train/clip_fraction         | 0.333       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.57       |\n",
      "|    agent/train/explained_variance    | 0.841       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.05       |\n",
      "|    agent/train/n_updates             | 190         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0123     |\n",
      "|    agent/train/std                   | 0.872       |\n",
      "|    agent/train/value_loss            | 0.0137      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -61.3    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -6.36    |\n",
      "|    agent/time/fps                      | 809      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 7.6      |\n",
      "|    agent/time/total_timesteps          | 3.69e+04 |\n",
      "|    agent/train/approx_kl               | 0.0103   |\n",
      "|    agent/train/clip_fraction           | 0.333    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.61    |\n",
      "|    agent/train/explained_variance      | 0.84     |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0443  |\n",
      "|    agent/train/n_updates               | 180      |\n",
      "|    agent/train/policy_gradient_loss    | -0.0124  |\n",
      "|    agent/train/std                     | 0.889    |\n",
      "|    agent/train/value_loss              | 0.0139   |\n",
      "|    preferences/entropy                 | 0.141    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.668    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.201    |\n",
      "|    reward/epoch-0/train/loss           | 0.596    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.709    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.198    |\n",
      "|    reward/epoch-1/train/loss           | 0.574    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.708    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.186    |\n",
      "|    reward/epoch-2/train/loss           | 0.553    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.708    |\n",
      "|    final/train/gt_reward_loss          | 0.186    |\n",
      "|    final/train/loss                    | 0.553    |\n",
      "-----------------------------------------------------\n",
      "Collecting 146 fragments (7300 transitions)\n",
      "Sampling 365 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 445 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409d419835ac46bc96b5d9f7adfde42d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -60.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.21       |\n",
      "|    agent/time/fps                    | 1064        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 1           |\n",
      "|    agent/time/total_timesteps        | 43008       |\n",
      "|    agent/train/approx_kl             | 0.010549548 |\n",
      "|    agent/train/clip_fraction         | 0.362       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.53       |\n",
      "|    agent/train/explained_variance    | 0.826       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0438     |\n",
      "|    agent/train/n_updates             | 200         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0137     |\n",
      "|    agent/train/std                   | 0.855       |\n",
      "|    agent/train/value_loss            | 0.011       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -60.9       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.25       |\n",
      "|    agent/time/fps                    | 829         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 4           |\n",
      "|    agent/time/total_timesteps        | 45056       |\n",
      "|    agent/train/approx_kl             | 0.007407979 |\n",
      "|    agent/train/clip_fraction         | 0.306       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.5        |\n",
      "|    agent/train/explained_variance    | 0.836       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0371     |\n",
      "|    agent/train/n_updates             | 210         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00636    |\n",
      "|    agent/train/std                   | 0.844       |\n",
      "|    agent/train/value_loss            | 0.00847     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.41       |\n",
      "|    agent/time/fps                    | 759         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 8           |\n",
      "|    agent/time/total_timesteps        | 47104       |\n",
      "|    agent/train/approx_kl             | 0.010573678 |\n",
      "|    agent/train/clip_fraction         | 0.338       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.48       |\n",
      "|    agent/train/explained_variance    | 0.886       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0453     |\n",
      "|    agent/train/n_updates             | 220         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0111     |\n",
      "|    agent/train/std                   | 0.833       |\n",
      "|    agent/train/value_loss            | 0.0103      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -59.5       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.4        |\n",
      "|    agent/time/fps                    | 736         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 11          |\n",
      "|    agent/time/total_timesteps        | 49152       |\n",
      "|    agent/train/approx_kl             | 0.011930142 |\n",
      "|    agent/train/clip_fraction         | 0.373       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.42       |\n",
      "|    agent/train/explained_variance    | 0.933       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0237     |\n",
      "|    agent/train/n_updates             | 230         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0104     |\n",
      "|    agent/train/std                   | 0.81        |\n",
      "|    agent/train/value_loss            | 0.00781     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -58.1       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.34       |\n",
      "|    agent/time/fps                    | 710         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 14          |\n",
      "|    agent/time/total_timesteps        | 51200       |\n",
      "|    agent/train/approx_kl             | 0.011936274 |\n",
      "|    agent/train/clip_fraction         | 0.356       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.37       |\n",
      "|    agent/train/explained_variance    | 0.881       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0403     |\n",
      "|    agent/train/n_updates             | 240         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0112     |\n",
      "|    agent/train/std                   | 0.788       |\n",
      "|    agent/train/value_loss            | 0.0106      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -60      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -6.32    |\n",
      "|    agent/time/fps                      | 820      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 7.6      |\n",
      "|    agent/time/total_timesteps          | 4.71e+04 |\n",
      "|    agent/train/approx_kl               | 0.0107   |\n",
      "|    agent/train/clip_fraction           | 0.343    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.42    |\n",
      "|    agent/train/explained_variance      | 0.889    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0314  |\n",
      "|    agent/train/n_updates               | 230      |\n",
      "|    agent/train/policy_gradient_loss    | -0.0101  |\n",
      "|    agent/train/std                     | 0.809    |\n",
      "|    agent/train/value_loss              | 0.00954  |\n",
      "|    preferences/entropy                 | 0.194    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.691    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.196    |\n",
      "|    reward/epoch-0/train/loss           | 0.613    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.704    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.197    |\n",
      "|    reward/epoch-1/train/loss           | 0.587    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.702    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.197    |\n",
      "|    reward/epoch-2/train/loss           | 0.57     |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.702    |\n",
      "|    final/train/gt_reward_loss          | 0.197    |\n",
      "|    final/train/loss                    | 0.57     |\n",
      "-----------------------------------------------------\n",
      "Collecting 126 fragments (6300 transitions)\n",
      "Sampling 315 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 510 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea02aa539b24d0caec6a8acc2ea04c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -58.1       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.31       |\n",
      "|    agent/time/fps                    | 959         |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 2           |\n",
      "|    agent/time/total_timesteps        | 53248       |\n",
      "|    agent/train/approx_kl             | 0.011453337 |\n",
      "|    agent/train/clip_fraction         | 0.344       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.33       |\n",
      "|    agent/train/explained_variance    | 0.909       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0105     |\n",
      "|    agent/train/n_updates             | 250         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0113     |\n",
      "|    agent/train/std                   | 0.77        |\n",
      "|    agent/train/value_loss            | 0.0105      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -56.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.56       |\n",
      "|    agent/time/fps                    | 745         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 5           |\n",
      "|    agent/time/total_timesteps        | 55296       |\n",
      "|    agent/train/approx_kl             | 0.011115662 |\n",
      "|    agent/train/clip_fraction         | 0.344       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.28       |\n",
      "|    agent/train/explained_variance    | 0.894       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0441     |\n",
      "|    agent/train/n_updates             | 260         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0094     |\n",
      "|    agent/train/std                   | 0.756       |\n",
      "|    agent/train/value_loss            | 0.0123      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -55.3       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.64       |\n",
      "|    agent/time/fps                    | 729         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 8           |\n",
      "|    agent/time/total_timesteps        | 57344       |\n",
      "|    agent/train/approx_kl             | 0.011908604 |\n",
      "|    agent/train/clip_fraction         | 0.371       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.25       |\n",
      "|    agent/train/explained_variance    | 0.923       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0108     |\n",
      "|    agent/train/n_updates             | 270         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0123     |\n",
      "|    agent/train/std                   | 0.743       |\n",
      "|    agent/train/value_loss            | 0.0102      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -53.2       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.62       |\n",
      "|    agent/time/fps                    | 719         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 11          |\n",
      "|    agent/time/total_timesteps        | 59392       |\n",
      "|    agent/train/approx_kl             | 0.013305306 |\n",
      "|    agent/train/clip_fraction         | 0.369       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.21       |\n",
      "|    agent/train/explained_variance    | 0.847       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0156     |\n",
      "|    agent/train/n_updates             | 280         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00539    |\n",
      "|    agent/train/std                   | 0.725       |\n",
      "|    agent/train/value_loss            | 0.0168      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -52.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.42       |\n",
      "|    agent/time/fps                    | 711         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 14          |\n",
      "|    agent/time/total_timesteps        | 61440       |\n",
      "|    agent/train/approx_kl             | 0.014061918 |\n",
      "|    agent/train/clip_fraction         | 0.369       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.17       |\n",
      "|    agent/train/explained_variance    | 0.915       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0291     |\n",
      "|    agent/train/n_updates             | 290         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0109     |\n",
      "|    agent/train/std                   | 0.711       |\n",
      "|    agent/train/value_loss            | 0.016       |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -55.2    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -6.51    |\n",
      "|    agent/time/fps                      | 773      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 8        |\n",
      "|    agent/time/total_timesteps          | 5.73e+04 |\n",
      "|    agent/train/approx_kl               | 0.0125   |\n",
      "|    agent/train/clip_fraction           | 0.364    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.2     |\n",
      "|    agent/train/explained_variance      | 0.903    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0239  |\n",
      "|    agent/train/n_updates               | 280      |\n",
      "|    agent/train/policy_gradient_loss    | -0.00918 |\n",
      "|    agent/train/std                     | 0.725    |\n",
      "|    agent/train/value_loss              | 0.0129   |\n",
      "|    preferences/entropy                 | 0.132    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.708    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.186    |\n",
      "|    reward/epoch-0/train/loss           | 0.567    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.74     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.186    |\n",
      "|    reward/epoch-1/train/loss           | 0.533    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.749    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.186    |\n",
      "|    reward/epoch-2/train/loss           | 0.529    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.749    |\n",
      "|    final/train/gt_reward_loss          | 0.186    |\n",
      "|    final/train/loss                    | 0.529    |\n",
      "-----------------------------------------------------\n",
      "Query schedule: [50, 127, 102, 85, 73, 63]\n",
      "Collecting 100 fragments (5000 transitions)\n",
      "Requested 4750 transitions but only 0 in buffer. Sampling 4750 additional transitions.\n",
      "Sampling 250 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 50 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62502c125a5343cba903bb04b49c4191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "---------------------------------------------------\n",
      "| raw/                                 |          |\n",
      "|    agent/rollout/ep_len_mean         | 50       |\n",
      "|    agent/rollout/ep_rew_mean         | -60.2    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -28.7    |\n",
      "|    agent/time/fps                    | 1036     |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 1        |\n",
      "|    agent/time/total_timesteps        | 2048     |\n",
      "---------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -60.9        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -18.4        |\n",
      "|    agent/time/fps                    | 790          |\n",
      "|    agent/time/iterations             | 2            |\n",
      "|    agent/time/time_elapsed           | 5            |\n",
      "|    agent/time/total_timesteps        | 4096         |\n",
      "|    agent/train/approx_kl             | 0.0029134867 |\n",
      "|    agent/train/clip_fraction         | 0.136        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.85        |\n",
      "|    agent/train/explained_variance    | 0.0472       |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.0135       |\n",
      "|    agent/train/n_updates             | 10           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00395     |\n",
      "|    agent/train/std                   | 1.01         |\n",
      "|    agent/train/value_loss            | 0.293        |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61.4        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -10.4        |\n",
      "|    agent/time/fps                    | 744          |\n",
      "|    agent/time/iterations             | 3            |\n",
      "|    agent/time/time_elapsed           | 8            |\n",
      "|    agent/time/total_timesteps        | 6144         |\n",
      "|    agent/train/approx_kl             | 0.0054650316 |\n",
      "|    agent/train/clip_fraction         | 0.242        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.86        |\n",
      "|    agent/train/explained_variance    | 0.305        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.000211    |\n",
      "|    agent/train/n_updates             | 20           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0121      |\n",
      "|    agent/train/std                   | 1.01         |\n",
      "|    agent/train/value_loss            | 0.136        |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61.4        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.37        |\n",
      "|    agent/time/fps                    | 726          |\n",
      "|    agent/time/iterations             | 4            |\n",
      "|    agent/time/time_elapsed           | 11           |\n",
      "|    agent/time/total_timesteps        | 8192         |\n",
      "|    agent/train/approx_kl             | 0.0063331677 |\n",
      "|    agent/train/clip_fraction         | 0.254        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.85        |\n",
      "|    agent/train/explained_variance    | 0.578        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.00682     |\n",
      "|    agent/train/n_updates             | 30           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0139      |\n",
      "|    agent/train/std                   | 1.01         |\n",
      "|    agent/train/value_loss            | 0.0865       |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61.7        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.21        |\n",
      "|    agent/time/fps                    | 720          |\n",
      "|    agent/time/iterations             | 5            |\n",
      "|    agent/time/time_elapsed           | 14           |\n",
      "|    agent/time/total_timesteps        | 10240        |\n",
      "|    agent/train/approx_kl             | 0.0058319904 |\n",
      "|    agent/train/clip_fraction         | 0.258        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.83        |\n",
      "|    agent/train/explained_variance    | 0.511        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0129      |\n",
      "|    agent/train/n_updates             | 40           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0126      |\n",
      "|    agent/train/std                   | 0.998        |\n",
      "|    agent/train/value_loss            | 0.0581       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                   |          |\n",
      "|    agent/rollout/ep_len_mean            | 50       |\n",
      "|    agent/rollout/ep_rew_mean            | -61.1    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean    | -14      |\n",
      "|    agent/time/fps                       | 803      |\n",
      "|    agent/time/iterations                | 3        |\n",
      "|    agent/time/time_elapsed              | 7.8      |\n",
      "|    agent/time/total_timesteps           | 6.14e+03 |\n",
      "|    agent/train/approx_kl                | 0.00575  |\n",
      "|    agent/train/clip_fraction            | 0.231    |\n",
      "|    agent/train/clip_range               | 0.1      |\n",
      "|    agent/train/entropy_loss             | -2.84    |\n",
      "|    agent/train/explained_variance       | 0.378    |\n",
      "|    agent/train/learning_rate            | 0.002    |\n",
      "|    agent/train/loss                     | -0.00904 |\n",
      "|    agent/train/n_updates                | 30       |\n",
      "|    agent/train/policy_gradient_loss     | -0.0116  |\n",
      "|    agent/train/std                      | 1        |\n",
      "|    agent/train/value_loss               | 0.122    |\n",
      "|    preferences/entropy                  | 0.202    |\n",
      "|    reward/epoch-0/train/accuracy        | 0.481    |\n",
      "|    reward/epoch-0/train/gt_reward_loss  | 0.142    |\n",
      "|    reward/epoch-0/train/loss            | 1.65     |\n",
      "|    reward/epoch-1/train/accuracy        | 0.58     |\n",
      "|    reward/epoch-1/train/gt_reward_loss  | 0.153    |\n",
      "|    reward/epoch-1/train/loss            | 1.19     |\n",
      "|    reward/epoch-10/train/accuracy       | 0.736    |\n",
      "|    reward/epoch-10/train/gt_reward_loss | 0.154    |\n",
      "|    reward/epoch-10/train/loss           | 0.544    |\n",
      "|    reward/epoch-11/train/accuracy       | 0.755    |\n",
      "|    reward/epoch-11/train/gt_reward_loss | 0.15     |\n",
      "|    reward/epoch-11/train/loss           | 0.548    |\n",
      "|    reward/epoch-2/train/accuracy        | 0.571    |\n",
      "|    reward/epoch-2/train/gt_reward_loss  | 0.13     |\n",
      "|    reward/epoch-2/train/loss            | 1.06     |\n",
      "|    reward/epoch-3/train/accuracy        | 0.651    |\n",
      "|    reward/epoch-3/train/gt_reward_loss  | 0.119    |\n",
      "|    reward/epoch-3/train/loss            | 0.787    |\n",
      "|    reward/epoch-4/train/accuracy        | 0.649    |\n",
      "|    reward/epoch-4/train/gt_reward_loss  | 0.14     |\n",
      "|    reward/epoch-4/train/loss            | 0.759    |\n",
      "|    reward/epoch-5/train/accuracy        | 0.609    |\n",
      "|    reward/epoch-5/train/gt_reward_loss  | 0.152    |\n",
      "|    reward/epoch-5/train/loss            | 0.707    |\n",
      "|    reward/epoch-6/train/accuracy        | 0.658    |\n",
      "|    reward/epoch-6/train/gt_reward_loss  | 0.134    |\n",
      "|    reward/epoch-6/train/loss            | 0.599    |\n",
      "|    reward/epoch-7/train/accuracy        | 0.717    |\n",
      "|    reward/epoch-7/train/gt_reward_loss  | 0.129    |\n",
      "|    reward/epoch-7/train/loss            | 0.581    |\n",
      "|    reward/epoch-8/train/accuracy        | 0.646    |\n",
      "|    reward/epoch-8/train/gt_reward_loss  | 0.131    |\n",
      "|    reward/epoch-8/train/loss            | 0.595    |\n",
      "|    reward/epoch-9/train/accuracy        | 0.634    |\n",
      "|    reward/epoch-9/train/gt_reward_loss  | 0.131    |\n",
      "|    reward/epoch-9/train/loss            | 0.566    |\n",
      "| reward/                                 |          |\n",
      "|    final/train/accuracy                 | 0.755    |\n",
      "|    final/train/gt_reward_loss           | 0.15     |\n",
      "|    final/train/loss                     | 0.548    |\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting 254 fragments (12700 transitions)\n",
      "Requested 12065 transitions but only 10200 in buffer. Sampling 1865 additional transitions.\n",
      "Sampling 635 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 180 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d2718c01204f50aacbce3f4d9d0351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.74       |\n",
      "|    agent/time/fps                    | 1061        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 1           |\n",
      "|    agent/time/total_timesteps        | 12288       |\n",
      "|    agent/train/approx_kl             | 0.008199505 |\n",
      "|    agent/train/clip_fraction         | 0.268       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.82       |\n",
      "|    agent/train/explained_variance    | 0.447       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0387     |\n",
      "|    agent/train/n_updates             | 50          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0155     |\n",
      "|    agent/train/std                   | 0.992       |\n",
      "|    agent/train/value_loss            | 0.0372      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -62.3       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.44       |\n",
      "|    agent/time/fps                    | 864         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 4           |\n",
      "|    agent/time/total_timesteps        | 14336       |\n",
      "|    agent/train/approx_kl             | 0.006112291 |\n",
      "|    agent/train/clip_fraction         | 0.247       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.83       |\n",
      "|    agent/train/explained_variance    | 0.174       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0359     |\n",
      "|    agent/train/n_updates             | 60          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0116     |\n",
      "|    agent/train/std                   | 0.994       |\n",
      "|    agent/train/value_loss            | 0.0579      |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -62.4        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.31        |\n",
      "|    agent/time/fps                    | 804          |\n",
      "|    agent/time/iterations             | 3            |\n",
      "|    agent/time/time_elapsed           | 7            |\n",
      "|    agent/time/total_timesteps        | 16384        |\n",
      "|    agent/train/approx_kl             | 0.0068252003 |\n",
      "|    agent/train/clip_fraction         | 0.241        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.82        |\n",
      "|    agent/train/explained_variance    | 0.625        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0207      |\n",
      "|    agent/train/n_updates             | 70           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0111      |\n",
      "|    agent/train/std                   | 0.989        |\n",
      "|    agent/train/value_loss            | 0.0438       |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -62.7        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.6         |\n",
      "|    agent/time/fps                    | 778          |\n",
      "|    agent/time/iterations             | 4            |\n",
      "|    agent/time/time_elapsed           | 10           |\n",
      "|    agent/time/total_timesteps        | 18432        |\n",
      "|    agent/train/approx_kl             | 0.0066386443 |\n",
      "|    agent/train/clip_fraction         | 0.286        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.8         |\n",
      "|    agent/train/explained_variance    | 0.738        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0268      |\n",
      "|    agent/train/n_updates             | 80           |\n",
      "|    agent/train/policy_gradient_loss  | -0.014       |\n",
      "|    agent/train/std                   | 0.98         |\n",
      "|    agent/train/value_loss            | 0.037        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.46       |\n",
      "|    agent/time/fps                    | 754         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 13          |\n",
      "|    agent/time/total_timesteps        | 20480       |\n",
      "|    agent/train/approx_kl             | 0.008157168 |\n",
      "|    agent/train/clip_fraction         | 0.312       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.78       |\n",
      "|    agent/train/explained_variance    | 0.65        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0287     |\n",
      "|    agent/train/n_updates             | 90          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0158     |\n",
      "|    agent/train/std                   | 0.972       |\n",
      "|    agent/train/value_loss            | 0.0273      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -62.2    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -6.91    |\n",
      "|    agent/time/fps                      | 852      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 7        |\n",
      "|    agent/time/total_timesteps          | 1.64e+04 |\n",
      "|    agent/train/approx_kl               | 0.00749  |\n",
      "|    agent/train/clip_fraction           | 0.28     |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.8     |\n",
      "|    agent/train/explained_variance      | 0.6      |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0308  |\n",
      "|    agent/train/n_updates               | 80       |\n",
      "|    agent/train/policy_gradient_loss    | -0.0133  |\n",
      "|    agent/train/std                     | 0.979    |\n",
      "|    agent/train/value_loss              | 0.0379   |\n",
      "|    preferences/entropy                 | 0.136    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.691    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.121    |\n",
      "|    reward/epoch-0/train/loss           | 0.598    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.689    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.119    |\n",
      "|    reward/epoch-1/train/loss           | 0.585    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.707    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.12     |\n",
      "|    reward/epoch-2/train/loss           | 0.544    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.707    |\n",
      "|    final/train/gt_reward_loss          | 0.12     |\n",
      "|    final/train/loss                    | 0.544    |\n",
      "-----------------------------------------------------\n",
      "Collecting 204 fragments (10200 transitions)\n",
      "Sampling 510 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 285 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7bcf855aebe4cd3b15624409b71edaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.3       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.27       |\n",
      "|    agent/time/fps                    | 1032        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 1           |\n",
      "|    agent/time/total_timesteps        | 22528       |\n",
      "|    agent/train/approx_kl             | 0.009716862 |\n",
      "|    agent/train/clip_fraction         | 0.313       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.76       |\n",
      "|    agent/train/explained_variance    | 0.814       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0418     |\n",
      "|    agent/train/n_updates             | 100         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0143     |\n",
      "|    agent/train/std                   | 0.961       |\n",
      "|    agent/train/value_loss            | 0.0236      |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61.1        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.61        |\n",
      "|    agent/time/fps                    | 787          |\n",
      "|    agent/time/iterations             | 2            |\n",
      "|    agent/time/time_elapsed           | 5            |\n",
      "|    agent/time/total_timesteps        | 24576        |\n",
      "|    agent/train/approx_kl             | 0.0066527943 |\n",
      "|    agent/train/clip_fraction         | 0.278        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.72        |\n",
      "|    agent/train/explained_variance    | 0.802        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0415      |\n",
      "|    agent/train/n_updates             | 110          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0107      |\n",
      "|    agent/train/std                   | 0.943        |\n",
      "|    agent/train/value_loss            | 0.0268       |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.91        |\n",
      "|    agent/time/fps                    | 747          |\n",
      "|    agent/time/iterations             | 3            |\n",
      "|    agent/time/time_elapsed           | 8            |\n",
      "|    agent/time/total_timesteps        | 26624        |\n",
      "|    agent/train/approx_kl             | 0.0101780305 |\n",
      "|    agent/train/clip_fraction         | 0.315        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.69        |\n",
      "|    agent/train/explained_variance    | 0.893        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0429      |\n",
      "|    agent/train/n_updates             | 120          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0141      |\n",
      "|    agent/train/std                   | 0.927        |\n",
      "|    agent/train/value_loss            | 0.0186       |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -59.7        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -8.07        |\n",
      "|    agent/time/fps                    | 614          |\n",
      "|    agent/time/iterations             | 4            |\n",
      "|    agent/time/time_elapsed           | 13           |\n",
      "|    agent/time/total_timesteps        | 28672        |\n",
      "|    agent/train/approx_kl             | 0.0092061935 |\n",
      "|    agent/train/clip_fraction         | 0.309        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.65        |\n",
      "|    agent/train/explained_variance    | 0.896        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0411      |\n",
      "|    agent/train/n_updates             | 130          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0137      |\n",
      "|    agent/train/std                   | 0.906        |\n",
      "|    agent/train/value_loss            | 0.0157       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -59.3       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.87       |\n",
      "|    agent/time/fps                    | 613         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 16          |\n",
      "|    agent/time/total_timesteps        | 30720       |\n",
      "|    agent/train/approx_kl             | 0.007621278 |\n",
      "|    agent/train/clip_fraction         | 0.28        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.62       |\n",
      "|    agent/train/explained_variance    | 0.919       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0145     |\n",
      "|    agent/train/n_updates             | 140         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0122     |\n",
      "|    agent/train/std                   | 0.893       |\n",
      "|    agent/train/value_loss            | 0.0149      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -60.5    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -7.75    |\n",
      "|    agent/time/fps                      | 759      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 8.6      |\n",
      "|    agent/time/total_timesteps          | 2.66e+04 |\n",
      "|    agent/train/approx_kl               | 0.00878  |\n",
      "|    agent/train/clip_fraction           | 0.3      |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.65    |\n",
      "|    agent/train/explained_variance      | 0.89     |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0363  |\n",
      "|    agent/train/n_updates               | 130      |\n",
      "|    agent/train/policy_gradient_loss    | -0.013   |\n",
      "|    agent/train/std                     | 0.908    |\n",
      "|    agent/train/value_loss              | 0.0176   |\n",
      "|    preferences/entropy                 | 0.179    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.699    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.123    |\n",
      "|    reward/epoch-0/train/loss           | 0.576    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.733    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.124    |\n",
      "|    reward/epoch-1/train/loss           | 0.536    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.734    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.122    |\n",
      "|    reward/epoch-2/train/loss           | 0.515    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.734    |\n",
      "|    final/train/gt_reward_loss          | 0.122    |\n",
      "|    final/train/loss                    | 0.515    |\n",
      "-----------------------------------------------------\n",
      "Collecting 170 fragments (8500 transitions)\n",
      "Sampling 425 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 370 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5089a61b086647cd8227861de6b0648a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -58.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.84       |\n",
      "|    agent/time/fps                    | 951         |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 2           |\n",
      "|    agent/time/total_timesteps        | 32768       |\n",
      "|    agent/train/approx_kl             | 0.010233745 |\n",
      "|    agent/train/clip_fraction         | 0.316       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.58       |\n",
      "|    agent/train/explained_variance    | 0.941       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0417     |\n",
      "|    agent/train/n_updates             | 150         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0143     |\n",
      "|    agent/train/std                   | 0.869       |\n",
      "|    agent/train/value_loss            | 0.0122      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -57.5       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -8.04       |\n",
      "|    agent/time/fps                    | 762         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 5           |\n",
      "|    agent/time/total_timesteps        | 34816       |\n",
      "|    agent/train/approx_kl             | 0.007824227 |\n",
      "|    agent/train/clip_fraction         | 0.295       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.52       |\n",
      "|    agent/train/explained_variance    | 0.898       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0292     |\n",
      "|    agent/train/n_updates             | 160         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0101     |\n",
      "|    agent/train/std                   | 0.85        |\n",
      "|    agent/train/value_loss            | 0.018       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -56.5       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -8.22       |\n",
      "|    agent/time/fps                    | 693         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 8           |\n",
      "|    agent/time/total_timesteps        | 36864       |\n",
      "|    agent/train/approx_kl             | 0.010652633 |\n",
      "|    agent/train/clip_fraction         | 0.344       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.47       |\n",
      "|    agent/train/explained_variance    | 0.917       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0483     |\n",
      "|    agent/train/n_updates             | 170         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0147     |\n",
      "|    agent/train/std                   | 0.829       |\n",
      "|    agent/train/value_loss            | 0.0116      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -55.1       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -8.2        |\n",
      "|    agent/time/fps                    | 673         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 12          |\n",
      "|    agent/time/total_timesteps        | 38912       |\n",
      "|    agent/train/approx_kl             | 0.012202168 |\n",
      "|    agent/train/clip_fraction         | 0.365       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.41       |\n",
      "|    agent/train/explained_variance    | 0.934       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.045      |\n",
      "|    agent/train/n_updates             | 180         |\n",
      "|    agent/train/policy_gradient_loss  | -0.017      |\n",
      "|    agent/train/std                   | 0.802       |\n",
      "|    agent/train/value_loss            | 0.014       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -54.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.99       |\n",
      "|    agent/time/fps                    | 577         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 17          |\n",
      "|    agent/time/total_timesteps        | 40960       |\n",
      "|    agent/train/approx_kl             | 0.012560564 |\n",
      "|    agent/train/clip_fraction         | 0.365       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.35       |\n",
      "|    agent/train/explained_variance    | 0.926       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0463     |\n",
      "|    agent/train/n_updates             | 190         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0136     |\n",
      "|    agent/train/std                   | 0.781       |\n",
      "|    agent/train/value_loss            | 0.0111      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -56.4    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -8.06    |\n",
      "|    agent/time/fps                      | 731      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 8.8      |\n",
      "|    agent/time/total_timesteps          | 3.69e+04 |\n",
      "|    agent/train/approx_kl               | 0.0114   |\n",
      "|    agent/train/clip_fraction           | 0.346    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.41    |\n",
      "|    agent/train/explained_variance      | 0.922    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0396  |\n",
      "|    agent/train/n_updates               | 180      |\n",
      "|    agent/train/policy_gradient_loss    | -0.0136  |\n",
      "|    agent/train/std                     | 0.805    |\n",
      "|    agent/train/value_loss              | 0.013    |\n",
      "|    preferences/entropy                 | 0.171    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.737    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.16     |\n",
      "|    reward/epoch-0/train/loss           | 0.512    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.79     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.158    |\n",
      "|    reward/epoch-1/train/loss           | 0.481    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.781    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.147    |\n",
      "|    reward/epoch-2/train/loss           | 0.469    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.781    |\n",
      "|    final/train/gt_reward_loss          | 0.147    |\n",
      "|    final/train/loss                    | 0.469    |\n",
      "-----------------------------------------------------\n",
      "Collecting 146 fragments (7300 transitions)\n",
      "Sampling 365 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 445 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cad08f17fc74bad8c05811f314818aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 50         |\n",
      "|    agent/rollout/ep_rew_mean         | -53.6      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -8.03      |\n",
      "|    agent/time/fps                    | 949        |\n",
      "|    agent/time/iterations             | 1          |\n",
      "|    agent/time/time_elapsed           | 2          |\n",
      "|    agent/time/total_timesteps        | 43008      |\n",
      "|    agent/train/approx_kl             | 0.01363267 |\n",
      "|    agent/train/clip_fraction         | 0.359      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -2.31      |\n",
      "|    agent/train/explained_variance    | 0.936      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | -0.0291    |\n",
      "|    agent/train/n_updates             | 200        |\n",
      "|    agent/train/policy_gradient_loss  | -0.0128    |\n",
      "|    agent/train/std                   | 0.762      |\n",
      "|    agent/train/value_loss            | 0.0104     |\n",
      "-----------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -53         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -8.09       |\n",
      "|    agent/time/fps                    | 794         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 5           |\n",
      "|    agent/time/total_timesteps        | 45056       |\n",
      "|    agent/train/approx_kl             | 0.013545668 |\n",
      "|    agent/train/clip_fraction         | 0.333       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.24       |\n",
      "|    agent/train/explained_variance    | 0.952       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.04       |\n",
      "|    agent/train/n_updates             | 210         |\n",
      "|    agent/train/policy_gradient_loss  | -0.009      |\n",
      "|    agent/train/std                   | 0.739       |\n",
      "|    agent/train/value_loss            | 0.00752     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -52.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -8.12       |\n",
      "|    agent/time/fps                    | 732         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 8           |\n",
      "|    agent/time/total_timesteps        | 47104       |\n",
      "|    agent/train/approx_kl             | 0.010459674 |\n",
      "|    agent/train/clip_fraction         | 0.37        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.19       |\n",
      "|    agent/train/explained_variance    | 0.949       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0312     |\n",
      "|    agent/train/n_updates             | 220         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0113     |\n",
      "|    agent/train/std                   | 0.726       |\n",
      "|    agent/train/value_loss            | 0.00727     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -51.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -8.02       |\n",
      "|    agent/time/fps                    | 708         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 11          |\n",
      "|    agent/time/total_timesteps        | 49152       |\n",
      "|    agent/train/approx_kl             | 0.012544818 |\n",
      "|    agent/train/clip_fraction         | 0.365       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.15       |\n",
      "|    agent/train/explained_variance    | 0.956       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0402     |\n",
      "|    agent/train/n_updates             | 230         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0112     |\n",
      "|    agent/train/std                   | 0.707       |\n",
      "|    agent/train/value_loss            | 0.00696     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -50.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.91       |\n",
      "|    agent/time/fps                    | 692         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 14          |\n",
      "|    agent/time/total_timesteps        | 51200       |\n",
      "|    agent/train/approx_kl             | 0.014106227 |\n",
      "|    agent/train/clip_fraction         | 0.402       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.08       |\n",
      "|    agent/train/explained_variance    | 0.932       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0349     |\n",
      "|    agent/train/n_updates             | 240         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0121     |\n",
      "|    agent/train/std                   | 0.678       |\n",
      "|    agent/train/value_loss            | 0.0101      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -52.2    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -8.03    |\n",
      "|    agent/time/fps                      | 775      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 8        |\n",
      "|    agent/time/total_timesteps          | 4.71e+04 |\n",
      "|    agent/train/approx_kl               | 0.0129   |\n",
      "|    agent/train/clip_fraction           | 0.374    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.13    |\n",
      "|    agent/train/explained_variance      | 0.951    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.034   |\n",
      "|    agent/train/n_updates               | 230      |\n",
      "|    agent/train/policy_gradient_loss    | -0.0108  |\n",
      "|    agent/train/std                     | 0.701    |\n",
      "|    agent/train/value_loss              | 0.00754  |\n",
      "|    preferences/entropy                 | 0.188    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.756    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.146    |\n",
      "|    reward/epoch-0/train/loss           | 0.502    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.755    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.146    |\n",
      "|    reward/epoch-1/train/loss           | 0.495    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.759    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.146    |\n",
      "|    reward/epoch-2/train/loss           | 0.479    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.759    |\n",
      "|    final/train/gt_reward_loss          | 0.146    |\n",
      "|    final/train/loss                    | 0.479    |\n",
      "-----------------------------------------------------\n",
      "Collecting 126 fragments (6300 transitions)\n",
      "Sampling 315 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 510 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46498ee79ca44532b3653cef9a0a52cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -49.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.95       |\n",
      "|    agent/time/fps                    | 1025        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 1           |\n",
      "|    agent/time/total_timesteps        | 53248       |\n",
      "|    agent/train/approx_kl             | 0.013616441 |\n",
      "|    agent/train/clip_fraction         | 0.401       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2          |\n",
      "|    agent/train/explained_variance    | 0.965       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0238     |\n",
      "|    agent/train/n_updates             | 250         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0107     |\n",
      "|    agent/train/std                   | 0.653       |\n",
      "|    agent/train/value_loss            | 0.00585     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -48.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -8.41       |\n",
      "|    agent/time/fps                    | 713         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 5           |\n",
      "|    agent/time/total_timesteps        | 55296       |\n",
      "|    agent/train/approx_kl             | 0.012786658 |\n",
      "|    agent/train/clip_fraction         | 0.349       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.95       |\n",
      "|    agent/train/explained_variance    | 0.933       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0128     |\n",
      "|    agent/train/n_updates             | 260         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00767    |\n",
      "|    agent/train/std                   | 0.637       |\n",
      "|    agent/train/value_loss            | 0.0136      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -47         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -8.62       |\n",
      "|    agent/time/fps                    | 667         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 9           |\n",
      "|    agent/time/total_timesteps        | 57344       |\n",
      "|    agent/train/approx_kl             | 0.012935513 |\n",
      "|    agent/train/clip_fraction         | 0.387       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.88       |\n",
      "|    agent/train/explained_variance    | 0.938       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.037      |\n",
      "|    agent/train/n_updates             | 270         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00842    |\n",
      "|    agent/train/std                   | 0.617       |\n",
      "|    agent/train/value_loss            | 0.00906     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -45.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -8.51       |\n",
      "|    agent/time/fps                    | 633         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 12          |\n",
      "|    agent/time/total_timesteps        | 59392       |\n",
      "|    agent/train/approx_kl             | 0.015510622 |\n",
      "|    agent/train/clip_fraction         | 0.391       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.82       |\n",
      "|    agent/train/explained_variance    | 0.946       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0191     |\n",
      "|    agent/train/n_updates             | 280         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0041     |\n",
      "|    agent/train/std                   | 0.601       |\n",
      "|    agent/train/value_loss            | 0.00963     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -44.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -8.3        |\n",
      "|    agent/time/fps                    | 638         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 16          |\n",
      "|    agent/time/total_timesteps        | 61440       |\n",
      "|    agent/train/approx_kl             | 0.016546344 |\n",
      "|    agent/train/clip_fraction         | 0.397       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.75       |\n",
      "|    agent/train/explained_variance    | 0.956       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.000203   |\n",
      "|    agent/train/n_updates             | 290         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0109     |\n",
      "|    agent/train/std                   | 0.579       |\n",
      "|    agent/train/value_loss            | 0.00853     |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -47      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -8.36    |\n",
      "|    agent/time/fps                      | 735      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 8.6      |\n",
      "|    agent/time/total_timesteps          | 5.73e+04 |\n",
      "|    agent/train/approx_kl               | 0.0142   |\n",
      "|    agent/train/clip_fraction           | 0.383    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.82    |\n",
      "|    agent/train/explained_variance      | 0.945    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0193  |\n",
      "|    agent/train/n_updates               | 280      |\n",
      "|    agent/train/policy_gradient_loss    | -0.00827 |\n",
      "|    agent/train/std                     | 0.6      |\n",
      "|    agent/train/value_loss              | 0.00995  |\n",
      "|    preferences/entropy                 | 0.152    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.775    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.154    |\n",
      "|    reward/epoch-0/train/loss           | 0.469    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.79     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.155    |\n",
      "|    reward/epoch-1/train/loss           | 0.457    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.808    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.154    |\n",
      "|    reward/epoch-2/train/loss           | 0.438    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.808    |\n",
      "|    final/train/gt_reward_loss          | 0.154    |\n",
      "|    final/train/loss                    | 0.438    |\n",
      "-----------------------------------------------------\n",
      "Query schedule: [50, 127, 102, 85, 73, 63]\n",
      "Collecting 100 fragments (5000 transitions)\n",
      "Requested 4750 transitions but only 0 in buffer. Sampling 4750 additional transitions.\n",
      "Sampling 250 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 50 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0872ca1343436bb8a9003ae5c5124f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "---------------------------------------------------\n",
      "| raw/                                 |          |\n",
      "|    agent/rollout/ep_len_mean         | 50       |\n",
      "|    agent/rollout/ep_rew_mean         | -60.2    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -28.7    |\n",
      "|    agent/time/fps                    | 695      |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 2        |\n",
      "|    agent/time/total_timesteps        | 2048     |\n",
      "---------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -60.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -18.2       |\n",
      "|    agent/time/fps                    | 581         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 7           |\n",
      "|    agent/time/total_timesteps        | 4096        |\n",
      "|    agent/train/approx_kl             | 0.002032823 |\n",
      "|    agent/train/clip_fraction         | 0.103       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.84       |\n",
      "|    agent/train/explained_variance    | -0.192      |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.00576    |\n",
      "|    agent/train/n_updates             | 10          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00247    |\n",
      "|    agent/train/std                   | 1.01        |\n",
      "|    agent/train/value_loss            | 0.215       |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61.3        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -9.84        |\n",
      "|    agent/time/fps                    | 597          |\n",
      "|    agent/time/iterations             | 3            |\n",
      "|    agent/time/time_elapsed           | 10           |\n",
      "|    agent/time/total_timesteps        | 6144         |\n",
      "|    agent/train/approx_kl             | 0.0036482543 |\n",
      "|    agent/train/clip_fraction         | 0.17         |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.85        |\n",
      "|    agent/train/explained_variance    | 0.366        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.00153     |\n",
      "|    agent/train/n_updates             | 20           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00804     |\n",
      "|    agent/train/std                   | 1.01         |\n",
      "|    agent/train/value_loss            | 0.0862       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.1       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.89       |\n",
      "|    agent/time/fps                    | 621         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 13          |\n",
      "|    agent/time/total_timesteps        | 8192        |\n",
      "|    agent/train/approx_kl             | 0.004570929 |\n",
      "|    agent/train/clip_fraction         | 0.209       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.84       |\n",
      "|    agent/train/explained_variance    | 0.625       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0324     |\n",
      "|    agent/train/n_updates             | 30          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00898    |\n",
      "|    agent/train/std                   | 1           |\n",
      "|    agent/train/value_loss            | 0.054       |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61.2        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.86        |\n",
      "|    agent/time/fps                    | 628          |\n",
      "|    agent/time/iterations             | 5            |\n",
      "|    agent/time/time_elapsed           | 16           |\n",
      "|    agent/time/total_timesteps        | 10240        |\n",
      "|    agent/train/approx_kl             | 0.0055710673 |\n",
      "|    agent/train/clip_fraction         | 0.245        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.83        |\n",
      "|    agent/train/explained_variance    | 0.607        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0326      |\n",
      "|    agent/train/n_updates             | 40           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0114      |\n",
      "|    agent/train/std                   | 1            |\n",
      "|    agent/train/value_loss            | 0.053        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                   |          |\n",
      "|    agent/rollout/ep_len_mean            | 50       |\n",
      "|    agent/rollout/ep_rew_mean            | -60.9    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean    | -13.7    |\n",
      "|    agent/time/fps                       | 624      |\n",
      "|    agent/time/iterations                | 3        |\n",
      "|    agent/time/time_elapsed              | 9.6      |\n",
      "|    agent/time/total_timesteps           | 6.14e+03 |\n",
      "|    agent/train/approx_kl                | 0.0043   |\n",
      "|    agent/train/clip_fraction            | 0.196    |\n",
      "|    agent/train/clip_range               | 0.1      |\n",
      "|    agent/train/entropy_loss             | -2.84    |\n",
      "|    agent/train/explained_variance       | 0.388    |\n",
      "|    agent/train/learning_rate            | 0.002    |\n",
      "|    agent/train/loss                     | -0.0205  |\n",
      "|    agent/train/n_updates                | 30       |\n",
      "|    agent/train/policy_gradient_loss     | -0.00855 |\n",
      "|    agent/train/std                      | 1        |\n",
      "|    agent/train/value_loss               | 0.0883   |\n",
      "|    preferences/entropy                  | 0.229    |\n",
      "|    reward/epoch-0/train/accuracy        | 0.516    |\n",
      "|    reward/epoch-0/train/gt_reward_loss  | 0.198    |\n",
      "|    reward/epoch-0/train/loss            | 1.4      |\n",
      "|    reward/epoch-1/train/accuracy        | 0.559    |\n",
      "|    reward/epoch-1/train/gt_reward_loss  | 0.217    |\n",
      "|    reward/epoch-1/train/loss            | 1.18     |\n",
      "|    reward/epoch-10/train/accuracy       | 0.76     |\n",
      "|    reward/epoch-10/train/gt_reward_loss | 0.213    |\n",
      "|    reward/epoch-10/train/loss           | 0.508    |\n",
      "|    reward/epoch-11/train/accuracy       | 0.727    |\n",
      "|    reward/epoch-11/train/gt_reward_loss | 0.231    |\n",
      "|    reward/epoch-11/train/loss           | 0.544    |\n",
      "|    reward/epoch-2/train/accuracy        | 0.611    |\n",
      "|    reward/epoch-2/train/gt_reward_loss  | 0.211    |\n",
      "|    reward/epoch-2/train/loss            | 0.904    |\n",
      "|    reward/epoch-3/train/accuracy        | 0.556    |\n",
      "|    reward/epoch-3/train/gt_reward_loss  | 0.196    |\n",
      "|    reward/epoch-3/train/loss            | 0.786    |\n",
      "|    reward/epoch-4/train/accuracy        | 0.488    |\n",
      "|    reward/epoch-4/train/gt_reward_loss  | 0.255    |\n",
      "|    reward/epoch-4/train/loss            | 0.724    |\n",
      "|    reward/epoch-5/train/accuracy        | 0.611    |\n",
      "|    reward/epoch-5/train/gt_reward_loss  | 0.218    |\n",
      "|    reward/epoch-5/train/loss            | 0.662    |\n",
      "|    reward/epoch-6/train/accuracy        | 0.609    |\n",
      "|    reward/epoch-6/train/gt_reward_loss  | 0.193    |\n",
      "|    reward/epoch-6/train/loss            | 0.666    |\n",
      "|    reward/epoch-7/train/accuracy        | 0.717    |\n",
      "|    reward/epoch-7/train/gt_reward_loss  | 0.23     |\n",
      "|    reward/epoch-7/train/loss            | 0.571    |\n",
      "|    reward/epoch-8/train/accuracy        | 0.677    |\n",
      "|    reward/epoch-8/train/gt_reward_loss  | 0.202    |\n",
      "|    reward/epoch-8/train/loss            | 0.587    |\n",
      "|    reward/epoch-9/train/accuracy        | 0.76     |\n",
      "|    reward/epoch-9/train/gt_reward_loss  | 0.235    |\n",
      "|    reward/epoch-9/train/loss            | 0.543    |\n",
      "| reward/                                 |          |\n",
      "|    final/train/accuracy                 | 0.727    |\n",
      "|    final/train/gt_reward_loss           | 0.231    |\n",
      "|    final/train/loss                     | 0.544    |\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting 254 fragments (12700 transitions)\n",
      "Requested 12065 transitions but only 10200 in buffer. Sampling 1865 additional transitions.\n",
      "Sampling 635 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 180 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d4b2d7807b4e988d6f9aa8cb12c7e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -60.9       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.52       |\n",
      "|    agent/time/fps                    | 846         |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 2           |\n",
      "|    agent/time/total_timesteps        | 12288       |\n",
      "|    agent/train/approx_kl             | 0.005668029 |\n",
      "|    agent/train/clip_fraction         | 0.252       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.83       |\n",
      "|    agent/train/explained_variance    | 0.537       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0302     |\n",
      "|    agent/train/n_updates             | 50          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0119     |\n",
      "|    agent/train/std                   | 0.998       |\n",
      "|    agent/train/value_loss            | 0.0334      |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -62          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.05        |\n",
      "|    agent/time/fps                    | 704          |\n",
      "|    agent/time/iterations             | 2            |\n",
      "|    agent/time/time_elapsed           | 5            |\n",
      "|    agent/time/total_timesteps        | 14336        |\n",
      "|    agent/train/approx_kl             | 0.0057184007 |\n",
      "|    agent/train/clip_fraction         | 0.242        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.83        |\n",
      "|    agent/train/explained_variance    | 0.372        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0266      |\n",
      "|    agent/train/n_updates             | 60           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00899     |\n",
      "|    agent/train/std                   | 1            |\n",
      "|    agent/train/value_loss            | 0.0419       |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -62.1        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.59        |\n",
      "|    agent/time/fps                    | 672          |\n",
      "|    agent/time/iterations             | 3            |\n",
      "|    agent/time/time_elapsed           | 9            |\n",
      "|    agent/time/total_timesteps        | 16384        |\n",
      "|    agent/train/approx_kl             | 0.0066964375 |\n",
      "|    agent/train/clip_fraction         | 0.284        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.83        |\n",
      "|    agent/train/explained_variance    | 0.493        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0202      |\n",
      "|    agent/train/n_updates             | 70           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0132      |\n",
      "|    agent/train/std                   | 0.992        |\n",
      "|    agent/train/value_loss            | 0.0487       |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -62.3        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.69        |\n",
      "|    agent/time/fps                    | 635          |\n",
      "|    agent/time/iterations             | 4            |\n",
      "|    agent/time/time_elapsed           | 12           |\n",
      "|    agent/time/total_timesteps        | 18432        |\n",
      "|    agent/train/approx_kl             | 0.0072585028 |\n",
      "|    agent/train/clip_fraction         | 0.298        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.8         |\n",
      "|    agent/train/explained_variance    | 0.675        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0447      |\n",
      "|    agent/train/n_updates             | 80           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0154      |\n",
      "|    agent/train/std                   | 0.978        |\n",
      "|    agent/train/value_loss            | 0.037        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.1       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.41       |\n",
      "|    agent/time/fps                    | 585         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 17          |\n",
      "|    agent/time/total_timesteps        | 20480       |\n",
      "|    agent/train/approx_kl             | 0.010053178 |\n",
      "|    agent/train/clip_fraction         | 0.333       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.76       |\n",
      "|    agent/train/explained_variance    | 0.716       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0349     |\n",
      "|    agent/train/n_updates             | 90          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0156     |\n",
      "|    agent/train/std                   | 0.958       |\n",
      "|    agent/train/value_loss            | 0.0158      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -61.7    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -6.25    |\n",
      "|    agent/time/fps                      | 688      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 9        |\n",
      "|    agent/time/total_timesteps          | 1.64e+04 |\n",
      "|    agent/train/approx_kl               | 0.00763  |\n",
      "|    agent/train/clip_fraction           | 0.295    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.79    |\n",
      "|    agent/train/explained_variance      | 0.609    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0274  |\n",
      "|    agent/train/n_updates               | 80       |\n",
      "|    agent/train/policy_gradient_loss    | -0.0134  |\n",
      "|    agent/train/std                     | 0.975    |\n",
      "|    agent/train/value_loss              | 0.0333   |\n",
      "|    preferences/entropy                 | 0.163    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.591    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.202    |\n",
      "|    reward/epoch-0/train/loss           | 0.696    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.702    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.199    |\n",
      "|    reward/epoch-1/train/loss           | 0.61     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.705    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.199    |\n",
      "|    reward/epoch-2/train/loss           | 0.584    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.705    |\n",
      "|    final/train/gt_reward_loss          | 0.199    |\n",
      "|    final/train/loss                    | 0.584    |\n",
      "-----------------------------------------------------\n",
      "Collecting 204 fragments (10200 transitions)\n",
      "Sampling 510 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 285 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a65d903b684061813d4f43b4e55356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -60.2       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.2        |\n",
      "|    agent/time/fps                    | 900         |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 2           |\n",
      "|    agent/time/total_timesteps        | 22528       |\n",
      "|    agent/train/approx_kl             | 0.008439384 |\n",
      "|    agent/train/clip_fraction         | 0.317       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.74       |\n",
      "|    agent/train/explained_variance    | 0.788       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0108     |\n",
      "|    agent/train/n_updates             | 100         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0139     |\n",
      "|    agent/train/std                   | 0.948       |\n",
      "|    agent/train/value_loss            | 0.0231      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -60.1       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.5        |\n",
      "|    agent/time/fps                    | 785         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 5           |\n",
      "|    agent/time/total_timesteps        | 24576       |\n",
      "|    agent/train/approx_kl             | 0.007609043 |\n",
      "|    agent/train/clip_fraction         | 0.274       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.7        |\n",
      "|    agent/train/explained_variance    | 0.66        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0332     |\n",
      "|    agent/train/n_updates             | 110         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00979    |\n",
      "|    agent/train/std                   | 0.931       |\n",
      "|    agent/train/value_loss            | 0.027       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -60.3       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.58       |\n",
      "|    agent/time/fps                    | 729         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 8           |\n",
      "|    agent/time/total_timesteps        | 26624       |\n",
      "|    agent/train/approx_kl             | 0.009775296 |\n",
      "|    agent/train/clip_fraction         | 0.327       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.68       |\n",
      "|    agent/train/explained_variance    | 0.806       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0363     |\n",
      "|    agent/train/n_updates             | 120         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0139     |\n",
      "|    agent/train/std                   | 0.924       |\n",
      "|    agent/train/value_loss            | 0.0227      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -59.5       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.43       |\n",
      "|    agent/time/fps                    | 679         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 12          |\n",
      "|    agent/time/total_timesteps        | 28672       |\n",
      "|    agent/train/approx_kl             | 0.008521406 |\n",
      "|    agent/train/clip_fraction         | 0.32        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.65       |\n",
      "|    agent/train/explained_variance    | 0.819       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0336     |\n",
      "|    agent/train/n_updates             | 130         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0115     |\n",
      "|    agent/train/std                   | 0.908       |\n",
      "|    agent/train/value_loss            | 0.0189      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -59.2       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.1        |\n",
      "|    agent/time/fps                    | 674         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 15          |\n",
      "|    agent/time/total_timesteps        | 30720       |\n",
      "|    agent/train/approx_kl             | 0.011344567 |\n",
      "|    agent/train/clip_fraction         | 0.355       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.61       |\n",
      "|    agent/train/explained_variance    | 0.768       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0494     |\n",
      "|    agent/train/n_updates             | 140         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0151     |\n",
      "|    agent/train/std                   | 0.892       |\n",
      "|    agent/train/value_loss            | 0.0206      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -59.9    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -6.36    |\n",
      "|    agent/time/fps                      | 753      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 8.4      |\n",
      "|    agent/time/total_timesteps          | 2.66e+04 |\n",
      "|    agent/train/approx_kl               | 0.00937  |\n",
      "|    agent/train/clip_fraction           | 0.322    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.64    |\n",
      "|    agent/train/explained_variance      | 0.774    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0405  |\n",
      "|    agent/train/n_updates               | 130      |\n",
      "|    agent/train/policy_gradient_loss    | -0.0131  |\n",
      "|    agent/train/std                     | 0.906    |\n",
      "|    agent/train/value_loss              | 0.0201   |\n",
      "|    preferences/entropy                 | 0.145    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.696    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.165    |\n",
      "|    reward/epoch-0/train/loss           | 0.59     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.691    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.166    |\n",
      "|    reward/epoch-1/train/loss           | 0.55     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.693    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.165    |\n",
      "|    reward/epoch-2/train/loss           | 0.525    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.693    |\n",
      "|    final/train/gt_reward_loss          | 0.165    |\n",
      "|    final/train/loss                    | 0.525    |\n",
      "-----------------------------------------------------\n",
      "Collecting 170 fragments (8500 transitions)\n",
      "Sampling 425 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 370 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2333319c2424a0caee4f8378b40859f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 50         |\n",
      "|    agent/rollout/ep_rew_mean         | -59.5      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6         |\n",
      "|    agent/time/fps                    | 914        |\n",
      "|    agent/time/iterations             | 1          |\n",
      "|    agent/time/time_elapsed           | 2          |\n",
      "|    agent/time/total_timesteps        | 32768      |\n",
      "|    agent/train/approx_kl             | 0.00960023 |\n",
      "|    agent/train/clip_fraction         | 0.336      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -2.58      |\n",
      "|    agent/train/explained_variance    | 0.815      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | -0.0502    |\n",
      "|    agent/train/n_updates             | 150        |\n",
      "|    agent/train/policy_gradient_loss  | -0.0152    |\n",
      "|    agent/train/std                   | 0.872      |\n",
      "|    agent/train/value_loss            | 0.0114     |\n",
      "-----------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -59         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.39       |\n",
      "|    agent/time/fps                    | 802         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 5           |\n",
      "|    agent/time/total_timesteps        | 34816       |\n",
      "|    agent/train/approx_kl             | 0.006337661 |\n",
      "|    agent/train/clip_fraction         | 0.234       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.56       |\n",
      "|    agent/train/explained_variance    | 0.696       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0509     |\n",
      "|    agent/train/n_updates             | 160         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00623    |\n",
      "|    agent/train/std                   | 0.869       |\n",
      "|    agent/train/value_loss            | 0.0113      |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -58.3        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.76        |\n",
      "|    agent/time/fps                    | 765          |\n",
      "|    agent/time/iterations             | 3            |\n",
      "|    agent/time/time_elapsed           | 8            |\n",
      "|    agent/time/total_timesteps        | 36864        |\n",
      "|    agent/train/approx_kl             | 0.0070128944 |\n",
      "|    agent/train/clip_fraction         | 0.282        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.53        |\n",
      "|    agent/train/explained_variance    | 0.894        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0473      |\n",
      "|    agent/train/n_updates             | 170          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0085      |\n",
      "|    agent/train/std                   | 0.851        |\n",
      "|    agent/train/value_loss            | 0.00994      |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -57.3       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.83       |\n",
      "|    agent/time/fps                    | 745         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 10          |\n",
      "|    agent/time/total_timesteps        | 38912       |\n",
      "|    agent/train/approx_kl             | 0.008823613 |\n",
      "|    agent/train/clip_fraction         | 0.307       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.48       |\n",
      "|    agent/train/explained_variance    | 0.927       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0366     |\n",
      "|    agent/train/n_updates             | 180         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0102     |\n",
      "|    agent/train/std                   | 0.831       |\n",
      "|    agent/train/value_loss            | 0.00683     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -58.3       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.71       |\n",
      "|    agent/time/fps                    | 718         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 14          |\n",
      "|    agent/time/total_timesteps        | 40960       |\n",
      "|    agent/train/approx_kl             | 0.010412844 |\n",
      "|    agent/train/clip_fraction         | 0.299       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.44       |\n",
      "|    agent/train/explained_variance    | 0.931       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0396     |\n",
      "|    agent/train/n_updates             | 190         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0111     |\n",
      "|    agent/train/std                   | 0.817       |\n",
      "|    agent/train/value_loss            | 0.00633     |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -58.5    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -6.54    |\n",
      "|    agent/time/fps                      | 789      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 7.8      |\n",
      "|    agent/time/total_timesteps          | 3.69e+04 |\n",
      "|    agent/train/approx_kl               | 0.0086   |\n",
      "|    agent/train/clip_fraction           | 0.294    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.48    |\n",
      "|    agent/train/explained_variance      | 0.882    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0445  |\n",
      "|    agent/train/n_updates               | 180      |\n",
      "|    agent/train/policy_gradient_loss    | -0.00939 |\n",
      "|    agent/train/std                     | 0.834    |\n",
      "|    agent/train/value_loss              | 0.00774  |\n",
      "|    preferences/entropy                 | 0.218    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.666    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.198    |\n",
      "|    reward/epoch-0/train/loss           | 0.572    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.694    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.185    |\n",
      "|    reward/epoch-1/train/loss           | 0.54     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.725    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.178    |\n",
      "|    reward/epoch-2/train/loss           | 0.519    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.725    |\n",
      "|    final/train/gt_reward_loss          | 0.178    |\n",
      "|    final/train/loss                    | 0.519    |\n",
      "-----------------------------------------------------\n",
      "Collecting 146 fragments (7300 transitions)\n",
      "Sampling 365 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 445 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a39f67e57ff4725a51be96a4fd52be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -58.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.75       |\n",
      "|    agent/time/fps                    | 957         |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 2           |\n",
      "|    agent/time/total_timesteps        | 43008       |\n",
      "|    agent/train/approx_kl             | 0.010389901 |\n",
      "|    agent/train/clip_fraction         | 0.349       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.4        |\n",
      "|    agent/train/explained_variance    | 0.962       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0479     |\n",
      "|    agent/train/n_updates             | 200         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0109     |\n",
      "|    agent/train/std                   | 0.803       |\n",
      "|    agent/train/value_loss            | 0.00425     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -59.2       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.51       |\n",
      "|    agent/time/fps                    | 749         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 5           |\n",
      "|    agent/time/total_timesteps        | 45056       |\n",
      "|    agent/train/approx_kl             | 0.008240357 |\n",
      "|    agent/train/clip_fraction         | 0.291       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.37       |\n",
      "|    agent/train/explained_variance    | 0.852       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0478     |\n",
      "|    agent/train/n_updates             | 210         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00736    |\n",
      "|    agent/train/std                   | 0.792       |\n",
      "|    agent/train/value_loss            | 0.00794     |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -58.9        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.34        |\n",
      "|    agent/time/fps                    | 723          |\n",
      "|    agent/time/iterations             | 3            |\n",
      "|    agent/time/time_elapsed           | 8            |\n",
      "|    agent/time/total_timesteps        | 47104        |\n",
      "|    agent/train/approx_kl             | 0.0085619725 |\n",
      "|    agent/train/clip_fraction         | 0.309        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.33        |\n",
      "|    agent/train/explained_variance    | 0.938        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0434      |\n",
      "|    agent/train/n_updates             | 220          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00847     |\n",
      "|    agent/train/std                   | 0.772        |\n",
      "|    agent/train/value_loss            | 0.00586      |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -57.5       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.93       |\n",
      "|    agent/time/fps                    | 718         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 11          |\n",
      "|    agent/time/total_timesteps        | 49152       |\n",
      "|    agent/train/approx_kl             | 0.008491475 |\n",
      "|    agent/train/clip_fraction         | 0.323       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.29       |\n",
      "|    agent/train/explained_variance    | 0.89        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.022      |\n",
      "|    agent/train/n_updates             | 230         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00697    |\n",
      "|    agent/train/std                   | 0.755       |\n",
      "|    agent/train/value_loss            | 0.0103      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -56.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.77       |\n",
      "|    agent/time/fps                    | 705         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 14          |\n",
      "|    agent/time/total_timesteps        | 51200       |\n",
      "|    agent/train/approx_kl             | 0.007734525 |\n",
      "|    agent/train/clip_fraction         | 0.303       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.24       |\n",
      "|    agent/train/explained_variance    | 0.942       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0295     |\n",
      "|    agent/train/n_updates             | 240         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0108     |\n",
      "|    agent/train/std                   | 0.74        |\n",
      "|    agent/train/value_loss            | 0.0123      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -58.2    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -6.26    |\n",
      "|    agent/time/fps                      | 770      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 8        |\n",
      "|    agent/time/total_timesteps          | 4.71e+04 |\n",
      "|    agent/train/approx_kl               | 0.00828  |\n",
      "|    agent/train/clip_fraction           | 0.306    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.28    |\n",
      "|    agent/train/explained_variance      | 0.915    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.03    |\n",
      "|    agent/train/n_updates               | 230      |\n",
      "|    agent/train/policy_gradient_loss    | -0.00826 |\n",
      "|    agent/train/std                     | 0.757    |\n",
      "|    agent/train/value_loss              | 0.00895  |\n",
      "|    preferences/entropy                 | 0.175    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.705    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.187    |\n",
      "|    reward/epoch-0/train/loss           | 0.535    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.701    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.188    |\n",
      "|    reward/epoch-1/train/loss           | 0.526    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.735    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.188    |\n",
      "|    reward/epoch-2/train/loss           | 0.512    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.735    |\n",
      "|    final/train/gt_reward_loss          | 0.188    |\n",
      "|    final/train/loss                    | 0.512    |\n",
      "-----------------------------------------------------\n",
      "Collecting 126 fragments (6300 transitions)\n",
      "Sampling 315 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 510 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff8d666cb5a46378b21b9a2fa3ce23d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -56.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.8        |\n",
      "|    agent/time/fps                    | 1044        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 1           |\n",
      "|    agent/time/total_timesteps        | 53248       |\n",
      "|    agent/train/approx_kl             | 0.008376967 |\n",
      "|    agent/train/clip_fraction         | 0.303       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.19       |\n",
      "|    agent/train/explained_variance    | 0.952       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.00727    |\n",
      "|    agent/train/n_updates             | 250         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00776    |\n",
      "|    agent/train/std                   | 0.726       |\n",
      "|    agent/train/value_loss            | 0.00835     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -55.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.92       |\n",
      "|    agent/time/fps                    | 775         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 5           |\n",
      "|    agent/time/total_timesteps        | 55296       |\n",
      "|    agent/train/approx_kl             | 0.008429438 |\n",
      "|    agent/train/clip_fraction         | 0.288       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.15       |\n",
      "|    agent/train/explained_variance    | 0.932       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0342     |\n",
      "|    agent/train/n_updates             | 260         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00673    |\n",
      "|    agent/train/std                   | 0.708       |\n",
      "|    agent/train/value_loss            | 0.0072      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -54.3       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.88       |\n",
      "|    agent/time/fps                    | 738         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 8           |\n",
      "|    agent/time/total_timesteps        | 57344       |\n",
      "|    agent/train/approx_kl             | 0.008814234 |\n",
      "|    agent/train/clip_fraction         | 0.294       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.11       |\n",
      "|    agent/train/explained_variance    | 0.967       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0359     |\n",
      "|    agent/train/n_updates             | 270         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00785    |\n",
      "|    agent/train/std                   | 0.694       |\n",
      "|    agent/train/value_loss            | 0.00546     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -51.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.61       |\n",
      "|    agent/time/fps                    | 717         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 11          |\n",
      "|    agent/time/total_timesteps        | 59392       |\n",
      "|    agent/train/approx_kl             | 0.008502988 |\n",
      "|    agent/train/clip_fraction         | 0.318       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.05       |\n",
      "|    agent/train/explained_variance    | 0.979       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0315     |\n",
      "|    agent/train/n_updates             | 280         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00918    |\n",
      "|    agent/train/std                   | 0.674       |\n",
      "|    agent/train/value_loss            | 0.00497     |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -51.5        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.33        |\n",
      "|    agent/time/fps                    | 703          |\n",
      "|    agent/time/iterations             | 5            |\n",
      "|    agent/time/time_elapsed           | 14           |\n",
      "|    agent/time/total_timesteps        | 61440        |\n",
      "|    agent/train/approx_kl             | 0.0090312045 |\n",
      "|    agent/train/clip_fraction         | 0.327        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2           |\n",
      "|    agent/train/explained_variance    | 0.983        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0436      |\n",
      "|    agent/train/n_updates             | 290          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0106      |\n",
      "|    agent/train/std                   | 0.653        |\n",
      "|    agent/train/value_loss            | 0.00434      |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -54      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -5.7     |\n",
      "|    agent/time/fps                      | 795      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 7.8      |\n",
      "|    agent/time/total_timesteps          | 5.73e+04 |\n",
      "|    agent/train/approx_kl               | 0.00907  |\n",
      "|    agent/train/clip_fraction           | 0.313    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.05    |\n",
      "|    agent/train/explained_variance      | 0.97     |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0357  |\n",
      "|    agent/train/n_updates               | 280      |\n",
      "|    agent/train/policy_gradient_loss    | -0.00847 |\n",
      "|    agent/train/std                     | 0.674    |\n",
      "|    agent/train/value_loss              | 0.00533  |\n",
      "|    preferences/entropy                 | 0.123    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.705    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.182    |\n",
      "|    reward/epoch-0/train/loss           | 0.531    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.708    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.181    |\n",
      "|    reward/epoch-1/train/loss           | 0.531    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.735    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.181    |\n",
      "|    reward/epoch-2/train/loss           | 0.51     |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.735    |\n",
      "|    final/train/gt_reward_loss          | 0.181    |\n",
      "|    final/train/loss                    | 0.51     |\n",
      "-----------------------------------------------------\n",
      "Query schedule: [50, 127, 102, 85, 73, 63]\n",
      "Collecting 100 fragments (5000 transitions)\n",
      "Requested 4750 transitions but only 0 in buffer. Sampling 4750 additional transitions.\n",
      "Sampling 250 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 50 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c321a53e20884546bcd635bb4aa7273d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "---------------------------------------------------\n",
      "| raw/                                 |          |\n",
      "|    agent/rollout/ep_len_mean         | 50       |\n",
      "|    agent/rollout/ep_rew_mean         | -60.2    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -28.7    |\n",
      "|    agent/time/fps                    | 998      |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 2        |\n",
      "|    agent/time/total_timesteps        | 2048     |\n",
      "---------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -60.9        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -17.5        |\n",
      "|    agent/time/fps                    | 823          |\n",
      "|    agent/time/iterations             | 2            |\n",
      "|    agent/time/time_elapsed           | 4            |\n",
      "|    agent/time/total_timesteps        | 4096         |\n",
      "|    agent/train/approx_kl             | 0.0040333765 |\n",
      "|    agent/train/clip_fraction         | 0.199        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.86        |\n",
      "|    agent/train/explained_variance    | 0.278        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.00169     |\n",
      "|    agent/train/n_updates             | 10           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00861     |\n",
      "|    agent/train/std                   | 1.01         |\n",
      "|    agent/train/value_loss            | 0.179        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -8.34       |\n",
      "|    agent/time/fps                    | 767         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 8           |\n",
      "|    agent/time/total_timesteps        | 6144        |\n",
      "|    agent/train/approx_kl             | 0.005593381 |\n",
      "|    agent/train/clip_fraction         | 0.264       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.88       |\n",
      "|    agent/train/explained_variance    | 0.51        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.00691     |\n",
      "|    agent/train/n_updates             | 20          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0146     |\n",
      "|    agent/train/std                   | 1.02        |\n",
      "|    agent/train/value_loss            | 0.101       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.75       |\n",
      "|    agent/time/fps                    | 754         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 10          |\n",
      "|    agent/time/total_timesteps        | 8192        |\n",
      "|    agent/train/approx_kl             | 0.004950756 |\n",
      "|    agent/train/clip_fraction         | 0.254       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.86       |\n",
      "|    agent/train/explained_variance    | 0.693       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.00218     |\n",
      "|    agent/train/n_updates             | 30          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0135     |\n",
      "|    agent/train/std                   | 1.02        |\n",
      "|    agent/train/value_loss            | 0.0751      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -62.5       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.31       |\n",
      "|    agent/time/fps                    | 745         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 13          |\n",
      "|    agent/time/total_timesteps        | 10240       |\n",
      "|    agent/train/approx_kl             | 0.006578625 |\n",
      "|    agent/train/clip_fraction         | 0.302       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.87       |\n",
      "|    agent/train/explained_variance    | 0.707       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0266     |\n",
      "|    agent/train/n_updates             | 40          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0174     |\n",
      "|    agent/train/std                   | 1.02        |\n",
      "|    agent/train/value_loss            | 0.0755      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                   |          |\n",
      "|    agent/rollout/ep_len_mean            | 50       |\n",
      "|    agent/rollout/ep_rew_mean            | -61.4    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean    | -12.3    |\n",
      "|    agent/time/fps                       | 817      |\n",
      "|    agent/time/iterations                | 3        |\n",
      "|    agent/time/time_elapsed              | 7.4      |\n",
      "|    agent/time/total_timesteps           | 6.14e+03 |\n",
      "|    agent/train/approx_kl                | 0.00551  |\n",
      "|    agent/train/clip_fraction            | 0.265    |\n",
      "|    agent/train/clip_range               | 0.1      |\n",
      "|    agent/train/entropy_loss             | -2.87    |\n",
      "|    agent/train/explained_variance       | 0.585    |\n",
      "|    agent/train/learning_rate            | 0.002    |\n",
      "|    agent/train/loss                     | -0.0129  |\n",
      "|    agent/train/n_updates                | 30       |\n",
      "|    agent/train/policy_gradient_loss     | -0.014   |\n",
      "|    agent/train/std                      | 1.02     |\n",
      "|    agent/train/value_loss               | 0.0996   |\n",
      "|    preferences/entropy                  | 0.198    |\n",
      "|    reward/epoch-0/train/accuracy        | 0.45     |\n",
      "|    reward/epoch-0/train/gt_reward_loss  | 0.117    |\n",
      "|    reward/epoch-0/train/loss            | 2.44     |\n",
      "|    reward/epoch-1/train/accuracy        | 0.422    |\n",
      "|    reward/epoch-1/train/gt_reward_loss  | 0.113    |\n",
      "|    reward/epoch-1/train/loss            | 2        |\n",
      "|    reward/epoch-10/train/accuracy       | 0.712    |\n",
      "|    reward/epoch-10/train/gt_reward_loss | 0.11     |\n",
      "|    reward/epoch-10/train/loss           | 0.72     |\n",
      "|    reward/epoch-11/train/accuracy       | 0.752    |\n",
      "|    reward/epoch-11/train/gt_reward_loss | 0.124    |\n",
      "|    reward/epoch-11/train/loss           | 0.644    |\n",
      "|    reward/epoch-2/train/accuracy        | 0.394    |\n",
      "|    reward/epoch-2/train/gt_reward_loss  | 0.126    |\n",
      "|    reward/epoch-2/train/loss            | 1.56     |\n",
      "|    reward/epoch-3/train/accuracy        | 0.342    |\n",
      "|    reward/epoch-3/train/gt_reward_loss  | 0.125    |\n",
      "|    reward/epoch-3/train/loss            | 1.25     |\n",
      "|    reward/epoch-4/train/accuracy        | 0.382    |\n",
      "|    reward/epoch-4/train/gt_reward_loss  | 0.104    |\n",
      "|    reward/epoch-4/train/loss            | 0.992    |\n",
      "|    reward/epoch-5/train/accuracy        | 0.54     |\n",
      "|    reward/epoch-5/train/gt_reward_loss  | 0.0992   |\n",
      "|    reward/epoch-5/train/loss            | 0.823    |\n",
      "|    reward/epoch-6/train/accuracy        | 0.649    |\n",
      "|    reward/epoch-6/train/gt_reward_loss  | 0.11     |\n",
      "|    reward/epoch-6/train/loss            | 0.768    |\n",
      "|    reward/epoch-7/train/accuracy        | 0.717    |\n",
      "|    reward/epoch-7/train/gt_reward_loss  | 0.129    |\n",
      "|    reward/epoch-7/train/loss            | 0.704    |\n",
      "|    reward/epoch-8/train/accuracy        | 0.72     |\n",
      "|    reward/epoch-8/train/gt_reward_loss  | 0.113    |\n",
      "|    reward/epoch-8/train/loss            | 0.738    |\n",
      "|    reward/epoch-9/train/accuracy        | 0.748    |\n",
      "|    reward/epoch-9/train/gt_reward_loss  | 0.115    |\n",
      "|    reward/epoch-9/train/loss            | 0.696    |\n",
      "| reward/                                 |          |\n",
      "|    final/train/accuracy                 | 0.752    |\n",
      "|    final/train/gt_reward_loss           | 0.124    |\n",
      "|    final/train/loss                     | 0.644    |\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting 254 fragments (12700 transitions)\n",
      "Requested 12065 transitions but only 10200 in buffer. Sampling 1865 additional transitions.\n",
      "Sampling 635 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 180 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "566e7edac2ed431ab922a68409712ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -62.9        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -2.09        |\n",
      "|    agent/time/fps                    | 1111         |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 1            |\n",
      "|    agent/time/total_timesteps        | 12288        |\n",
      "|    agent/train/approx_kl             | 0.0064050416 |\n",
      "|    agent/train/clip_fraction         | 0.307        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.86        |\n",
      "|    agent/train/explained_variance    | 0.736        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0453      |\n",
      "|    agent/train/n_updates             | 50           |\n",
      "|    agent/train/policy_gradient_loss  | -0.016       |\n",
      "|    agent/train/std                   | 1.01         |\n",
      "|    agent/train/value_loss            | 0.0668       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -64.1       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.91       |\n",
      "|    agent/time/fps                    | 810         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 5           |\n",
      "|    agent/time/total_timesteps        | 14336       |\n",
      "|    agent/train/approx_kl             | 0.005139713 |\n",
      "|    agent/train/clip_fraction         | 0.192       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.86       |\n",
      "|    agent/train/explained_variance    | 0.0738      |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.0396      |\n",
      "|    agent/train/n_updates             | 60          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00758    |\n",
      "|    agent/train/std                   | 1.01        |\n",
      "|    agent/train/value_loss            | 0.15        |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -64.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.01       |\n",
      "|    agent/time/fps                    | 701         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 8           |\n",
      "|    agent/time/total_timesteps        | 16384       |\n",
      "|    agent/train/approx_kl             | 0.006480382 |\n",
      "|    agent/train/clip_fraction         | 0.241       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.84       |\n",
      "|    agent/train/explained_variance    | 0.598       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.00264    |\n",
      "|    agent/train/n_updates             | 70          |\n",
      "|    agent/train/policy_gradient_loss  | -0.012      |\n",
      "|    agent/train/std                   | 1           |\n",
      "|    agent/train/value_loss            | 0.0858      |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -64.9        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.88        |\n",
      "|    agent/time/fps                    | 684          |\n",
      "|    agent/time/iterations             | 4            |\n",
      "|    agent/time/time_elapsed           | 11           |\n",
      "|    agent/time/total_timesteps        | 18432        |\n",
      "|    agent/train/approx_kl             | 0.0059698178 |\n",
      "|    agent/train/clip_fraction         | 0.232        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.83        |\n",
      "|    agent/train/explained_variance    | 0.514        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0238      |\n",
      "|    agent/train/n_updates             | 80           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0105      |\n",
      "|    agent/train/std                   | 0.998        |\n",
      "|    agent/train/value_loss            | 0.066        |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -63.9        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.77        |\n",
      "|    agent/time/fps                    | 677          |\n",
      "|    agent/time/iterations             | 5            |\n",
      "|    agent/time/time_elapsed           | 15           |\n",
      "|    agent/time/total_timesteps        | 20480        |\n",
      "|    agent/train/approx_kl             | 0.0074786562 |\n",
      "|    agent/train/clip_fraction         | 0.289        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.83        |\n",
      "|    agent/train/explained_variance    | 0.645        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0167      |\n",
      "|    agent/train/n_updates             | 90           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0166      |\n",
      "|    agent/train/std                   | 0.994        |\n",
      "|    agent/train/value_loss            | 0.0531       |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -64.1    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -5.13    |\n",
      "|    agent/time/fps                      | 797      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 8        |\n",
      "|    agent/time/total_timesteps          | 1.64e+04 |\n",
      "|    agent/train/approx_kl               | 0.00661  |\n",
      "|    agent/train/clip_fraction           | 0.247    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.84    |\n",
      "|    agent/train/explained_variance      | 0.48     |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.00263 |\n",
      "|    agent/train/n_updates               | 80       |\n",
      "|    agent/train/policy_gradient_loss    | -0.012   |\n",
      "|    agent/train/std                     | 1        |\n",
      "|    agent/train/value_loss              | 0.08     |\n",
      "|    preferences/entropy                 | 0.171    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.566    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.151    |\n",
      "|    reward/epoch-0/train/loss           | 1.03     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.553    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.148    |\n",
      "|    reward/epoch-1/train/loss           | 0.732    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.633    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.142    |\n",
      "|    reward/epoch-2/train/loss           | 0.631    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.633    |\n",
      "|    final/train/gt_reward_loss          | 0.142    |\n",
      "|    final/train/loss                    | 0.631    |\n",
      "-----------------------------------------------------\n",
      "Collecting 204 fragments (10200 transitions)\n",
      "Sampling 510 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 285 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c20a5052d646c2989f9b7a03a69e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -63.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.53       |\n",
      "|    agent/time/fps                    | 1055        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 1           |\n",
      "|    agent/time/total_timesteps        | 22528       |\n",
      "|    agent/train/approx_kl             | 0.007966993 |\n",
      "|    agent/train/clip_fraction         | 0.279       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.83       |\n",
      "|    agent/train/explained_variance    | 0.57        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.00956    |\n",
      "|    agent/train/n_updates             | 100         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0134     |\n",
      "|    agent/train/std                   | 0.996       |\n",
      "|    agent/train/value_loss            | 0.0452      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -63.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.53       |\n",
      "|    agent/time/fps                    | 801         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 5           |\n",
      "|    agent/time/total_timesteps        | 24576       |\n",
      "|    agent/train/approx_kl             | 0.007781481 |\n",
      "|    agent/train/clip_fraction         | 0.296       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.82       |\n",
      "|    agent/train/explained_variance    | 0.654       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0386     |\n",
      "|    agent/train/n_updates             | 110         |\n",
      "|    agent/train/policy_gradient_loss  | -0.014      |\n",
      "|    agent/train/std                   | 0.986       |\n",
      "|    agent/train/value_loss            | 0.0263      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -64         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.46       |\n",
      "|    agent/time/fps                    | 718         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 8           |\n",
      "|    agent/time/total_timesteps        | 26624       |\n",
      "|    agent/train/approx_kl             | 0.006806855 |\n",
      "|    agent/train/clip_fraction         | 0.259       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.8        |\n",
      "|    agent/train/explained_variance    | 0.527       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0242     |\n",
      "|    agent/train/n_updates             | 120         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00956    |\n",
      "|    agent/train/std                   | 0.98        |\n",
      "|    agent/train/value_loss            | 0.021       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -63.1       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.36       |\n",
      "|    agent/time/fps                    | 699         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 11          |\n",
      "|    agent/time/total_timesteps        | 28672       |\n",
      "|    agent/train/approx_kl             | 0.008349306 |\n",
      "|    agent/train/clip_fraction         | 0.278       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.77       |\n",
      "|    agent/train/explained_variance    | 0.725       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0384     |\n",
      "|    agent/train/n_updates             | 130         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0125     |\n",
      "|    agent/train/std                   | 0.963       |\n",
      "|    agent/train/value_loss            | 0.0113      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -62.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.22       |\n",
      "|    agent/time/fps                    | 691         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 14          |\n",
      "|    agent/time/total_timesteps        | 30720       |\n",
      "|    agent/train/approx_kl             | 0.008239504 |\n",
      "|    agent/train/clip_fraction         | 0.295       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.76       |\n",
      "|    agent/train/explained_variance    | 0.8         |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0397     |\n",
      "|    agent/train/n_updates             | 140         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0138     |\n",
      "|    agent/train/std                   | 0.959       |\n",
      "|    agent/train/value_loss            | 0.0137      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -63.4    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -6.42    |\n",
      "|    agent/time/fps                      | 793      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 7.8      |\n",
      "|    agent/time/total_timesteps          | 2.66e+04 |\n",
      "|    agent/train/approx_kl               | 0.00768  |\n",
      "|    agent/train/clip_fraction           | 0.284    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.78    |\n",
      "|    agent/train/explained_variance      | 0.706    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0338  |\n",
      "|    agent/train/n_updates               | 130      |\n",
      "|    agent/train/policy_gradient_loss    | -0.0127  |\n",
      "|    agent/train/std                     | 0.97     |\n",
      "|    agent/train/value_loss              | 0.0167   |\n",
      "|    preferences/entropy                 | 0.136    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.661    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.142    |\n",
      "|    reward/epoch-0/train/loss           | 0.622    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.664    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.141    |\n",
      "|    reward/epoch-1/train/loss           | 0.583    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.687    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.142    |\n",
      "|    reward/epoch-2/train/loss           | 0.562    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.687    |\n",
      "|    final/train/gt_reward_loss          | 0.142    |\n",
      "|    final/train/loss                    | 0.562    |\n",
      "-----------------------------------------------------\n",
      "Collecting 170 fragments (8500 transitions)\n",
      "Sampling 425 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 370 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4eb8d5b546d4d56bb915feae775593d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -62.4        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.24        |\n",
      "|    agent/time/fps                    | 1092         |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 1            |\n",
      "|    agent/time/total_timesteps        | 32768        |\n",
      "|    agent/train/approx_kl             | 0.0072165076 |\n",
      "|    agent/train/clip_fraction         | 0.294        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.76        |\n",
      "|    agent/train/explained_variance    | 0.822        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0283      |\n",
      "|    agent/train/n_updates             | 150          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0136      |\n",
      "|    agent/train/std                   | 0.963        |\n",
      "|    agent/train/value_loss            | 0.0112       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -62.1       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.47       |\n",
      "|    agent/time/fps                    | 802         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 5           |\n",
      "|    agent/time/total_timesteps        | 34816       |\n",
      "|    agent/train/approx_kl             | 0.006801748 |\n",
      "|    agent/train/clip_fraction         | 0.27        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.77       |\n",
      "|    agent/train/explained_variance    | 0.664       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0331     |\n",
      "|    agent/train/n_updates             | 160         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00959    |\n",
      "|    agent/train/std                   | 0.966       |\n",
      "|    agent/train/value_loss            | 0.0153      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.9       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.63       |\n",
      "|    agent/time/fps                    | 675         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 9           |\n",
      "|    agent/time/total_timesteps        | 36864       |\n",
      "|    agent/train/approx_kl             | 0.008307984 |\n",
      "|    agent/train/clip_fraction         | 0.285       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.74       |\n",
      "|    agent/train/explained_variance    | 0.783       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0359     |\n",
      "|    agent/train/n_updates             | 170         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00962    |\n",
      "|    agent/train/std                   | 0.952       |\n",
      "|    agent/train/value_loss            | 0.01        |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.59       |\n",
      "|    agent/time/fps                    | 674         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 12          |\n",
      "|    agent/time/total_timesteps        | 38912       |\n",
      "|    agent/train/approx_kl             | 0.010220733 |\n",
      "|    agent/train/clip_fraction         | 0.309       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.73       |\n",
      "|    agent/train/explained_variance    | 0.852       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0397     |\n",
      "|    agent/train/n_updates             | 180         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0148     |\n",
      "|    agent/train/std                   | 0.943       |\n",
      "|    agent/train/value_loss            | 0.0114      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.5        |\n",
      "|    agent/time/fps                    | 682         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 15          |\n",
      "|    agent/time/total_timesteps        | 40960       |\n",
      "|    agent/train/approx_kl             | 0.008992517 |\n",
      "|    agent/train/clip_fraction         | 0.339       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.7        |\n",
      "|    agent/train/explained_variance    | 0.824       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0466     |\n",
      "|    agent/train/n_updates             | 190         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0134     |\n",
      "|    agent/train/std                   | 0.927       |\n",
      "|    agent/train/value_loss            | 0.011       |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -61.9    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -6.49    |\n",
      "|    agent/time/fps                      | 785      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 8.4      |\n",
      "|    agent/time/total_timesteps          | 3.69e+04 |\n",
      "|    agent/train/approx_kl               | 0.00876  |\n",
      "|    agent/train/clip_fraction           | 0.309    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.72    |\n",
      "|    agent/train/explained_variance      | 0.802    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0406  |\n",
      "|    agent/train/n_updates               | 180      |\n",
      "|    agent/train/policy_gradient_loss    | -0.0123  |\n",
      "|    agent/train/std                     | 0.941    |\n",
      "|    agent/train/value_loss              | 0.0119   |\n",
      "|    preferences/entropy                 | 0.197    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.677    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.186    |\n",
      "|    reward/epoch-0/train/loss           | 0.601    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.726    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.188    |\n",
      "|    reward/epoch-1/train/loss           | 0.571    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.742    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.171    |\n",
      "|    reward/epoch-2/train/loss           | 0.542    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.742    |\n",
      "|    final/train/gt_reward_loss          | 0.171    |\n",
      "|    final/train/loss                    | 0.542    |\n",
      "-----------------------------------------------------\n",
      "Collecting 146 fragments (7300 transitions)\n",
      "Sampling 365 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 445 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "255da88574e14b1f95a2433910e07ac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.65       |\n",
      "|    agent/time/fps                    | 1102        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 1           |\n",
      "|    agent/time/total_timesteps        | 43008       |\n",
      "|    agent/train/approx_kl             | 0.009471059 |\n",
      "|    agent/train/clip_fraction         | 0.34        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.68       |\n",
      "|    agent/train/explained_variance    | 0.885       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0479     |\n",
      "|    agent/train/n_updates             | 200         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0139     |\n",
      "|    agent/train/std                   | 0.919       |\n",
      "|    agent/train/value_loss            | 0.0116      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 50         |\n",
      "|    agent/rollout/ep_rew_mean         | -61        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.64      |\n",
      "|    agent/time/fps                    | 863        |\n",
      "|    agent/time/iterations             | 2          |\n",
      "|    agent/time/time_elapsed           | 4          |\n",
      "|    agent/time/total_timesteps        | 45056      |\n",
      "|    agent/train/approx_kl             | 0.01038979 |\n",
      "|    agent/train/clip_fraction         | 0.35       |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -2.64      |\n",
      "|    agent/train/explained_variance    | 0.939      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | -0.046     |\n",
      "|    agent/train/n_updates             | 210        |\n",
      "|    agent/train/policy_gradient_loss  | -0.0138    |\n",
      "|    agent/train/std                   | 0.902      |\n",
      "|    agent/train/value_loss            | 0.00825    |\n",
      "-----------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.2       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.59       |\n",
      "|    agent/time/fps                    | 807         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 7           |\n",
      "|    agent/time/total_timesteps        | 47104       |\n",
      "|    agent/train/approx_kl             | 0.010227708 |\n",
      "|    agent/train/clip_fraction         | 0.326       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.62       |\n",
      "|    agent/train/explained_variance    | 0.953       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0261     |\n",
      "|    agent/train/n_updates             | 220         |\n",
      "|    agent/train/policy_gradient_loss  | -0.011      |\n",
      "|    agent/train/std                   | 0.892       |\n",
      "|    agent/train/value_loss            | 0.00591     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -60.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.44       |\n",
      "|    agent/time/fps                    | 780         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 10          |\n",
      "|    agent/time/total_timesteps        | 49152       |\n",
      "|    agent/train/approx_kl             | 0.011206234 |\n",
      "|    agent/train/clip_fraction         | 0.339       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.57       |\n",
      "|    agent/train/explained_variance    | 0.955       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0488     |\n",
      "|    agent/train/n_updates             | 230         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0146     |\n",
      "|    agent/train/std                   | 0.871       |\n",
      "|    agent/train/value_loss            | 0.00524     |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 50         |\n",
      "|    agent/rollout/ep_rew_mean         | -60.3      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.4       |\n",
      "|    agent/time/fps                    | 765        |\n",
      "|    agent/time/iterations             | 5          |\n",
      "|    agent/time/time_elapsed           | 13         |\n",
      "|    agent/time/total_timesteps        | 51200      |\n",
      "|    agent/train/approx_kl             | 0.01173629 |\n",
      "|    agent/train/clip_fraction         | 0.364      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -2.51      |\n",
      "|    agent/train/explained_variance    | 0.941      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | -0.0307    |\n",
      "|    agent/train/n_updates             | 240        |\n",
      "|    agent/train/policy_gradient_loss  | -0.0137    |\n",
      "|    agent/train/std                   | 0.847      |\n",
      "|    agent/train/value_loss            | 0.00757    |\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -60.9    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -6.54    |\n",
      "|    agent/time/fps                      | 863      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 7        |\n",
      "|    agent/time/total_timesteps          | 4.71e+04 |\n",
      "|    agent/train/approx_kl               | 0.0114   |\n",
      "|    agent/train/clip_fraction           | 0.351    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.56    |\n",
      "|    agent/train/explained_variance      | 0.947    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0375  |\n",
      "|    agent/train/n_updates               | 230      |\n",
      "|    agent/train/policy_gradient_loss    | -0.0132  |\n",
      "|    agent/train/std                     | 0.867    |\n",
      "|    agent/train/value_loss              | 0.0064   |\n",
      "|    preferences/entropy                 | 0.201    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.731    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.164    |\n",
      "|    reward/epoch-0/train/loss           | 0.536    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.752    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.164    |\n",
      "|    reward/epoch-1/train/loss           | 0.526    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.766    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.164    |\n",
      "|    reward/epoch-2/train/loss           | 0.514    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.766    |\n",
      "|    final/train/gt_reward_loss          | 0.164    |\n",
      "|    final/train/loss                    | 0.514    |\n",
      "-----------------------------------------------------\n",
      "Collecting 126 fragments (6300 transitions)\n",
      "Sampling 315 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 510 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a593d060356b43268864ede29eef9a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -60.2       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.36       |\n",
      "|    agent/time/fps                    | 1027        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 1           |\n",
      "|    agent/time/total_timesteps        | 53248       |\n",
      "|    agent/train/approx_kl             | 0.013639806 |\n",
      "|    agent/train/clip_fraction         | 0.376       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.46       |\n",
      "|    agent/train/explained_variance    | 0.945       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0361     |\n",
      "|    agent/train/n_updates             | 250         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0129     |\n",
      "|    agent/train/std                   | 0.822       |\n",
      "|    agent/train/value_loss            | 0.00504     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -59.1       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.74       |\n",
      "|    agent/time/fps                    | 861         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 4           |\n",
      "|    agent/time/total_timesteps        | 55296       |\n",
      "|    agent/train/approx_kl             | 0.010546084 |\n",
      "|    agent/train/clip_fraction         | 0.331       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.42       |\n",
      "|    agent/train/explained_variance    | 0.936       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.032      |\n",
      "|    agent/train/n_updates             | 260         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00892    |\n",
      "|    agent/train/std                   | 0.808       |\n",
      "|    agent/train/value_loss            | 0.00698     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -58         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.83       |\n",
      "|    agent/time/fps                    | 803         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 7           |\n",
      "|    agent/time/total_timesteps        | 57344       |\n",
      "|    agent/train/approx_kl             | 0.012160312 |\n",
      "|    agent/train/clip_fraction         | 0.37        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.37       |\n",
      "|    agent/train/explained_variance    | 0.96        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0286     |\n",
      "|    agent/train/n_updates             | 270         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0137     |\n",
      "|    agent/train/std                   | 0.789       |\n",
      "|    agent/train/value_loss            | 0.00475     |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -57          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.73        |\n",
      "|    agent/time/fps                    | 779          |\n",
      "|    agent/time/iterations             | 4            |\n",
      "|    agent/time/time_elapsed           | 10           |\n",
      "|    agent/time/total_timesteps        | 59392        |\n",
      "|    agent/train/approx_kl             | 0.0119493725 |\n",
      "|    agent/train/clip_fraction         | 0.377        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.32        |\n",
      "|    agent/train/explained_variance    | 0.953        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0378      |\n",
      "|    agent/train/n_updates             | 280          |\n",
      "|    agent/train/policy_gradient_loss  | -0.01        |\n",
      "|    agent/train/std                   | 0.769        |\n",
      "|    agent/train/value_loss            | 0.00462      |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -57.5       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.58       |\n",
      "|    agent/time/fps                    | 764         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 13          |\n",
      "|    agent/time/total_timesteps        | 61440       |\n",
      "|    agent/train/approx_kl             | 0.011786291 |\n",
      "|    agent/train/clip_fraction         | 0.365       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.26       |\n",
      "|    agent/train/explained_variance    | 0.968       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0309     |\n",
      "|    agent/train/n_updates             | 290         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00922    |\n",
      "|    agent/train/std                   | 0.745       |\n",
      "|    agent/train/value_loss            | 0.0031      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -58.4    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -6.65    |\n",
      "|    agent/time/fps                      | 847      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 7        |\n",
      "|    agent/time/total_timesteps          | 5.73e+04 |\n",
      "|    agent/train/approx_kl               | 0.0124   |\n",
      "|    agent/train/clip_fraction           | 0.363    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.31    |\n",
      "|    agent/train/explained_variance      | 0.944    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0318  |\n",
      "|    agent/train/n_updates               | 280      |\n",
      "|    agent/train/policy_gradient_loss    | -0.00975 |\n",
      "|    agent/train/std                     | 0.767    |\n",
      "|    agent/train/value_loss              | 0.00499  |\n",
      "|    preferences/entropy                 | 0.16     |\n",
      "|    reward/epoch-0/train/accuracy       | 0.755    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.165    |\n",
      "|    reward/epoch-0/train/loss           | 0.511    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.777    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.165    |\n",
      "|    reward/epoch-1/train/loss           | 0.499    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.761    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.165    |\n",
      "|    reward/epoch-2/train/loss           | 0.497    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.761    |\n",
      "|    final/train/gt_reward_loss          | 0.165    |\n",
      "|    final/train/loss                    | 0.497    |\n",
      "-----------------------------------------------------\n",
      "Query schedule: [50, 127, 102, 85, 73, 63]\n",
      "Collecting 100 fragments (5000 transitions)\n",
      "Requested 4750 transitions but only 0 in buffer. Sampling 4750 additional transitions.\n",
      "Sampling 250 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 50 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c25cf71bea149919ba7013c6217c3c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "---------------------------------------------------\n",
      "| raw/                                 |          |\n",
      "|    agent/rollout/ep_len_mean         | 50       |\n",
      "|    agent/rollout/ep_rew_mean         | -60.2    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -28.7    |\n",
      "|    agent/time/fps                    | 1075     |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 1        |\n",
      "|    agent/time/total_timesteps        | 2048     |\n",
      "---------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -17.6        |\n",
      "|    agent/time/fps                    | 863          |\n",
      "|    agent/time/iterations             | 2            |\n",
      "|    agent/time/time_elapsed           | 4            |\n",
      "|    agent/time/total_timesteps        | 4096         |\n",
      "|    agent/train/approx_kl             | 0.0033060662 |\n",
      "|    agent/train/clip_fraction         | 0.174        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.86        |\n",
      "|    agent/train/explained_variance    | 0.114        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.00943     |\n",
      "|    agent/train/n_updates             | 10           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00638     |\n",
      "|    agent/train/std                   | 1.01         |\n",
      "|    agent/train/value_loss            | 0.171        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -8.47       |\n",
      "|    agent/time/fps                    | 805         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 7           |\n",
      "|    agent/time/total_timesteps        | 6144        |\n",
      "|    agent/train/approx_kl             | 0.004938728 |\n",
      "|    agent/train/clip_fraction         | 0.215       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.88       |\n",
      "|    agent/train/explained_variance    | 0.483       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0164     |\n",
      "|    agent/train/n_updates             | 20          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0106     |\n",
      "|    agent/train/std                   | 1.02        |\n",
      "|    agent/train/value_loss            | 0.0796      |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61.9        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.93        |\n",
      "|    agent/time/fps                    | 780          |\n",
      "|    agent/time/iterations             | 4            |\n",
      "|    agent/time/time_elapsed           | 10           |\n",
      "|    agent/time/total_timesteps        | 8192         |\n",
      "|    agent/train/approx_kl             | 0.0045075873 |\n",
      "|    agent/train/clip_fraction         | 0.213        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.87        |\n",
      "|    agent/train/explained_variance    | 0.793        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.014       |\n",
      "|    agent/train/n_updates             | 30           |\n",
      "|    agent/train/policy_gradient_loss  | -0.01        |\n",
      "|    agent/train/std                   | 1.02         |\n",
      "|    agent/train/value_loss            | 0.0549       |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -62.5        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.61        |\n",
      "|    agent/time/fps                    | 766          |\n",
      "|    agent/time/iterations             | 5            |\n",
      "|    agent/time/time_elapsed           | 13           |\n",
      "|    agent/time/total_timesteps        | 10240        |\n",
      "|    agent/train/approx_kl             | 0.0051723756 |\n",
      "|    agent/train/clip_fraction         | 0.258        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.89        |\n",
      "|    agent/train/explained_variance    | 0.797        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0473      |\n",
      "|    agent/train/n_updates             | 40           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0131      |\n",
      "|    agent/train/std                   | 1.02         |\n",
      "|    agent/train/value_loss            | 0.0422       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                   |          |\n",
      "|    agent/rollout/ep_len_mean            | 50       |\n",
      "|    agent/rollout/ep_rew_mean            | -61.5    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean    | -12.5    |\n",
      "|    agent/time/fps                       | 858      |\n",
      "|    agent/time/iterations                | 3        |\n",
      "|    agent/time/time_elapsed              | 7        |\n",
      "|    agent/time/total_timesteps           | 6.14e+03 |\n",
      "|    agent/train/approx_kl                | 0.00487  |\n",
      "|    agent/train/clip_fraction            | 0.222    |\n",
      "|    agent/train/clip_range               | 0.1      |\n",
      "|    agent/train/entropy_loss             | -2.87    |\n",
      "|    agent/train/explained_variance       | 0.608    |\n",
      "|    agent/train/learning_rate            | 0.002    |\n",
      "|    agent/train/loss                     | -0.0256  |\n",
      "|    agent/train/n_updates                | 30       |\n",
      "|    agent/train/policy_gradient_loss     | -0.0107  |\n",
      "|    agent/train/std                      | 1.02     |\n",
      "|    agent/train/value_loss               | 0.0768   |\n",
      "|    preferences/entropy                  | 0.198    |\n",
      "|    reward/epoch-0/train/accuracy        | 0.434    |\n",
      "|    reward/epoch-0/train/gt_reward_loss  | 0.121    |\n",
      "|    reward/epoch-0/train/loss            | 1.4      |\n",
      "|    reward/epoch-1/train/accuracy        | 0.469    |\n",
      "|    reward/epoch-1/train/gt_reward_loss  | 0.12     |\n",
      "|    reward/epoch-1/train/loss            | 1.12     |\n",
      "|    reward/epoch-10/train/accuracy       | 0.764    |\n",
      "|    reward/epoch-10/train/gt_reward_loss | 0.123    |\n",
      "|    reward/epoch-10/train/loss           | 0.464    |\n",
      "|    reward/epoch-11/train/accuracy       | 0.776    |\n",
      "|    reward/epoch-11/train/gt_reward_loss | 0.131    |\n",
      "|    reward/epoch-11/train/loss           | 0.447    |\n",
      "|    reward/epoch-2/train/accuracy        | 0.559    |\n",
      "|    reward/epoch-2/train/gt_reward_loss  | 0.116    |\n",
      "|    reward/epoch-2/train/loss            | 0.873    |\n",
      "|    reward/epoch-3/train/accuracy        | 0.562    |\n",
      "|    reward/epoch-3/train/gt_reward_loss  | 0.111    |\n",
      "|    reward/epoch-3/train/loss            | 0.76     |\n",
      "|    reward/epoch-4/train/accuracy        | 0.698    |\n",
      "|    reward/epoch-4/train/gt_reward_loss  | 0.123    |\n",
      "|    reward/epoch-4/train/loss            | 0.601    |\n",
      "|    reward/epoch-5/train/accuracy        | 0.689    |\n",
      "|    reward/epoch-5/train/gt_reward_loss  | 0.124    |\n",
      "|    reward/epoch-5/train/loss            | 0.581    |\n",
      "|    reward/epoch-6/train/accuracy        | 0.733    |\n",
      "|    reward/epoch-6/train/gt_reward_loss  | 0.11     |\n",
      "|    reward/epoch-6/train/loss            | 0.542    |\n",
      "|    reward/epoch-7/train/accuracy        | 0.847    |\n",
      "|    reward/epoch-7/train/gt_reward_loss  | 0.124    |\n",
      "|    reward/epoch-7/train/loss            | 0.473    |\n",
      "|    reward/epoch-8/train/accuracy        | 0.736    |\n",
      "|    reward/epoch-8/train/gt_reward_loss  | 0.111    |\n",
      "|    reward/epoch-8/train/loss            | 0.533    |\n",
      "|    reward/epoch-9/train/accuracy        | 0.74     |\n",
      "|    reward/epoch-9/train/gt_reward_loss  | 0.13     |\n",
      "|    reward/epoch-9/train/loss            | 0.525    |\n",
      "| reward/                                 |          |\n",
      "|    final/train/accuracy                 | 0.776    |\n",
      "|    final/train/gt_reward_loss           | 0.131    |\n",
      "|    final/train/loss                     | 0.447    |\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting 254 fragments (12700 transitions)\n",
      "Requested 12065 transitions but only 10200 in buffer. Sampling 1865 additional transitions.\n",
      "Sampling 635 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 180 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f77f4a15802479a9c2a46d926c3b380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -62.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -2.97       |\n",
      "|    agent/time/fps                    | 1103        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 1           |\n",
      "|    agent/time/total_timesteps        | 12288       |\n",
      "|    agent/train/approx_kl             | 0.006442847 |\n",
      "|    agent/train/clip_fraction         | 0.252       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.87       |\n",
      "|    agent/train/explained_variance    | 0.855       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0407     |\n",
      "|    agent/train/n_updates             | 50          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0136     |\n",
      "|    agent/train/std                   | 1.02        |\n",
      "|    agent/train/value_loss            | 0.0359      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -63.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -4.1        |\n",
      "|    agent/time/fps                    | 861         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 4           |\n",
      "|    agent/time/total_timesteps        | 14336       |\n",
      "|    agent/train/approx_kl             | 0.004421518 |\n",
      "|    agent/train/clip_fraction         | 0.174       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.87       |\n",
      "|    agent/train/explained_variance    | 0.402       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.00183     |\n",
      "|    agent/train/n_updates             | 60          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0073     |\n",
      "|    agent/train/std                   | 1.02        |\n",
      "|    agent/train/value_loss            | 0.0767      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -63.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.17       |\n",
      "|    agent/time/fps                    | 791         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 7           |\n",
      "|    agent/time/total_timesteps        | 16384       |\n",
      "|    agent/train/approx_kl             | 0.005165047 |\n",
      "|    agent/train/clip_fraction         | 0.23        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.87       |\n",
      "|    agent/train/explained_variance    | 0.764       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0271     |\n",
      "|    agent/train/n_updates             | 70          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0106     |\n",
      "|    agent/train/std                   | 1.02        |\n",
      "|    agent/train/value_loss            | 0.0434      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -63.9       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.75       |\n",
      "|    agent/time/fps                    | 767         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 10          |\n",
      "|    agent/time/total_timesteps        | 18432       |\n",
      "|    agent/train/approx_kl             | 0.006478932 |\n",
      "|    agent/train/clip_fraction         | 0.262       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.87       |\n",
      "|    agent/train/explained_variance    | 0.698       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0297     |\n",
      "|    agent/train/n_updates             | 80          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0121     |\n",
      "|    agent/train/std                   | 1.02        |\n",
      "|    agent/train/value_loss            | 0.0356      |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -63.5        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.61        |\n",
      "|    agent/time/fps                    | 756          |\n",
      "|    agent/time/iterations             | 5            |\n",
      "|    agent/time/time_elapsed           | 13           |\n",
      "|    agent/time/total_timesteps        | 20480        |\n",
      "|    agent/train/approx_kl             | 0.0076724747 |\n",
      "|    agent/train/clip_fraction         | 0.3          |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.88        |\n",
      "|    agent/train/explained_variance    | 0.661        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0294      |\n",
      "|    agent/train/n_updates             | 90           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0167      |\n",
      "|    agent/train/std                   | 1.02         |\n",
      "|    agent/train/value_loss            | 0.0364       |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -63.5    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -4.72    |\n",
      "|    agent/time/fps                      | 856      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 7        |\n",
      "|    agent/time/total_timesteps          | 1.64e+04 |\n",
      "|    agent/train/approx_kl               | 0.00616  |\n",
      "|    agent/train/clip_fraction           | 0.253    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.87    |\n",
      "|    agent/train/explained_variance      | 0.645    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0257  |\n",
      "|    agent/train/n_updates               | 80       |\n",
      "|    agent/train/policy_gradient_loss    | -0.0123  |\n",
      "|    agent/train/std                     | 1.02     |\n",
      "|    agent/train/value_loss              | 0.0437   |\n",
      "|    preferences/entropy                 | 0.188    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.629    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.182    |\n",
      "|    reward/epoch-0/train/loss           | 0.79     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.657    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.174    |\n",
      "|    reward/epoch-1/train/loss           | 0.664    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.655    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.175    |\n",
      "|    reward/epoch-2/train/loss           | 0.622    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.655    |\n",
      "|    final/train/gt_reward_loss          | 0.175    |\n",
      "|    final/train/loss                    | 0.622    |\n",
      "-----------------------------------------------------\n",
      "Collecting 204 fragments (10200 transitions)\n",
      "Sampling 510 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 285 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b210de02ec447ab27fc378076bdbea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -62.9        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.38        |\n",
      "|    agent/time/fps                    | 1107         |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 1            |\n",
      "|    agent/time/total_timesteps        | 22528        |\n",
      "|    agent/train/approx_kl             | 0.0070858533 |\n",
      "|    agent/train/clip_fraction         | 0.301        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.87        |\n",
      "|    agent/train/explained_variance    | 0.7          |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0443      |\n",
      "|    agent/train/n_updates             | 100          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0148      |\n",
      "|    agent/train/std                   | 1.01         |\n",
      "|    agent/train/value_loss            | 0.0265       |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -63.2        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.49        |\n",
      "|    agent/time/fps                    | 866          |\n",
      "|    agent/time/iterations             | 2            |\n",
      "|    agent/time/time_elapsed           | 4            |\n",
      "|    agent/time/total_timesteps        | 24576        |\n",
      "|    agent/train/approx_kl             | 0.0073077185 |\n",
      "|    agent/train/clip_fraction         | 0.27         |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.84        |\n",
      "|    agent/train/explained_variance    | 0.617        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0302      |\n",
      "|    agent/train/n_updates             | 110          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0133      |\n",
      "|    agent/train/std                   | 1            |\n",
      "|    agent/train/value_loss            | 0.0205       |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -63.5        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.53        |\n",
      "|    agent/time/fps                    | 796          |\n",
      "|    agent/time/iterations             | 3            |\n",
      "|    agent/time/time_elapsed           | 7            |\n",
      "|    agent/time/total_timesteps        | 26624        |\n",
      "|    agent/train/approx_kl             | 0.0073997285 |\n",
      "|    agent/train/clip_fraction         | 0.287        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.83        |\n",
      "|    agent/train/explained_variance    | 0.83         |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0258      |\n",
      "|    agent/train/n_updates             | 120          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0127      |\n",
      "|    agent/train/std                   | 0.996        |\n",
      "|    agent/train/value_loss            | 0.0124       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -62.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.45       |\n",
      "|    agent/time/fps                    | 773         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 10          |\n",
      "|    agent/time/total_timesteps        | 28672       |\n",
      "|    agent/train/approx_kl             | 0.009260997 |\n",
      "|    agent/train/clip_fraction         | 0.301       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.81       |\n",
      "|    agent/train/explained_variance    | 0.851       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0515     |\n",
      "|    agent/train/n_updates             | 130         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0143     |\n",
      "|    agent/train/std                   | 0.985       |\n",
      "|    agent/train/value_loss            | 0.0125      |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -62.8        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.36        |\n",
      "|    agent/time/fps                    | 761          |\n",
      "|    agent/time/iterations             | 5            |\n",
      "|    agent/time/time_elapsed           | 13           |\n",
      "|    agent/time/total_timesteps        | 30720        |\n",
      "|    agent/train/approx_kl             | 0.0104925595 |\n",
      "|    agent/train/clip_fraction         | 0.307        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.79        |\n",
      "|    agent/train/explained_variance    | 0.848        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0394      |\n",
      "|    agent/train/n_updates             | 140          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0135      |\n",
      "|    agent/train/std                   | 0.977        |\n",
      "|    agent/train/value_loss            | 0.0116       |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -63      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -5.44    |\n",
      "|    agent/time/fps                      | 861      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 7        |\n",
      "|    agent/time/total_timesteps          | 2.66e+04 |\n",
      "|    agent/train/approx_kl               | 0.00846  |\n",
      "|    agent/train/clip_fraction           | 0.295    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.81    |\n",
      "|    agent/train/explained_variance      | 0.808    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0343  |\n",
      "|    agent/train/n_updates               | 130      |\n",
      "|    agent/train/policy_gradient_loss    | -0.0134  |\n",
      "|    agent/train/std                     | 0.985    |\n",
      "|    agent/train/value_loss              | 0.0134   |\n",
      "|    preferences/entropy                 | 0.226    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.628    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.185    |\n",
      "|    reward/epoch-0/train/loss           | 0.664    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.653    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.184    |\n",
      "|    reward/epoch-1/train/loss           | 0.614    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.663    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.184    |\n",
      "|    reward/epoch-2/train/loss           | 0.594    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.663    |\n",
      "|    final/train/gt_reward_loss          | 0.184    |\n",
      "|    final/train/loss                    | 0.594    |\n",
      "-----------------------------------------------------\n",
      "Collecting 170 fragments (8500 transitions)\n",
      "Sampling 425 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 370 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65530018a91c4c609f2ed3dda47b5016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -63.3       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.33       |\n",
      "|    agent/time/fps                    | 1118        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 1           |\n",
      "|    agent/time/total_timesteps        | 32768       |\n",
      "|    agent/train/approx_kl             | 0.007856287 |\n",
      "|    agent/train/clip_fraction         | 0.31        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.77       |\n",
      "|    agent/train/explained_variance    | 0.895       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0245     |\n",
      "|    agent/train/n_updates             | 150         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0131     |\n",
      "|    agent/train/std                   | 0.966       |\n",
      "|    agent/train/value_loss            | 0.01        |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -62.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.46       |\n",
      "|    agent/time/fps                    | 828         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 4           |\n",
      "|    agent/time/total_timesteps        | 34816       |\n",
      "|    agent/train/approx_kl             | 0.007852017 |\n",
      "|    agent/train/clip_fraction         | 0.303       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.74       |\n",
      "|    agent/train/explained_variance    | 0.843       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0261     |\n",
      "|    agent/train/n_updates             | 160         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0137     |\n",
      "|    agent/train/std                   | 0.948       |\n",
      "|    agent/train/value_loss            | 0.0118      |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61.9        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.6         |\n",
      "|    agent/time/fps                    | 782          |\n",
      "|    agent/time/iterations             | 3            |\n",
      "|    agent/time/time_elapsed           | 7            |\n",
      "|    agent/time/total_timesteps        | 36864        |\n",
      "|    agent/train/approx_kl             | 0.0099497875 |\n",
      "|    agent/train/clip_fraction         | 0.302        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.71        |\n",
      "|    agent/train/explained_variance    | 0.891        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0466      |\n",
      "|    agent/train/n_updates             | 170          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0105      |\n",
      "|    agent/train/std                   | 0.93         |\n",
      "|    agent/train/value_loss            | 0.0102       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.5        |\n",
      "|    agent/time/fps                    | 763         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 10          |\n",
      "|    agent/time/total_timesteps        | 38912       |\n",
      "|    agent/train/approx_kl             | 0.008202089 |\n",
      "|    agent/train/clip_fraction         | 0.318       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.67       |\n",
      "|    agent/train/explained_variance    | 0.93        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0468     |\n",
      "|    agent/train/n_updates             | 180         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0131     |\n",
      "|    agent/train/std                   | 0.918       |\n",
      "|    agent/train/value_loss            | 0.00738     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.1       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.37       |\n",
      "|    agent/time/fps                    | 751         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 13          |\n",
      "|    agent/time/total_timesteps        | 40960       |\n",
      "|    agent/train/approx_kl             | 0.012089671 |\n",
      "|    agent/train/clip_fraction         | 0.369       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.62       |\n",
      "|    agent/train/explained_variance    | 0.906       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0483     |\n",
      "|    agent/train/n_updates             | 190         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0151     |\n",
      "|    agent/train/std                   | 0.897       |\n",
      "|    agent/train/value_loss            | 0.00948     |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -62      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -5.45    |\n",
      "|    agent/time/fps                      | 848      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 7        |\n",
      "|    agent/time/total_timesteps          | 3.69e+04 |\n",
      "|    agent/train/approx_kl               | 0.00961  |\n",
      "|    agent/train/clip_fraction           | 0.324    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.66    |\n",
      "|    agent/train/explained_variance      | 0.901    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0425  |\n",
      "|    agent/train/n_updates               | 180      |\n",
      "|    agent/train/policy_gradient_loss    | -0.0131  |\n",
      "|    agent/train/std                     | 0.914    |\n",
      "|    agent/train/value_loss              | 0.00886  |\n",
      "|    preferences/entropy                 | 0.18     |\n",
      "|    reward/epoch-0/train/accuracy       | 0.644    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.209    |\n",
      "|    reward/epoch-0/train/loss           | 0.63     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.676    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.204    |\n",
      "|    reward/epoch-1/train/loss           | 0.608    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.687    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.198    |\n",
      "|    reward/epoch-2/train/loss           | 0.596    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.687    |\n",
      "|    final/train/gt_reward_loss          | 0.198    |\n",
      "|    final/train/loss                    | 0.596    |\n",
      "-----------------------------------------------------\n",
      "Collecting 146 fragments (7300 transitions)\n",
      "Sampling 365 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 445 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1dc5f8ffc574a0c8fae50e79b886679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.2       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.39       |\n",
      "|    agent/time/fps                    | 1074        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 1           |\n",
      "|    agent/time/total_timesteps        | 43008       |\n",
      "|    agent/train/approx_kl             | 0.009948848 |\n",
      "|    agent/train/clip_fraction         | 0.329       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.58       |\n",
      "|    agent/train/explained_variance    | 0.933       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0444     |\n",
      "|    agent/train/n_updates             | 200         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0128     |\n",
      "|    agent/train/std                   | 0.876       |\n",
      "|    agent/train/value_loss            | 0.00539     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.3        |\n",
      "|    agent/time/fps                    | 854         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 4           |\n",
      "|    agent/time/total_timesteps        | 45056       |\n",
      "|    agent/train/approx_kl             | 0.007091939 |\n",
      "|    agent/train/clip_fraction         | 0.307       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.56       |\n",
      "|    agent/train/explained_variance    | 0.877       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0332     |\n",
      "|    agent/train/n_updates             | 210         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00792    |\n",
      "|    agent/train/std                   | 0.87        |\n",
      "|    agent/train/value_loss            | 0.0061      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.13       |\n",
      "|    agent/time/fps                    | 802         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 7           |\n",
      "|    agent/time/total_timesteps        | 47104       |\n",
      "|    agent/train/approx_kl             | 0.009985581 |\n",
      "|    agent/train/clip_fraction         | 0.3         |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.54       |\n",
      "|    agent/train/explained_variance    | 0.908       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0323     |\n",
      "|    agent/train/n_updates             | 220         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00597    |\n",
      "|    agent/train/std                   | 0.86        |\n",
      "|    agent/train/value_loss            | 0.00651     |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -60.9        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -4.94        |\n",
      "|    agent/time/fps                    | 775          |\n",
      "|    agent/time/iterations             | 4            |\n",
      "|    agent/time/time_elapsed           | 10           |\n",
      "|    agent/time/total_timesteps        | 49152        |\n",
      "|    agent/train/approx_kl             | 0.0096049765 |\n",
      "|    agent/train/clip_fraction         | 0.354        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.5         |\n",
      "|    agent/train/explained_variance    | 0.937        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0121      |\n",
      "|    agent/train/n_updates             | 230          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0126      |\n",
      "|    agent/train/std                   | 0.844        |\n",
      "|    agent/train/value_loss            | 0.00454      |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -59.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -4.87       |\n",
      "|    agent/time/fps                    | 761         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 13          |\n",
      "|    agent/time/total_timesteps        | 51200       |\n",
      "|    agent/train/approx_kl             | 0.013074178 |\n",
      "|    agent/train/clip_fraction         | 0.371       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.44       |\n",
      "|    agent/train/explained_variance    | 0.926       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0296     |\n",
      "|    agent/train/n_updates             | 240         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0107     |\n",
      "|    agent/train/std                   | 0.812       |\n",
      "|    agent/train/value_loss            | 0.0052      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -61.1    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -5.12    |\n",
      "|    agent/time/fps                      | 853      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 7        |\n",
      "|    agent/time/total_timesteps          | 4.71e+04 |\n",
      "|    agent/train/approx_kl               | 0.01     |\n",
      "|    agent/train/clip_fraction           | 0.341    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.48    |\n",
      "|    agent/train/explained_variance      | 0.917    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0297  |\n",
      "|    agent/train/n_updates               | 230      |\n",
      "|    agent/train/policy_gradient_loss    | -0.00995 |\n",
      "|    agent/train/std                     | 0.834    |\n",
      "|    agent/train/value_loss              | 0.00527  |\n",
      "|    preferences/entropy                 | 0.17     |\n",
      "|    reward/epoch-0/train/accuracy       | 0.689    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.212    |\n",
      "|    reward/epoch-0/train/loss           | 0.61     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.713    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.212    |\n",
      "|    reward/epoch-1/train/loss           | 0.586    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.735    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.211    |\n",
      "|    reward/epoch-2/train/loss           | 0.577    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.735    |\n",
      "|    final/train/gt_reward_loss          | 0.211    |\n",
      "|    final/train/loss                    | 0.577    |\n",
      "-----------------------------------------------------\n",
      "Collecting 126 fragments (6300 transitions)\n",
      "Sampling 315 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 510 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f614fc9db771424eb57b27c7e2073aed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -60.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -4.88       |\n",
      "|    agent/time/fps                    | 1088        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 1           |\n",
      "|    agent/time/total_timesteps        | 53248       |\n",
      "|    agent/train/approx_kl             | 0.010434477 |\n",
      "|    agent/train/clip_fraction         | 0.372       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.36       |\n",
      "|    agent/train/explained_variance    | 0.94        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0413     |\n",
      "|    agent/train/n_updates             | 250         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0125     |\n",
      "|    agent/train/std                   | 0.787       |\n",
      "|    agent/train/value_loss            | 0.00397     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -59.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.12       |\n",
      "|    agent/time/fps                    | 862         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 4           |\n",
      "|    agent/time/total_timesteps        | 55296       |\n",
      "|    agent/train/approx_kl             | 0.009065969 |\n",
      "|    agent/train/clip_fraction         | 0.327       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.33       |\n",
      "|    agent/train/explained_variance    | 0.828       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0174     |\n",
      "|    agent/train/n_updates             | 260         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00603    |\n",
      "|    agent/train/std                   | 0.774       |\n",
      "|    agent/train/value_loss            | 0.00602     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -57.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.07       |\n",
      "|    agent/time/fps                    | 806         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 7           |\n",
      "|    agent/time/total_timesteps        | 57344       |\n",
      "|    agent/train/approx_kl             | 0.009718233 |\n",
      "|    agent/train/clip_fraction         | 0.326       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.31       |\n",
      "|    agent/train/explained_variance    | 0.873       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0308     |\n",
      "|    agent/train/n_updates             | 270         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00749    |\n",
      "|    agent/train/std                   | 0.765       |\n",
      "|    agent/train/value_loss            | 0.00509     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -56.3       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -4.89       |\n",
      "|    agent/time/fps                    | 779         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 10          |\n",
      "|    agent/time/total_timesteps        | 59392       |\n",
      "|    agent/train/approx_kl             | 0.011474676 |\n",
      "|    agent/train/clip_fraction         | 0.353       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.26       |\n",
      "|    agent/train/explained_variance    | 0.913       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0343     |\n",
      "|    agent/train/n_updates             | 280         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00881    |\n",
      "|    agent/train/std                   | 0.751       |\n",
      "|    agent/train/value_loss            | 0.00626     |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 50         |\n",
      "|    agent/rollout/ep_rew_mean         | -55.9      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -4.77      |\n",
      "|    agent/time/fps                    | 765        |\n",
      "|    agent/time/iterations             | 5          |\n",
      "|    agent/time/time_elapsed           | 13         |\n",
      "|    agent/time/total_timesteps        | 61440      |\n",
      "|    agent/train/approx_kl             | 0.01562935 |\n",
      "|    agent/train/clip_fraction         | 0.35       |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -2.24      |\n",
      "|    agent/train/explained_variance    | 0.892      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | -0.0457    |\n",
      "|    agent/train/n_updates             | 290        |\n",
      "|    agent/train/policy_gradient_loss  | -0.01      |\n",
      "|    agent/train/std                   | 0.744      |\n",
      "|    agent/train/value_loss            | 0.00643    |\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -58      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -4.95    |\n",
      "|    agent/time/fps                      | 860      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 7        |\n",
      "|    agent/time/total_timesteps          | 5.73e+04 |\n",
      "|    agent/train/approx_kl               | 0.0112   |\n",
      "|    agent/train/clip_fraction           | 0.34     |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.27    |\n",
      "|    agent/train/explained_variance      | 0.883    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0316  |\n",
      "|    agent/train/n_updates               | 280      |\n",
      "|    agent/train/policy_gradient_loss    | -0.00795 |\n",
      "|    agent/train/std                     | 0.754    |\n",
      "|    agent/train/value_loss              | 0.00587  |\n",
      "|    preferences/entropy                 | 0.128    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.71     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.214    |\n",
      "|    reward/epoch-0/train/loss           | 0.581    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.717    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.216    |\n",
      "|    reward/epoch-1/train/loss           | 0.563    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.718    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.215    |\n",
      "|    reward/epoch-2/train/loss           | 0.56     |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.718    |\n",
      "|    final/train/gt_reward_loss          | 0.215    |\n",
      "|    final/train/loss                    | 0.56     |\n",
      "-----------------------------------------------------\n",
      "Query schedule: [50, 127, 102, 85, 73, 63]\n",
      "Collecting 100 fragments (5000 transitions)\n",
      "Requested 4750 transitions but only 0 in buffer. Sampling 4750 additional transitions.\n",
      "Sampling 250 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 50 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b86cd5094840f1a2a2cab222c4ff89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "---------------------------------------------------\n",
      "| raw/                                 |          |\n",
      "|    agent/rollout/ep_len_mean         | 50       |\n",
      "|    agent/rollout/ep_rew_mean         | -60.2    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -28.7    |\n",
      "|    agent/time/fps                    | 1108     |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 1        |\n",
      "|    agent/time/total_timesteps        | 2048     |\n",
      "---------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -60.9        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -18.7        |\n",
      "|    agent/time/fps                    | 869          |\n",
      "|    agent/time/iterations             | 2            |\n",
      "|    agent/time/time_elapsed           | 4            |\n",
      "|    agent/time/total_timesteps        | 4096         |\n",
      "|    agent/train/approx_kl             | 0.0020185409 |\n",
      "|    agent/train/clip_fraction         | 0.0934       |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.85        |\n",
      "|    agent/train/explained_variance    | -0.266       |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.00968      |\n",
      "|    agent/train/n_updates             | 10           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00163     |\n",
      "|    agent/train/std                   | 1.01         |\n",
      "|    agent/train/value_loss            | 0.309        |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61.3        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -10.8        |\n",
      "|    agent/time/fps                    | 807          |\n",
      "|    agent/time/iterations             | 3            |\n",
      "|    agent/time/time_elapsed           | 7            |\n",
      "|    agent/time/total_timesteps        | 6144         |\n",
      "|    agent/train/approx_kl             | 0.0037505329 |\n",
      "|    agent/train/clip_fraction         | 0.177        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.85        |\n",
      "|    agent/train/explained_variance    | 0.456        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.00138     |\n",
      "|    agent/train/n_updates             | 20           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0073      |\n",
      "|    agent/train/std                   | 1.01         |\n",
      "|    agent/train/value_loss            | 0.13         |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.96        |\n",
      "|    agent/time/fps                    | 780          |\n",
      "|    agent/time/iterations             | 4            |\n",
      "|    agent/time/time_elapsed           | 10           |\n",
      "|    agent/time/total_timesteps        | 8192         |\n",
      "|    agent/train/approx_kl             | 0.0051457062 |\n",
      "|    agent/train/clip_fraction         | 0.183        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.85        |\n",
      "|    agent/train/explained_variance    | 0.69         |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0319      |\n",
      "|    agent/train/n_updates             | 30           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00976     |\n",
      "|    agent/train/std                   | 1            |\n",
      "|    agent/train/value_loss            | 0.0877       |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -60.9        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.82        |\n",
      "|    agent/time/fps                    | 747          |\n",
      "|    agent/time/iterations             | 5            |\n",
      "|    agent/time/time_elapsed           | 13           |\n",
      "|    agent/time/total_timesteps        | 10240        |\n",
      "|    agent/train/approx_kl             | 0.0045557804 |\n",
      "|    agent/train/clip_fraction         | 0.211        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.84        |\n",
      "|    agent/train/explained_variance    | 0.6          |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.017       |\n",
      "|    agent/train/n_updates             | 40           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0104      |\n",
      "|    agent/train/std                   | 1            |\n",
      "|    agent/train/value_loss            | 0.0888       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                   |          |\n",
      "|    agent/rollout/ep_len_mean            | 50       |\n",
      "|    agent/rollout/ep_rew_mean            | -60.8    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean    | -14.4    |\n",
      "|    agent/time/fps                       | 862      |\n",
      "|    agent/time/iterations                | 3        |\n",
      "|    agent/time/time_elapsed              | 7        |\n",
      "|    agent/time/total_timesteps           | 6.14e+03 |\n",
      "|    agent/train/approx_kl                | 0.00437  |\n",
      "|    agent/train/clip_fraction            | 0.182    |\n",
      "|    agent/train/clip_range               | 0.1      |\n",
      "|    agent/train/entropy_loss             | -2.85    |\n",
      "|    agent/train/explained_variance       | 0.44     |\n",
      "|    agent/train/learning_rate            | 0.002    |\n",
      "|    agent/train/loss                     | -0.0132  |\n",
      "|    agent/train/n_updates                | 30       |\n",
      "|    agent/train/policy_gradient_loss     | -0.00862 |\n",
      "|    agent/train/std                      | 1        |\n",
      "|    agent/train/value_loss               | 0.132    |\n",
      "|    preferences/entropy                  | 0.125    |\n",
      "|    reward/epoch-0/train/accuracy        | 0.503    |\n",
      "|    reward/epoch-0/train/gt_reward_loss  | 0.0524   |\n",
      "|    reward/epoch-0/train/loss            | 1.68     |\n",
      "|    reward/epoch-1/train/accuracy        | 0.484    |\n",
      "|    reward/epoch-1/train/gt_reward_loss  | 0.0605   |\n",
      "|    reward/epoch-1/train/loss            | 1.2      |\n",
      "|    reward/epoch-10/train/accuracy       | 0.894    |\n",
      "|    reward/epoch-10/train/gt_reward_loss | 0.0585   |\n",
      "|    reward/epoch-10/train/loss           | 0.346    |\n",
      "|    reward/epoch-11/train/accuracy       | 0.91     |\n",
      "|    reward/epoch-11/train/gt_reward_loss | 0.0572   |\n",
      "|    reward/epoch-11/train/loss           | 0.331    |\n",
      "|    reward/epoch-2/train/accuracy        | 0.54     |\n",
      "|    reward/epoch-2/train/gt_reward_loss  | 0.0626   |\n",
      "|    reward/epoch-2/train/loss            | 0.915    |\n",
      "|    reward/epoch-3/train/accuracy        | 0.587    |\n",
      "|    reward/epoch-3/train/gt_reward_loss  | 0.0571   |\n",
      "|    reward/epoch-3/train/loss            | 0.8      |\n",
      "|    reward/epoch-4/train/accuracy        | 0.655    |\n",
      "|    reward/epoch-4/train/gt_reward_loss  | 0.0659   |\n",
      "|    reward/epoch-4/train/loss            | 0.634    |\n",
      "|    reward/epoch-5/train/accuracy        | 0.733    |\n",
      "|    reward/epoch-5/train/gt_reward_loss  | 0.0586   |\n",
      "|    reward/epoch-5/train/loss            | 0.578    |\n",
      "|    reward/epoch-6/train/accuracy        | 0.712    |\n",
      "|    reward/epoch-6/train/gt_reward_loss  | 0.0653   |\n",
      "|    reward/epoch-6/train/loss            | 0.552    |\n",
      "|    reward/epoch-7/train/accuracy        | 0.78     |\n",
      "|    reward/epoch-7/train/gt_reward_loss  | 0.0527   |\n",
      "|    reward/epoch-7/train/loss            | 0.458    |\n",
      "|    reward/epoch-8/train/accuracy        | 0.811    |\n",
      "|    reward/epoch-8/train/gt_reward_loss  | 0.0612   |\n",
      "|    reward/epoch-8/train/loss            | 0.452    |\n",
      "|    reward/epoch-9/train/accuracy        | 0.87     |\n",
      "|    reward/epoch-9/train/gt_reward_loss  | 0.0618   |\n",
      "|    reward/epoch-9/train/loss            | 0.398    |\n",
      "| reward/                                 |          |\n",
      "|    final/train/accuracy                 | 0.91     |\n",
      "|    final/train/gt_reward_loss           | 0.0572   |\n",
      "|    final/train/loss                     | 0.331    |\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting 254 fragments (12700 transitions)\n",
      "Requested 12065 transitions but only 10200 in buffer. Sampling 1865 additional transitions.\n",
      "Sampling 635 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 180 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4359a12c0aa8419eb4ebc22f685a0ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -60.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.35       |\n",
      "|    agent/time/fps                    | 1108        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 1           |\n",
      "|    agent/time/total_timesteps        | 12288       |\n",
      "|    agent/train/approx_kl             | 0.006401297 |\n",
      "|    agent/train/clip_fraction         | 0.247       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.84       |\n",
      "|    agent/train/explained_variance    | 0.718       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0252     |\n",
      "|    agent/train/n_updates             | 50          |\n",
      "|    agent/train/policy_gradient_loss  | -0.014      |\n",
      "|    agent/train/std                   | 1           |\n",
      "|    agent/train/value_loss            | 0.0441      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.6        |\n",
      "|    agent/time/fps                    | 842         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 4           |\n",
      "|    agent/time/total_timesteps        | 14336       |\n",
      "|    agent/train/approx_kl             | 0.008066907 |\n",
      "|    agent/train/clip_fraction         | 0.265       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.84       |\n",
      "|    agent/train/explained_variance    | 0.577       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0411     |\n",
      "|    agent/train/n_updates             | 60          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0135     |\n",
      "|    agent/train/std                   | 0.995       |\n",
      "|    agent/train/value_loss            | 0.0431      |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61.6        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.91        |\n",
      "|    agent/time/fps                    | 796          |\n",
      "|    agent/time/iterations             | 3            |\n",
      "|    agent/time/time_elapsed           | 7            |\n",
      "|    agent/time/total_timesteps        | 16384        |\n",
      "|    agent/train/approx_kl             | 0.0070372736 |\n",
      "|    agent/train/clip_fraction         | 0.277        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.83        |\n",
      "|    agent/train/explained_variance    | 0.71         |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0326      |\n",
      "|    agent/train/n_updates             | 70           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0142      |\n",
      "|    agent/train/std                   | 0.994        |\n",
      "|    agent/train/value_loss            | 0.0385       |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61.6        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.77        |\n",
      "|    agent/time/fps                    | 775          |\n",
      "|    agent/time/iterations             | 4            |\n",
      "|    agent/time/time_elapsed           | 10           |\n",
      "|    agent/time/total_timesteps        | 18432        |\n",
      "|    agent/train/approx_kl             | 0.0071030445 |\n",
      "|    agent/train/clip_fraction         | 0.274        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.8         |\n",
      "|    agent/train/explained_variance    | 0.798        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0587      |\n",
      "|    agent/train/n_updates             | 80           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0146      |\n",
      "|    agent/train/std                   | 0.981        |\n",
      "|    agent/train/value_loss            | 0.0333       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.1       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.57       |\n",
      "|    agent/time/fps                    | 763         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 13          |\n",
      "|    agent/time/total_timesteps        | 20480       |\n",
      "|    agent/train/approx_kl             | 0.008640708 |\n",
      "|    agent/train/clip_fraction         | 0.298       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.78       |\n",
      "|    agent/train/explained_variance    | 0.795       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.049      |\n",
      "|    agent/train/n_updates             | 90          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0163     |\n",
      "|    agent/train/std                   | 0.97        |\n",
      "|    agent/train/value_loss            | 0.0245      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -61.4    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -6.64    |\n",
      "|    agent/time/fps                      | 857      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 7        |\n",
      "|    agent/time/total_timesteps          | 1.64e+04 |\n",
      "|    agent/train/approx_kl               | 0.00783  |\n",
      "|    agent/train/clip_fraction           | 0.281    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.8     |\n",
      "|    agent/train/explained_variance      | 0.752    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0437  |\n",
      "|    agent/train/n_updates               | 80       |\n",
      "|    agent/train/policy_gradient_loss    | -0.0142  |\n",
      "|    agent/train/std                     | 0.98     |\n",
      "|    agent/train/value_loss              | 0.0317   |\n",
      "|    preferences/entropy                 | 0.219    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.694    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.129    |\n",
      "|    reward/epoch-0/train/loss           | 0.655    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.663    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.132    |\n",
      "|    reward/epoch-1/train/loss           | 0.64     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.682    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.124    |\n",
      "|    reward/epoch-2/train/loss           | 0.579    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.682    |\n",
      "|    final/train/gt_reward_loss          | 0.124    |\n",
      "|    final/train/loss                    | 0.579    |\n",
      "-----------------------------------------------------\n",
      "Collecting 204 fragments (10200 transitions)\n",
      "Sampling 510 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 285 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad82a56b88da4abbb642be1e81742c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -60.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.32       |\n",
      "|    agent/time/fps                    | 1117        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 1           |\n",
      "|    agent/time/total_timesteps        | 22528       |\n",
      "|    agent/train/approx_kl             | 0.008305134 |\n",
      "|    agent/train/clip_fraction         | 0.289       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.76       |\n",
      "|    agent/train/explained_variance    | 0.88        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0372     |\n",
      "|    agent/train/n_updates             | 100         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0122     |\n",
      "|    agent/train/std                   | 0.959       |\n",
      "|    agent/train/value_loss            | 0.0189      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -60.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.36       |\n",
      "|    agent/time/fps                    | 852         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 4           |\n",
      "|    agent/time/total_timesteps        | 24576       |\n",
      "|    agent/train/approx_kl             | 0.006750157 |\n",
      "|    agent/train/clip_fraction         | 0.296       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.74       |\n",
      "|    agent/train/explained_variance    | 0.893       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0283     |\n",
      "|    agent/train/n_updates             | 110         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0123     |\n",
      "|    agent/train/std                   | 0.951       |\n",
      "|    agent/train/value_loss            | 0.0197      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -60.9       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.47       |\n",
      "|    agent/time/fps                    | 777         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 7           |\n",
      "|    agent/time/total_timesteps        | 26624       |\n",
      "|    agent/train/approx_kl             | 0.011193913 |\n",
      "|    agent/train/clip_fraction         | 0.344       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.72       |\n",
      "|    agent/train/explained_variance    | 0.879       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0386     |\n",
      "|    agent/train/n_updates             | 120         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0174     |\n",
      "|    agent/train/std                   | 0.94        |\n",
      "|    agent/train/value_loss            | 0.0221      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -59.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.55       |\n",
      "|    agent/time/fps                    | 760         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 10          |\n",
      "|    agent/time/total_timesteps        | 28672       |\n",
      "|    agent/train/approx_kl             | 0.009825073 |\n",
      "|    agent/train/clip_fraction         | 0.332       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.68       |\n",
      "|    agent/train/explained_variance    | 0.908       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0433     |\n",
      "|    agent/train/n_updates             | 130         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0176     |\n",
      "|    agent/train/std                   | 0.919       |\n",
      "|    agent/train/value_loss            | 0.0186      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -59.2       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.31       |\n",
      "|    agent/time/fps                    | 751         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 13          |\n",
      "|    agent/time/total_timesteps        | 30720       |\n",
      "|    agent/train/approx_kl             | 0.012375911 |\n",
      "|    agent/train/clip_fraction         | 0.349       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.62       |\n",
      "|    agent/train/explained_variance    | 0.91        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0306     |\n",
      "|    agent/train/n_updates             | 140         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0142     |\n",
      "|    agent/train/std                   | 0.897       |\n",
      "|    agent/train/value_loss            | 0.0188      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -60.3    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -6.4     |\n",
      "|    agent/time/fps                      | 851      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 7        |\n",
      "|    agent/time/total_timesteps          | 2.66e+04 |\n",
      "|    agent/train/approx_kl               | 0.0103   |\n",
      "|    agent/train/clip_fraction           | 0.33     |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.67    |\n",
      "|    agent/train/explained_variance      | 0.907    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0333  |\n",
      "|    agent/train/n_updates               | 130      |\n",
      "|    agent/train/policy_gradient_loss    | -0.015   |\n",
      "|    agent/train/std                     | 0.918    |\n",
      "|    agent/train/value_loss              | 0.0181   |\n",
      "|    preferences/entropy                 | 0.213    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.655    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.148    |\n",
      "|    reward/epoch-0/train/loss           | 0.628    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.666    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.149    |\n",
      "|    reward/epoch-1/train/loss           | 0.589    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.66     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.148    |\n",
      "|    reward/epoch-2/train/loss           | 0.56     |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.66     |\n",
      "|    final/train/gt_reward_loss          | 0.148    |\n",
      "|    final/train/loss                    | 0.56     |\n",
      "-----------------------------------------------------\n",
      "Collecting 170 fragments (8500 transitions)\n",
      "Sampling 425 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 370 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd7482ca9ce4683a34602db2cc047a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 50         |\n",
      "|    agent/rollout/ep_rew_mean         | -59.1      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.1       |\n",
      "|    agent/time/fps                    | 912        |\n",
      "|    agent/time/iterations             | 1          |\n",
      "|    agent/time/time_elapsed           | 2          |\n",
      "|    agent/time/total_timesteps        | 32768      |\n",
      "|    agent/train/approx_kl             | 0.01132974 |\n",
      "|    agent/train/clip_fraction         | 0.33       |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -2.6       |\n",
      "|    agent/train/explained_variance    | 0.948      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | -0.0259    |\n",
      "|    agent/train/n_updates             | 150        |\n",
      "|    agent/train/policy_gradient_loss  | -0.0137    |\n",
      "|    agent/train/std                   | 0.884      |\n",
      "|    agent/train/value_loss            | 0.0112     |\n",
      "-----------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -58.5       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.39       |\n",
      "|    agent/time/fps                    | 746         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 5           |\n",
      "|    agent/time/total_timesteps        | 34816       |\n",
      "|    agent/train/approx_kl             | 0.008380967 |\n",
      "|    agent/train/clip_fraction         | 0.31        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.58       |\n",
      "|    agent/train/explained_variance    | 0.921       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0385     |\n",
      "|    agent/train/n_updates             | 160         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00823    |\n",
      "|    agent/train/std                   | 0.878       |\n",
      "|    agent/train/value_loss            | 0.0122      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -57.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.61       |\n",
      "|    agent/time/fps                    | 707         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 8           |\n",
      "|    agent/time/total_timesteps        | 36864       |\n",
      "|    agent/train/approx_kl             | 0.010947444 |\n",
      "|    agent/train/clip_fraction         | 0.336       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.53       |\n",
      "|    agent/train/explained_variance    | 0.893       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.051      |\n",
      "|    agent/train/n_updates             | 170         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0138     |\n",
      "|    agent/train/std                   | 0.854       |\n",
      "|    agent/train/value_loss            | 0.0166      |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -56.6        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.6         |\n",
      "|    agent/time/fps                    | 680          |\n",
      "|    agent/time/iterations             | 4            |\n",
      "|    agent/time/time_elapsed           | 12           |\n",
      "|    agent/time/total_timesteps        | 38912        |\n",
      "|    agent/train/approx_kl             | 0.0098821325 |\n",
      "|    agent/train/clip_fraction         | 0.323        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.49        |\n",
      "|    agent/train/explained_variance    | 0.952        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0181      |\n",
      "|    agent/train/n_updates             | 180          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0117      |\n",
      "|    agent/train/std                   | 0.843        |\n",
      "|    agent/train/value_loss            | 0.0101       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -56.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.38       |\n",
      "|    agent/time/fps                    | 644         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 15          |\n",
      "|    agent/time/total_timesteps        | 40960       |\n",
      "|    agent/train/approx_kl             | 0.011986613 |\n",
      "|    agent/train/clip_fraction         | 0.355       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.46       |\n",
      "|    agent/train/explained_variance    | 0.93        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0337     |\n",
      "|    agent/train/n_updates             | 190         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0133     |\n",
      "|    agent/train/std                   | 0.827       |\n",
      "|    agent/train/value_loss            | 0.00906     |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -57.7    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -6.42    |\n",
      "|    agent/time/fps                      | 738      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 8.4      |\n",
      "|    agent/time/total_timesteps          | 3.69e+04 |\n",
      "|    agent/train/approx_kl               | 0.0103   |\n",
      "|    agent/train/clip_fraction           | 0.332    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.49    |\n",
      "|    agent/train/explained_variance      | 0.927    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0327  |\n",
      "|    agent/train/n_updates               | 180      |\n",
      "|    agent/train/policy_gradient_loss    | -0.0117  |\n",
      "|    agent/train/std                     | 0.843    |\n",
      "|    agent/train/value_loss              | 0.0112   |\n",
      "|    preferences/entropy                 | 0.175    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.664    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.154    |\n",
      "|    reward/epoch-0/train/loss           | 0.577    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.679    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.154    |\n",
      "|    reward/epoch-1/train/loss           | 0.554    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.699    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.151    |\n",
      "|    reward/epoch-2/train/loss           | 0.538    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.699    |\n",
      "|    final/train/gt_reward_loss          | 0.151    |\n",
      "|    final/train/loss                    | 0.538    |\n",
      "-----------------------------------------------------\n",
      "Collecting 146 fragments (7300 transitions)\n",
      "Sampling 365 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 445 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba71949e455d47c1883d5bf9969d5e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -56.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.43       |\n",
      "|    agent/time/fps                    | 977         |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 2           |\n",
      "|    agent/time/total_timesteps        | 43008       |\n",
      "|    agent/train/approx_kl             | 0.010507662 |\n",
      "|    agent/train/clip_fraction         | 0.334       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.41       |\n",
      "|    agent/train/explained_variance    | 0.941       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.022      |\n",
      "|    agent/train/n_updates             | 200         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0112     |\n",
      "|    agent/train/std                   | 0.811       |\n",
      "|    agent/train/value_loss            | 0.00788     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -56.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.58       |\n",
      "|    agent/time/fps                    | 804         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 5           |\n",
      "|    agent/time/total_timesteps        | 45056       |\n",
      "|    agent/train/approx_kl             | 0.011638487 |\n",
      "|    agent/train/clip_fraction         | 0.355       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.37       |\n",
      "|    agent/train/explained_variance    | 0.929       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0399     |\n",
      "|    agent/train/n_updates             | 210         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0111     |\n",
      "|    agent/train/std                   | 0.789       |\n",
      "|    agent/train/value_loss            | 0.0097      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -55.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.76       |\n",
      "|    agent/time/fps                    | 771         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 7           |\n",
      "|    agent/time/total_timesteps        | 47104       |\n",
      "|    agent/train/approx_kl             | 0.015037538 |\n",
      "|    agent/train/clip_fraction         | 0.348       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.32       |\n",
      "|    agent/train/explained_variance    | 0.915       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0242     |\n",
      "|    agent/train/n_updates             | 220         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0139     |\n",
      "|    agent/train/std                   | 0.765       |\n",
      "|    agent/train/value_loss            | 0.0114      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -54.2       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.7        |\n",
      "|    agent/time/fps                    | 755         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 10          |\n",
      "|    agent/time/total_timesteps        | 49152       |\n",
      "|    agent/train/approx_kl             | 0.011895161 |\n",
      "|    agent/train/clip_fraction         | 0.367       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.27       |\n",
      "|    agent/train/explained_variance    | 0.927       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0273     |\n",
      "|    agent/train/n_updates             | 230         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0101     |\n",
      "|    agent/train/std                   | 0.75        |\n",
      "|    agent/train/value_loss            | 0.00967     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -52.9       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.56       |\n",
      "|    agent/time/fps                    | 746         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 13          |\n",
      "|    agent/time/total_timesteps        | 51200       |\n",
      "|    agent/train/approx_kl             | 0.010688551 |\n",
      "|    agent/train/clip_fraction         | 0.364       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.22       |\n",
      "|    agent/train/explained_variance    | 0.899       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0314     |\n",
      "|    agent/train/n_updates             | 240         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00829    |\n",
      "|    agent/train/std                   | 0.733       |\n",
      "|    agent/train/value_loss            | 0.0111      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -55.3    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -6.61    |\n",
      "|    agent/time/fps                      | 811      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 7.4      |\n",
      "|    agent/time/total_timesteps          | 4.71e+04 |\n",
      "|    agent/train/approx_kl               | 0.0122   |\n",
      "|    agent/train/clip_fraction           | 0.36     |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.27    |\n",
      "|    agent/train/explained_variance      | 0.905    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0305  |\n",
      "|    agent/train/n_updates               | 230      |\n",
      "|    agent/train/policy_gradient_loss    | -0.0102  |\n",
      "|    agent/train/std                     | 0.752    |\n",
      "|    agent/train/value_loss              | 0.0103   |\n",
      "|    preferences/entropy                 | 0.169    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.683    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.156    |\n",
      "|    reward/epoch-0/train/loss           | 0.575    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.696    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.156    |\n",
      "|    reward/epoch-1/train/loss           | 0.565    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.716    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.156    |\n",
      "|    reward/epoch-2/train/loss           | 0.553    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.716    |\n",
      "|    final/train/gt_reward_loss          | 0.156    |\n",
      "|    final/train/loss                    | 0.553    |\n",
      "-----------------------------------------------------\n",
      "Collecting 126 fragments (6300 transitions)\n",
      "Sampling 315 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 510 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e10c23b9d243b2ab6f81c4c7da056a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -52.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.59       |\n",
      "|    agent/time/fps                    | 1062        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 1           |\n",
      "|    agent/time/total_timesteps        | 53248       |\n",
      "|    agent/train/approx_kl             | 0.011558592 |\n",
      "|    agent/train/clip_fraction         | 0.366       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.19       |\n",
      "|    agent/train/explained_variance    | 0.856       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0298     |\n",
      "|    agent/train/n_updates             | 250         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00795    |\n",
      "|    agent/train/std                   | 0.724       |\n",
      "|    agent/train/value_loss            | 0.00944     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -52.2       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.93       |\n",
      "|    agent/time/fps                    | 857         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 4           |\n",
      "|    agent/time/total_timesteps        | 55296       |\n",
      "|    agent/train/approx_kl             | 0.013049027 |\n",
      "|    agent/train/clip_fraction         | 0.366       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.15       |\n",
      "|    agent/train/explained_variance    | 0.913       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0159     |\n",
      "|    agent/train/n_updates             | 260         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00872    |\n",
      "|    agent/train/std                   | 0.703       |\n",
      "|    agent/train/value_loss            | 0.0135      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -50.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.01       |\n",
      "|    agent/time/fps                    | 794         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 7           |\n",
      "|    agent/time/total_timesteps        | 57344       |\n",
      "|    agent/train/approx_kl             | 0.010848326 |\n",
      "|    agent/train/clip_fraction         | 0.342       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.1        |\n",
      "|    agent/train/explained_variance    | 0.884       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0233     |\n",
      "|    agent/train/n_updates             | 270         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0115     |\n",
      "|    agent/train/std                   | 0.689       |\n",
      "|    agent/train/value_loss            | 0.0226      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -48.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.79       |\n",
      "|    agent/time/fps                    | 769         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 10          |\n",
      "|    agent/time/total_timesteps        | 59392       |\n",
      "|    agent/train/approx_kl             | 0.017872635 |\n",
      "|    agent/train/clip_fraction         | 0.406       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.03       |\n",
      "|    agent/train/explained_variance    | 0.909       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0266     |\n",
      "|    agent/train/n_updates             | 280         |\n",
      "|    agent/train/policy_gradient_loss  | -0.011      |\n",
      "|    agent/train/std                   | 0.664       |\n",
      "|    agent/train/value_loss            | 0.0192      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -48.2       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.47       |\n",
      "|    agent/time/fps                    | 755         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 13          |\n",
      "|    agent/time/total_timesteps        | 61440       |\n",
      "|    agent/train/approx_kl             | 0.015441405 |\n",
      "|    agent/train/clip_fraction         | 0.385       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.98       |\n",
      "|    agent/train/explained_variance    | 0.917       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0127     |\n",
      "|    agent/train/n_updates             | 290         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00969    |\n",
      "|    agent/train/std                   | 0.649       |\n",
      "|    agent/train/value_loss            | 0.0162      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -50.5    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -6.76    |\n",
      "|    agent/time/fps                      | 847      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 7        |\n",
      "|    agent/time/total_timesteps          | 5.73e+04 |\n",
      "|    agent/train/approx_kl               | 0.0143   |\n",
      "|    agent/train/clip_fraction           | 0.381    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.04    |\n",
      "|    agent/train/explained_variance      | 0.914    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0208  |\n",
      "|    agent/train/n_updates               | 280      |\n",
      "|    agent/train/policy_gradient_loss    | -0.0103  |\n",
      "|    agent/train/std                     | 0.667    |\n",
      "|    agent/train/value_loss              | 0.0167   |\n",
      "|    preferences/entropy                 | 0.152    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.7      |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.169    |\n",
      "|    reward/epoch-0/train/loss           | 0.55     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.735    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.169    |\n",
      "|    reward/epoch-1/train/loss           | 0.529    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.743    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.169    |\n",
      "|    reward/epoch-2/train/loss           | 0.517    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.743    |\n",
      "|    final/train/gt_reward_loss          | 0.169    |\n",
      "|    final/train/loss                    | 0.517    |\n",
      "-----------------------------------------------------\n",
      "Query schedule: [50, 127, 102, 85, 73, 63]\n",
      "Collecting 100 fragments (5000 transitions)\n",
      "Requested 4750 transitions but only 0 in buffer. Sampling 4750 additional transitions.\n",
      "Sampling 250 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 50 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da5ec603f96f42f2a43595ecf51febfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "---------------------------------------------------\n",
      "| raw/                                 |          |\n",
      "|    agent/rollout/ep_len_mean         | 50       |\n",
      "|    agent/rollout/ep_rew_mean         | -60.2    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -28.7    |\n",
      "|    agent/time/fps                    | 1054     |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 1        |\n",
      "|    agent/time/total_timesteps        | 2048     |\n",
      "---------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -60.9        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -18.1        |\n",
      "|    agent/time/fps                    | 875          |\n",
      "|    agent/time/iterations             | 2            |\n",
      "|    agent/time/time_elapsed           | 4            |\n",
      "|    agent/time/total_timesteps        | 4096         |\n",
      "|    agent/train/approx_kl             | 0.0031727315 |\n",
      "|    agent/train/clip_fraction         | 0.122        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.85        |\n",
      "|    agent/train/explained_variance    | -0.174       |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.000727    |\n",
      "|    agent/train/n_updates             | 10           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00331     |\n",
      "|    agent/train/std                   | 1.01         |\n",
      "|    agent/train/value_loss            | 0.222        |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61.4        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -9.55        |\n",
      "|    agent/time/fps                    | 792          |\n",
      "|    agent/time/iterations             | 3            |\n",
      "|    agent/time/time_elapsed           | 7            |\n",
      "|    agent/time/total_timesteps        | 6144         |\n",
      "|    agent/train/approx_kl             | 0.0040879576 |\n",
      "|    agent/train/clip_fraction         | 0.179        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.85        |\n",
      "|    agent/train/explained_variance    | 0.388        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0161      |\n",
      "|    agent/train/n_updates             | 20           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00859     |\n",
      "|    agent/train/std                   | 1.01         |\n",
      "|    agent/train/value_loss            | 0.0808       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.5       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.48       |\n",
      "|    agent/time/fps                    | 769         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 10          |\n",
      "|    agent/time/total_timesteps        | 8192        |\n",
      "|    agent/train/approx_kl             | 0.003964113 |\n",
      "|    agent/train/clip_fraction         | 0.183       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.86       |\n",
      "|    agent/train/explained_variance    | 0.676       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.029      |\n",
      "|    agent/train/n_updates             | 30          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00879    |\n",
      "|    agent/train/std                   | 1.01        |\n",
      "|    agent/train/value_loss            | 0.0528      |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61.9        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.35        |\n",
      "|    agent/time/fps                    | 754          |\n",
      "|    agent/time/iterations             | 5            |\n",
      "|    agent/time/time_elapsed           | 13           |\n",
      "|    agent/time/total_timesteps        | 10240        |\n",
      "|    agent/train/approx_kl             | 0.0045644892 |\n",
      "|    agent/train/clip_fraction         | 0.213        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.87        |\n",
      "|    agent/train/explained_variance    | 0.712        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0277      |\n",
      "|    agent/train/n_updates             | 40           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0115      |\n",
      "|    agent/train/std                   | 1.01         |\n",
      "|    agent/train/value_loss            | 0.0417       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                   |          |\n",
      "|    agent/rollout/ep_len_mean            | 50       |\n",
      "|    agent/rollout/ep_rew_mean            | -61.2    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean    | -13.4    |\n",
      "|    agent/time/fps                       | 849      |\n",
      "|    agent/time/iterations                | 3        |\n",
      "|    agent/time/time_elapsed              | 7        |\n",
      "|    agent/time/total_timesteps           | 6.14e+03 |\n",
      "|    agent/train/approx_kl                | 0.00443  |\n",
      "|    agent/train/clip_fraction            | 0.191    |\n",
      "|    agent/train/clip_range               | 0.1      |\n",
      "|    agent/train/entropy_loss             | -2.86    |\n",
      "|    agent/train/explained_variance       | 0.473    |\n",
      "|    agent/train/learning_rate            | 0.002    |\n",
      "|    agent/train/loss                     | -0.0197  |\n",
      "|    agent/train/n_updates                | 30       |\n",
      "|    agent/train/policy_gradient_loss     | -0.00907 |\n",
      "|    agent/train/std                      | 1.01     |\n",
      "|    agent/train/value_loss               | 0.0872   |\n",
      "|    preferences/entropy                  | 0.147    |\n",
      "|    reward/epoch-0/train/accuracy        | 0.441    |\n",
      "|    reward/epoch-0/train/gt_reward_loss  | 0.112    |\n",
      "|    reward/epoch-0/train/loss            | 1.89     |\n",
      "|    reward/epoch-1/train/accuracy        | 0.41     |\n",
      "|    reward/epoch-1/train/gt_reward_loss  | 0.1      |\n",
      "|    reward/epoch-1/train/loss            | 1.53     |\n",
      "|    reward/epoch-10/train/accuracy       | 0.712    |\n",
      "|    reward/epoch-10/train/gt_reward_loss | 0.0904   |\n",
      "|    reward/epoch-10/train/loss           | 0.597    |\n",
      "|    reward/epoch-11/train/accuracy       | 0.736    |\n",
      "|    reward/epoch-11/train/gt_reward_loss | 0.102    |\n",
      "|    reward/epoch-11/train/loss           | 0.562    |\n",
      "|    reward/epoch-2/train/accuracy        | 0.476    |\n",
      "|    reward/epoch-2/train/gt_reward_loss  | 0.116    |\n",
      "|    reward/epoch-2/train/loss            | 1.33     |\n",
      "|    reward/epoch-3/train/accuracy        | 0.488    |\n",
      "|    reward/epoch-3/train/gt_reward_loss  | 0.103    |\n",
      "|    reward/epoch-3/train/loss            | 0.958    |\n",
      "|    reward/epoch-4/train/accuracy        | 0.599    |\n",
      "|    reward/epoch-4/train/gt_reward_loss  | 0.0775   |\n",
      "|    reward/epoch-4/train/loss            | 0.761    |\n",
      "|    reward/epoch-5/train/accuracy        | 0.661    |\n",
      "|    reward/epoch-5/train/gt_reward_loss  | 0.0792   |\n",
      "|    reward/epoch-5/train/loss            | 0.642    |\n",
      "|    reward/epoch-6/train/accuracy        | 0.708    |\n",
      "|    reward/epoch-6/train/gt_reward_loss  | 0.0866   |\n",
      "|    reward/epoch-6/train/loss            | 0.624    |\n",
      "|    reward/epoch-7/train/accuracy        | 0.736    |\n",
      "|    reward/epoch-7/train/gt_reward_loss  | 0.0926   |\n",
      "|    reward/epoch-7/train/loss            | 0.603    |\n",
      "|    reward/epoch-8/train/accuracy        | 0.748    |\n",
      "|    reward/epoch-8/train/gt_reward_loss  | 0.09     |\n",
      "|    reward/epoch-8/train/loss            | 0.594    |\n",
      "|    reward/epoch-9/train/accuracy        | 0.748    |\n",
      "|    reward/epoch-9/train/gt_reward_loss  | 0.0826   |\n",
      "|    reward/epoch-9/train/loss            | 0.535    |\n",
      "| reward/                                 |          |\n",
      "|    final/train/accuracy                 | 0.736    |\n",
      "|    final/train/gt_reward_loss           | 0.102    |\n",
      "|    final/train/loss                     | 0.562    |\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting 254 fragments (12700 transitions)\n",
      "Requested 12065 transitions but only 10200 in buffer. Sampling 1865 additional transitions.\n",
      "Sampling 635 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 180 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342dfeb37c8a46f9b027c015d6db90fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61.9        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.05        |\n",
      "|    agent/time/fps                    | 1047         |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 1            |\n",
      "|    agent/time/total_timesteps        | 12288        |\n",
      "|    agent/train/approx_kl             | 0.0063374992 |\n",
      "|    agent/train/clip_fraction         | 0.256        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.85        |\n",
      "|    agent/train/explained_variance    | 0.761        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0251      |\n",
      "|    agent/train/n_updates             | 50           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0132      |\n",
      "|    agent/train/std                   | 1            |\n",
      "|    agent/train/value_loss            | 0.0382       |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -62.6        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.65        |\n",
      "|    agent/time/fps                    | 863          |\n",
      "|    agent/time/iterations             | 2            |\n",
      "|    agent/time/time_elapsed           | 4            |\n",
      "|    agent/time/total_timesteps        | 14336        |\n",
      "|    agent/train/approx_kl             | 0.0050906595 |\n",
      "|    agent/train/clip_fraction         | 0.241        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.85        |\n",
      "|    agent/train/explained_variance    | 0.569        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0356      |\n",
      "|    agent/train/n_updates             | 60           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00998     |\n",
      "|    agent/train/std                   | 1.01         |\n",
      "|    agent/train/value_loss            | 0.0536       |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -63.1        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.37        |\n",
      "|    agent/time/fps                    | 772          |\n",
      "|    agent/time/iterations             | 3            |\n",
      "|    agent/time/time_elapsed           | 7            |\n",
      "|    agent/time/total_timesteps        | 16384        |\n",
      "|    agent/train/approx_kl             | 0.0055931304 |\n",
      "|    agent/train/clip_fraction         | 0.226        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.84        |\n",
      "|    agent/train/explained_variance    | 0.726        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0411      |\n",
      "|    agent/train/n_updates             | 70           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0114      |\n",
      "|    agent/train/std                   | 1            |\n",
      "|    agent/train/value_loss            | 0.0467       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -63.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.66       |\n",
      "|    agent/time/fps                    | 728         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 11          |\n",
      "|    agent/time/total_timesteps        | 18432       |\n",
      "|    agent/train/approx_kl             | 0.005437659 |\n",
      "|    agent/train/clip_fraction         | 0.251       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.82       |\n",
      "|    agent/train/explained_variance    | 0.725       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0376     |\n",
      "|    agent/train/n_updates             | 80          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0107     |\n",
      "|    agent/train/std                   | 0.992       |\n",
      "|    agent/train/value_loss            | 0.0425      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -63.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.74       |\n",
      "|    agent/time/fps                    | 726         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 14          |\n",
      "|    agent/time/total_timesteps        | 20480       |\n",
      "|    agent/train/approx_kl             | 0.007320806 |\n",
      "|    agent/train/clip_fraction         | 0.281       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.79       |\n",
      "|    agent/train/explained_variance    | 0.82        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0461     |\n",
      "|    agent/train/n_updates             | 90          |\n",
      "|    agent/train/policy_gradient_loss  | -0.014      |\n",
      "|    agent/train/std                   | 0.979       |\n",
      "|    agent/train/value_loss            | 0.0346      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -63      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -6.09    |\n",
      "|    agent/time/fps                      | 827      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 7.4      |\n",
      "|    agent/time/total_timesteps          | 1.64e+04 |\n",
      "|    agent/train/approx_kl               | 0.00612  |\n",
      "|    agent/train/clip_fraction           | 0.255    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.82    |\n",
      "|    agent/train/explained_variance      | 0.73     |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0334  |\n",
      "|    agent/train/n_updates               | 80       |\n",
      "|    agent/train/policy_gradient_loss    | -0.0117  |\n",
      "|    agent/train/std                     | 0.991    |\n",
      "|    agent/train/value_loss              | 0.0424   |\n",
      "|    preferences/entropy                 | 0.145    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.626    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.132    |\n",
      "|    reward/epoch-0/train/loss           | 0.699    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.746    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.131    |\n",
      "|    reward/epoch-1/train/loss           | 0.591    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.718    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.142    |\n",
      "|    reward/epoch-2/train/loss           | 0.575    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.718    |\n",
      "|    final/train/gt_reward_loss          | 0.142    |\n",
      "|    final/train/loss                    | 0.575    |\n",
      "-----------------------------------------------------\n",
      "Collecting 204 fragments (10200 transitions)\n",
      "Sampling 510 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 285 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37c03693db5418b8dba7b9cfb5e6355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -63         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.61       |\n",
      "|    agent/time/fps                    | 1013        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 2           |\n",
      "|    agent/time/total_timesteps        | 22528       |\n",
      "|    agent/train/approx_kl             | 0.007153519 |\n",
      "|    agent/train/clip_fraction         | 0.275       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.78       |\n",
      "|    agent/train/explained_variance    | 0.809       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.00651    |\n",
      "|    agent/train/n_updates             | 100         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0124     |\n",
      "|    agent/train/std                   | 0.972       |\n",
      "|    agent/train/value_loss            | 0.0345      |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -63.7        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.98        |\n",
      "|    agent/time/fps                    | 788          |\n",
      "|    agent/time/iterations             | 2            |\n",
      "|    agent/time/time_elapsed           | 5            |\n",
      "|    agent/time/total_timesteps        | 24576        |\n",
      "|    agent/train/approx_kl             | 0.0059942417 |\n",
      "|    agent/train/clip_fraction         | 0.224        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.77        |\n",
      "|    agent/train/explained_variance    | 0.755        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0317      |\n",
      "|    agent/train/n_updates             | 110          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00807     |\n",
      "|    agent/train/std                   | 0.967        |\n",
      "|    agent/train/value_loss            | 0.0305       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -63.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.44       |\n",
      "|    agent/time/fps                    | 762         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 8           |\n",
      "|    agent/time/total_timesteps        | 26624       |\n",
      "|    agent/train/approx_kl             | 0.005566786 |\n",
      "|    agent/train/clip_fraction         | 0.223       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.77       |\n",
      "|    agent/train/explained_variance    | 0.865       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0351     |\n",
      "|    agent/train/n_updates             | 120         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00851    |\n",
      "|    agent/train/std                   | 0.964       |\n",
      "|    agent/train/value_loss            | 0.0248      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -62.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.44       |\n",
      "|    agent/time/fps                    | 722         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 11          |\n",
      "|    agent/time/total_timesteps        | 28672       |\n",
      "|    agent/train/approx_kl             | 0.007061442 |\n",
      "|    agent/train/clip_fraction         | 0.276       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.75       |\n",
      "|    agent/train/explained_variance    | 0.878       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0298     |\n",
      "|    agent/train/n_updates             | 130         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0153     |\n",
      "|    agent/train/std                   | 0.955       |\n",
      "|    agent/train/value_loss            | 0.0244      |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -62.2        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.21        |\n",
      "|    agent/time/fps                    | 716          |\n",
      "|    agent/time/iterations             | 5            |\n",
      "|    agent/time/time_elapsed           | 14           |\n",
      "|    agent/time/total_timesteps        | 30720        |\n",
      "|    agent/train/approx_kl             | 0.0071730455 |\n",
      "|    agent/train/clip_fraction         | 0.267        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.73        |\n",
      "|    agent/train/explained_variance    | 0.91         |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.034       |\n",
      "|    agent/train/n_updates             | 140          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0108      |\n",
      "|    agent/train/std                   | 0.942        |\n",
      "|    agent/train/value_loss            | 0.0142       |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -63      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -7.14    |\n",
      "|    agent/time/fps                      | 800      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 8        |\n",
      "|    agent/time/total_timesteps          | 2.66e+04 |\n",
      "|    agent/train/approx_kl               | 0.00684  |\n",
      "|    agent/train/clip_fraction           | 0.257    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.74    |\n",
      "|    agent/train/explained_variance      | 0.869    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0338  |\n",
      "|    agent/train/n_updates               | 130      |\n",
      "|    agent/train/policy_gradient_loss    | -0.0108  |\n",
      "|    agent/train/std                     | 0.951    |\n",
      "|    agent/train/value_loss              | 0.0214   |\n",
      "|    preferences/entropy                 | 0.195    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.654    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.155    |\n",
      "|    reward/epoch-0/train/loss           | 0.658    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.685    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.153    |\n",
      "|    reward/epoch-1/train/loss           | 0.576    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.737    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.154    |\n",
      "|    reward/epoch-2/train/loss           | 0.565    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.737    |\n",
      "|    final/train/gt_reward_loss          | 0.154    |\n",
      "|    final/train/loss                    | 0.565    |\n",
      "-----------------------------------------------------\n",
      "Collecting 170 fragments (8500 transitions)\n",
      "Sampling 425 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 370 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716dec617a28467fb39815a8514e8e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.9       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.05       |\n",
      "|    agent/time/fps                    | 1054        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 1           |\n",
      "|    agent/time/total_timesteps        | 32768       |\n",
      "|    agent/train/approx_kl             | 0.008388509 |\n",
      "|    agent/train/clip_fraction         | 0.292       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.69       |\n",
      "|    agent/train/explained_variance    | 0.936       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0384     |\n",
      "|    agent/train/n_updates             | 150         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0113     |\n",
      "|    agent/train/std                   | 0.925       |\n",
      "|    agent/train/value_loss            | 0.0134      |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61.5        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.17        |\n",
      "|    agent/time/fps                    | 832          |\n",
      "|    agent/time/iterations             | 2            |\n",
      "|    agent/time/time_elapsed           | 4            |\n",
      "|    agent/time/total_timesteps        | 34816        |\n",
      "|    agent/train/approx_kl             | 0.0072485814 |\n",
      "|    agent/train/clip_fraction         | 0.265        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.67        |\n",
      "|    agent/train/explained_variance    | 0.884        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0326      |\n",
      "|    agent/train/n_updates             | 160          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0108      |\n",
      "|    agent/train/std                   | 0.918        |\n",
      "|    agent/train/value_loss            | 0.0131       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.31       |\n",
      "|    agent/time/fps                    | 780         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 7           |\n",
      "|    agent/time/total_timesteps        | 36864       |\n",
      "|    agent/train/approx_kl             | 0.008838681 |\n",
      "|    agent/train/clip_fraction         | 0.285       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.66       |\n",
      "|    agent/train/explained_variance    | 0.929       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0461     |\n",
      "|    agent/train/n_updates             | 170         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0106     |\n",
      "|    agent/train/std                   | 0.918       |\n",
      "|    agent/train/value_loss            | 0.0134      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -60.3       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.29       |\n",
      "|    agent/time/fps                    | 759         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 10          |\n",
      "|    agent/time/total_timesteps        | 38912       |\n",
      "|    agent/train/approx_kl             | 0.008590658 |\n",
      "|    agent/train/clip_fraction         | 0.296       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.64       |\n",
      "|    agent/train/explained_variance    | 0.933       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0312     |\n",
      "|    agent/train/n_updates             | 180         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0128     |\n",
      "|    agent/train/std                   | 0.906       |\n",
      "|    agent/train/value_loss            | 0.0104      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -59.1       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.16       |\n",
      "|    agent/time/fps                    | 742         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 13          |\n",
      "|    agent/time/total_timesteps        | 40960       |\n",
      "|    agent/train/approx_kl             | 0.010019839 |\n",
      "|    agent/train/clip_fraction         | 0.312       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.6        |\n",
      "|    agent/train/explained_variance    | 0.961       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0494     |\n",
      "|    agent/train/n_updates             | 190         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0169     |\n",
      "|    agent/train/std                   | 0.884       |\n",
      "|    agent/train/value_loss            | 0.00878     |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -60.8    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -7.2     |\n",
      "|    agent/time/fps                      | 833      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 7        |\n",
      "|    agent/time/total_timesteps          | 3.69e+04 |\n",
      "|    agent/train/approx_kl               | 0.00891  |\n",
      "|    agent/train/clip_fraction           | 0.293    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.63    |\n",
      "|    agent/train/explained_variance      | 0.933    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0417  |\n",
      "|    agent/train/n_updates               | 180      |\n",
      "|    agent/train/policy_gradient_loss    | -0.0125  |\n",
      "|    agent/train/std                     | 0.899    |\n",
      "|    agent/train/value_loss              | 0.0107   |\n",
      "|    preferences/entropy                 | 0.138    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.668    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.189    |\n",
      "|    reward/epoch-0/train/loss           | 0.62     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.698    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.183    |\n",
      "|    reward/epoch-1/train/loss           | 0.575    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.697    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.172    |\n",
      "|    reward/epoch-2/train/loss           | 0.558    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.697    |\n",
      "|    final/train/gt_reward_loss          | 0.172    |\n",
      "|    final/train/loss                    | 0.558    |\n",
      "-----------------------------------------------------\n",
      "Collecting 146 fragments (7300 transitions)\n",
      "Sampling 365 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 445 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e667e140964ebda3271b15b0971e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -58.5       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.19       |\n",
      "|    agent/time/fps                    | 1078        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 1           |\n",
      "|    agent/time/total_timesteps        | 43008       |\n",
      "|    agent/train/approx_kl             | 0.009874223 |\n",
      "|    agent/train/clip_fraction         | 0.306       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.57       |\n",
      "|    agent/train/explained_variance    | 0.961       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0492     |\n",
      "|    agent/train/n_updates             | 200         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0114     |\n",
      "|    agent/train/std                   | 0.871       |\n",
      "|    agent/train/value_loss            | 0.00773     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -59         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.11       |\n",
      "|    agent/time/fps                    | 865         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 4           |\n",
      "|    agent/time/total_timesteps        | 45056       |\n",
      "|    agent/train/approx_kl             | 0.010813877 |\n",
      "|    agent/train/clip_fraction         | 0.345       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.52       |\n",
      "|    agent/train/explained_variance    | 0.959       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.047      |\n",
      "|    agent/train/n_updates             | 210         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0126     |\n",
      "|    agent/train/std                   | 0.851       |\n",
      "|    agent/train/value_loss            | 0.00861     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -59.3       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.03       |\n",
      "|    agent/time/fps                    | 806         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 7           |\n",
      "|    agent/time/total_timesteps        | 47104       |\n",
      "|    agent/train/approx_kl             | 0.013172309 |\n",
      "|    agent/train/clip_fraction         | 0.349       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.49       |\n",
      "|    agent/train/explained_variance    | 0.943       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0395     |\n",
      "|    agent/train/n_updates             | 220         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0124     |\n",
      "|    agent/train/std                   | 0.836       |\n",
      "|    agent/train/value_loss            | 0.0113      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -59.3       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.8        |\n",
      "|    agent/time/fps                    | 779         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 10          |\n",
      "|    agent/time/total_timesteps        | 49152       |\n",
      "|    agent/train/approx_kl             | 0.009848916 |\n",
      "|    agent/train/clip_fraction         | 0.33        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.46       |\n",
      "|    agent/train/explained_variance    | 0.957       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0304     |\n",
      "|    agent/train/n_updates             | 230         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0109     |\n",
      "|    agent/train/std                   | 0.825       |\n",
      "|    agent/train/value_loss            | 0.0128      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -58.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.72       |\n",
      "|    agent/time/fps                    | 764         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 13          |\n",
      "|    agent/time/total_timesteps        | 51200       |\n",
      "|    agent/train/approx_kl             | 0.011475313 |\n",
      "|    agent/train/clip_fraction         | 0.341       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.42       |\n",
      "|    agent/train/explained_variance    | 0.979       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0291     |\n",
      "|    agent/train/n_updates             | 240         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0145     |\n",
      "|    agent/train/std                   | 0.807       |\n",
      "|    agent/train/value_loss            | 0.00602     |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -58.9    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -6.97    |\n",
      "|    agent/time/fps                      | 858      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 7        |\n",
      "|    agent/time/total_timesteps          | 4.71e+04 |\n",
      "|    agent/train/approx_kl               | 0.0115   |\n",
      "|    agent/train/clip_fraction           | 0.343    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.45    |\n",
      "|    agent/train/explained_variance      | 0.963    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0347  |\n",
      "|    agent/train/n_updates               | 230      |\n",
      "|    agent/train/policy_gradient_loss    | -0.0119  |\n",
      "|    agent/train/std                     | 0.822    |\n",
      "|    agent/train/value_loss              | 0.00899  |\n",
      "|    preferences/entropy                 | 0.169    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.694    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.172    |\n",
      "|    reward/epoch-0/train/loss           | 0.583    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.699    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.171    |\n",
      "|    reward/epoch-1/train/loss           | 0.565    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.714    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.172    |\n",
      "|    reward/epoch-2/train/loss           | 0.554    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.714    |\n",
      "|    final/train/gt_reward_loss          | 0.172    |\n",
      "|    final/train/loss                    | 0.554    |\n",
      "-----------------------------------------------------\n",
      "Collecting 126 fragments (6300 transitions)\n",
      "Sampling 315 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 510 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d21554043584a1caecb8d9a2c68cc19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -58         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.74       |\n",
      "|    agent/time/fps                    | 1037        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 1           |\n",
      "|    agent/time/total_timesteps        | 53248       |\n",
      "|    agent/train/approx_kl             | 0.012332324 |\n",
      "|    agent/train/clip_fraction         | 0.35        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.37       |\n",
      "|    agent/train/explained_variance    | 0.974       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0273     |\n",
      "|    agent/train/n_updates             | 250         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00924    |\n",
      "|    agent/train/std                   | 0.79        |\n",
      "|    agent/train/value_loss            | 0.00625     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -56.5       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.97       |\n",
      "|    agent/time/fps                    | 798         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 5           |\n",
      "|    agent/time/total_timesteps        | 55296       |\n",
      "|    agent/train/approx_kl             | 0.010976942 |\n",
      "|    agent/train/clip_fraction         | 0.34        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.33       |\n",
      "|    agent/train/explained_variance    | 0.97        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0363     |\n",
      "|    agent/train/n_updates             | 260         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0102     |\n",
      "|    agent/train/std                   | 0.777       |\n",
      "|    agent/train/value_loss            | 0.00605     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -56.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.11       |\n",
      "|    agent/time/fps                    | 751         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 8           |\n",
      "|    agent/time/total_timesteps        | 57344       |\n",
      "|    agent/train/approx_kl             | 0.011953979 |\n",
      "|    agent/train/clip_fraction         | 0.375       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.29       |\n",
      "|    agent/train/explained_variance    | 0.961       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0595     |\n",
      "|    agent/train/n_updates             | 270         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0129     |\n",
      "|    agent/train/std                   | 0.759       |\n",
      "|    agent/train/value_loss            | 0.00632     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -55.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.03       |\n",
      "|    agent/time/fps                    | 738         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 11          |\n",
      "|    agent/time/total_timesteps        | 59392       |\n",
      "|    agent/train/approx_kl             | 0.012097489 |\n",
      "|    agent/train/clip_fraction         | 0.353       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.24       |\n",
      "|    agent/train/explained_variance    | 0.964       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.00479     |\n",
      "|    agent/train/n_updates             | 280         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0108     |\n",
      "|    agent/train/std                   | 0.736       |\n",
      "|    agent/train/value_loss            | 0.00684     |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -55.4        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.88        |\n",
      "|    agent/time/fps                    | 724          |\n",
      "|    agent/time/iterations             | 5            |\n",
      "|    agent/time/time_elapsed           | 14           |\n",
      "|    agent/time/total_timesteps        | 61440        |\n",
      "|    agent/train/approx_kl             | 0.0114262365 |\n",
      "|    agent/train/clip_fraction         | 0.364        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.17        |\n",
      "|    agent/train/explained_variance    | 0.979        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0408      |\n",
      "|    agent/train/n_updates             | 290          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0116      |\n",
      "|    agent/train/std                   | 0.716        |\n",
      "|    agent/train/value_loss            | 0.00465      |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -56.4    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -6.95    |\n",
      "|    agent/time/fps                      | 810      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 7.8      |\n",
      "|    agent/time/total_timesteps          | 5.73e+04 |\n",
      "|    agent/train/approx_kl               | 0.0122   |\n",
      "|    agent/train/clip_fraction           | 0.361    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.23    |\n",
      "|    agent/train/explained_variance      | 0.971    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0341  |\n",
      "|    agent/train/n_updates               | 280      |\n",
      "|    agent/train/policy_gradient_loss    | -0.0109  |\n",
      "|    agent/train/std                     | 0.737    |\n",
      "|    agent/train/value_loss              | 0.00565  |\n",
      "|    preferences/entropy                 | 0.212    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.708    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.18     |\n",
      "|    reward/epoch-0/train/loss           | 0.563    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.709    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.18     |\n",
      "|    reward/epoch-1/train/loss           | 0.554    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.718    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.18     |\n",
      "|    reward/epoch-2/train/loss           | 0.536    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.718    |\n",
      "|    final/train/gt_reward_loss          | 0.18     |\n",
      "|    final/train/loss                    | 0.536    |\n",
      "-----------------------------------------------------\n",
      "Query schedule: [50, 127, 102, 85, 73, 63]\n",
      "Collecting 100 fragments (5000 transitions)\n",
      "Requested 4750 transitions but only 0 in buffer. Sampling 4750 additional transitions.\n",
      "Sampling 250 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 50 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d88f4e2beb743bab5eec58d317d2c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "---------------------------------------------------\n",
      "| raw/                                 |          |\n",
      "|    agent/rollout/ep_len_mean         | 50       |\n",
      "|    agent/rollout/ep_rew_mean         | -60.2    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -28.7    |\n",
      "|    agent/time/fps                    | 999      |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 2        |\n",
      "|    agent/time/total_timesteps        | 2048     |\n",
      "---------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -60.9        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -18.2        |\n",
      "|    agent/time/fps                    | 847          |\n",
      "|    agent/time/iterations             | 2            |\n",
      "|    agent/time/time_elapsed           | 4            |\n",
      "|    agent/time/total_timesteps        | 4096         |\n",
      "|    agent/train/approx_kl             | 0.0025577005 |\n",
      "|    agent/train/clip_fraction         | 0.117        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.86        |\n",
      "|    agent/train/explained_variance    | -0.136       |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.00152     |\n",
      "|    agent/train/n_updates             | 10           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00297     |\n",
      "|    agent/train/std                   | 1.01         |\n",
      "|    agent/train/value_loss            | 0.215        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -9.85       |\n",
      "|    agent/time/fps                    | 755         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 8           |\n",
      "|    agent/time/total_timesteps        | 6144        |\n",
      "|    agent/train/approx_kl             | 0.004657798 |\n",
      "|    agent/train/clip_fraction         | 0.188       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.87       |\n",
      "|    agent/train/explained_variance    | 0.208       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.00334    |\n",
      "|    agent/train/n_updates             | 20          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00886    |\n",
      "|    agent/train/std                   | 1.02        |\n",
      "|    agent/train/value_loss            | 0.0946      |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61.7        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.8         |\n",
      "|    agent/time/fps                    | 708          |\n",
      "|    agent/time/iterations             | 4            |\n",
      "|    agent/time/time_elapsed           | 11           |\n",
      "|    agent/time/total_timesteps        | 8192         |\n",
      "|    agent/train/approx_kl             | 0.0046954947 |\n",
      "|    agent/train/clip_fraction         | 0.211        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.87        |\n",
      "|    agent/train/explained_variance    | 0.662        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0219      |\n",
      "|    agent/train/n_updates             | 30           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00868     |\n",
      "|    agent/train/std                   | 1.02         |\n",
      "|    agent/train/value_loss            | 0.0583       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -62.2       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.68       |\n",
      "|    agent/time/fps                    | 705         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 14          |\n",
      "|    agent/time/total_timesteps        | 10240       |\n",
      "|    agent/train/approx_kl             | 0.005867277 |\n",
      "|    agent/train/clip_fraction         | 0.212       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.88       |\n",
      "|    agent/train/explained_variance    | 0.626       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0363     |\n",
      "|    agent/train/n_updates             | 40          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0102     |\n",
      "|    agent/train/std                   | 1.02        |\n",
      "|    agent/train/value_loss            | 0.0508      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                   |          |\n",
      "|    agent/rollout/ep_len_mean            | 50       |\n",
      "|    agent/rollout/ep_rew_mean            | -61.3    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean    | -13.7    |\n",
      "|    agent/time/fps                       | 803      |\n",
      "|    agent/time/iterations                | 3        |\n",
      "|    agent/time/time_elapsed              | 7.8      |\n",
      "|    agent/time/total_timesteps           | 6.14e+03 |\n",
      "|    agent/train/approx_kl                | 0.00508  |\n",
      "|    agent/train/clip_fraction            | 0.201    |\n",
      "|    agent/train/clip_range               | 0.1      |\n",
      "|    agent/train/entropy_loss             | -2.87    |\n",
      "|    agent/train/explained_variance       | 0.403    |\n",
      "|    agent/train/learning_rate            | 0.002    |\n",
      "|    agent/train/loss                     | -0.0194  |\n",
      "|    agent/train/n_updates                | 30       |\n",
      "|    agent/train/policy_gradient_loss     | -0.00872 |\n",
      "|    agent/train/std                      | 1.02     |\n",
      "|    agent/train/value_loss               | 0.0911   |\n",
      "|    preferences/entropy                  | 0.163    |\n",
      "|    reward/epoch-0/train/accuracy        | 0.472    |\n",
      "|    reward/epoch-0/train/gt_reward_loss  | 0.153    |\n",
      "|    reward/epoch-0/train/loss            | 1.98     |\n",
      "|    reward/epoch-1/train/accuracy        | 0.509    |\n",
      "|    reward/epoch-1/train/gt_reward_loss  | 0.136    |\n",
      "|    reward/epoch-1/train/loss            | 1.49     |\n",
      "|    reward/epoch-10/train/accuracy       | 0.712    |\n",
      "|    reward/epoch-10/train/gt_reward_loss | 0.143    |\n",
      "|    reward/epoch-10/train/loss           | 0.638    |\n",
      "|    reward/epoch-11/train/accuracy       | 0.736    |\n",
      "|    reward/epoch-11/train/gt_reward_loss | 0.14     |\n",
      "|    reward/epoch-11/train/loss           | 0.585    |\n",
      "|    reward/epoch-2/train/accuracy        | 0.54     |\n",
      "|    reward/epoch-2/train/gt_reward_loss  | 0.131    |\n",
      "|    reward/epoch-2/train/loss            | 1.1      |\n",
      "|    reward/epoch-3/train/accuracy        | 0.559    |\n",
      "|    reward/epoch-3/train/gt_reward_loss  | 0.16     |\n",
      "|    reward/epoch-3/train/loss            | 0.895    |\n",
      "|    reward/epoch-4/train/accuracy        | 0.637    |\n",
      "|    reward/epoch-4/train/gt_reward_loss  | 0.212    |\n",
      "|    reward/epoch-4/train/loss            | 0.819    |\n",
      "|    reward/epoch-5/train/accuracy        | 0.618    |\n",
      "|    reward/epoch-5/train/gt_reward_loss  | 0.166    |\n",
      "|    reward/epoch-5/train/loss            | 0.726    |\n",
      "|    reward/epoch-6/train/accuracy        | 0.642    |\n",
      "|    reward/epoch-6/train/gt_reward_loss  | 0.173    |\n",
      "|    reward/epoch-6/train/loss            | 0.702    |\n",
      "|    reward/epoch-7/train/accuracy        | 0.622    |\n",
      "|    reward/epoch-7/train/gt_reward_loss  | 0.17     |\n",
      "|    reward/epoch-7/train/loss            | 0.734    |\n",
      "|    reward/epoch-8/train/accuracy        | 0.575    |\n",
      "|    reward/epoch-8/train/gt_reward_loss  | 0.161    |\n",
      "|    reward/epoch-8/train/loss            | 0.697    |\n",
      "|    reward/epoch-9/train/accuracy        | 0.655    |\n",
      "|    reward/epoch-9/train/gt_reward_loss  | 0.174    |\n",
      "|    reward/epoch-9/train/loss            | 0.64     |\n",
      "| reward/                                 |          |\n",
      "|    final/train/accuracy                 | 0.736    |\n",
      "|    final/train/gt_reward_loss           | 0.14     |\n",
      "|    final/train/loss                     | 0.585    |\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting 254 fragments (12700 transitions)\n",
      "Requested 12065 transitions but only 10200 in buffer. Sampling 1865 additional transitions.\n",
      "Sampling 635 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 180 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aea6159f7b746489924fdc79130a1ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -62.4        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.3         |\n",
      "|    agent/time/fps                    | 1025         |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 1            |\n",
      "|    agent/time/total_timesteps        | 12288        |\n",
      "|    agent/train/approx_kl             | 0.0076466994 |\n",
      "|    agent/train/clip_fraction         | 0.276        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.89        |\n",
      "|    agent/train/explained_variance    | 0.656        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0341      |\n",
      "|    agent/train/n_updates             | 50           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0129      |\n",
      "|    agent/train/std                   | 1.03         |\n",
      "|    agent/train/value_loss            | 0.0364       |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -63.3        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.77        |\n",
      "|    agent/time/fps                    | 854          |\n",
      "|    agent/time/iterations             | 2            |\n",
      "|    agent/time/time_elapsed           | 4            |\n",
      "|    agent/time/total_timesteps        | 14336        |\n",
      "|    agent/train/approx_kl             | 0.0063747335 |\n",
      "|    agent/train/clip_fraction         | 0.246        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.9         |\n",
      "|    agent/train/explained_variance    | 0.14         |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0434      |\n",
      "|    agent/train/n_updates             | 60           |\n",
      "|    agent/train/policy_gradient_loss  | -0.011       |\n",
      "|    agent/train/std                   | 1.03         |\n",
      "|    agent/train/value_loss            | 0.0488       |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -63.5        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.31        |\n",
      "|    agent/time/fps                    | 775          |\n",
      "|    agent/time/iterations             | 3            |\n",
      "|    agent/time/time_elapsed           | 7            |\n",
      "|    agent/time/total_timesteps        | 16384        |\n",
      "|    agent/train/approx_kl             | 0.0051834146 |\n",
      "|    agent/train/clip_fraction         | 0.24         |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.9         |\n",
      "|    agent/train/explained_variance    | 0.647        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.037       |\n",
      "|    agent/train/n_updates             | 70           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0113      |\n",
      "|    agent/train/std                   | 1.03         |\n",
      "|    agent/train/value_loss            | 0.0361       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -63.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.39       |\n",
      "|    agent/time/fps                    | 740         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 11          |\n",
      "|    agent/time/total_timesteps        | 18432       |\n",
      "|    agent/train/approx_kl             | 0.007062987 |\n",
      "|    agent/train/clip_fraction         | 0.256       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.88       |\n",
      "|    agent/train/explained_variance    | 0.751       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0376     |\n",
      "|    agent/train/n_updates             | 80          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0135     |\n",
      "|    agent/train/std                   | 1.02        |\n",
      "|    agent/train/value_loss            | 0.0306      |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -63.1        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.24        |\n",
      "|    agent/time/fps                    | 733          |\n",
      "|    agent/time/iterations             | 5            |\n",
      "|    agent/time/time_elapsed           | 13           |\n",
      "|    agent/time/total_timesteps        | 20480        |\n",
      "|    agent/train/approx_kl             | 0.0059053786 |\n",
      "|    agent/train/clip_fraction         | 0.267        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.84        |\n",
      "|    agent/train/explained_variance    | 0.788        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0479      |\n",
      "|    agent/train/n_updates             | 90           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0125      |\n",
      "|    agent/train/std                   | 1            |\n",
      "|    agent/train/value_loss            | 0.0235       |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -63.2    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -6       |\n",
      "|    agent/time/fps                      | 825      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 7.2      |\n",
      "|    agent/time/total_timesteps          | 1.64e+04 |\n",
      "|    agent/train/approx_kl               | 0.00653  |\n",
      "|    agent/train/clip_fraction           | 0.257    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.87    |\n",
      "|    agent/train/explained_variance      | 0.637    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0398  |\n",
      "|    agent/train/n_updates               | 80       |\n",
      "|    agent/train/policy_gradient_loss    | -0.0124  |\n",
      "|    agent/train/std                     | 1.01     |\n",
      "|    agent/train/value_loss              | 0.0315   |\n",
      "|    preferences/entropy                 | 0.127    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.545    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.115    |\n",
      "|    reward/epoch-0/train/loss           | 0.806    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.597    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.116    |\n",
      "|    reward/epoch-1/train/loss           | 0.681    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.664    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.118    |\n",
      "|    reward/epoch-2/train/loss           | 0.612    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.664    |\n",
      "|    final/train/gt_reward_loss          | 0.118    |\n",
      "|    final/train/loss                    | 0.612    |\n",
      "-----------------------------------------------------\n",
      "Collecting 204 fragments (10200 transitions)\n",
      "Sampling 510 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 285 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84da49d00b98438f8929ecbc8970bd73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -62.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.09       |\n",
      "|    agent/time/fps                    | 982         |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 2           |\n",
      "|    agent/time/total_timesteps        | 22528       |\n",
      "|    agent/train/approx_kl             | 0.008117765 |\n",
      "|    agent/train/clip_fraction         | 0.278       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.82       |\n",
      "|    agent/train/explained_variance    | 0.859       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0329     |\n",
      "|    agent/train/n_updates             | 100         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0138     |\n",
      "|    agent/train/std                   | 0.989       |\n",
      "|    agent/train/value_loss            | 0.0186      |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -62          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.5         |\n",
      "|    agent/time/fps                    | 786          |\n",
      "|    agent/time/iterations             | 2            |\n",
      "|    agent/time/time_elapsed           | 5            |\n",
      "|    agent/time/total_timesteps        | 24576        |\n",
      "|    agent/train/approx_kl             | 0.0069392966 |\n",
      "|    agent/train/clip_fraction         | 0.269        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.79        |\n",
      "|    agent/train/explained_variance    | 0.861        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0304      |\n",
      "|    agent/train/n_updates             | 110          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0116      |\n",
      "|    agent/train/std                   | 0.975        |\n",
      "|    agent/train/value_loss            | 0.0268       |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 50         |\n",
      "|    agent/rollout/ep_rew_mean         | -61.7      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.8       |\n",
      "|    agent/time/fps                    | 747        |\n",
      "|    agent/time/iterations             | 3          |\n",
      "|    agent/time/time_elapsed           | 8          |\n",
      "|    agent/time/total_timesteps        | 26624      |\n",
      "|    agent/train/approx_kl             | 0.00803189 |\n",
      "|    agent/train/clip_fraction         | 0.289      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -2.77      |\n",
      "|    agent/train/explained_variance    | 0.885      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | -0.0451    |\n",
      "|    agent/train/n_updates             | 120        |\n",
      "|    agent/train/policy_gradient_loss  | -0.0119    |\n",
      "|    agent/train/std                   | 0.967      |\n",
      "|    agent/train/value_loss            | 0.0229     |\n",
      "-----------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.1       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.08       |\n",
      "|    agent/time/fps                    | 735         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 11          |\n",
      "|    agent/time/total_timesteps        | 28672       |\n",
      "|    agent/train/approx_kl             | 0.007521078 |\n",
      "|    agent/train/clip_fraction         | 0.282       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.74       |\n",
      "|    agent/train/explained_variance    | 0.94        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0315     |\n",
      "|    agent/train/n_updates             | 130         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0126     |\n",
      "|    agent/train/std                   | 0.952       |\n",
      "|    agent/train/value_loss            | 0.0165      |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -60.7        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.9         |\n",
      "|    agent/time/fps                    | 718          |\n",
      "|    agent/time/iterations             | 5            |\n",
      "|    agent/time/time_elapsed           | 14           |\n",
      "|    agent/time/total_timesteps        | 30720        |\n",
      "|    agent/train/approx_kl             | 0.0076785106 |\n",
      "|    agent/train/clip_fraction         | 0.307        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.72        |\n",
      "|    agent/train/explained_variance    | 0.938        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0439      |\n",
      "|    agent/train/n_updates             | 140          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0127      |\n",
      "|    agent/train/std                   | 0.94         |\n",
      "|    agent/train/value_loss            | 0.0137       |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -61.6    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -6.68    |\n",
      "|    agent/time/fps                      | 794      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 8        |\n",
      "|    agent/time/total_timesteps          | 2.66e+04 |\n",
      "|    agent/train/approx_kl               | 0.00799  |\n",
      "|    agent/train/clip_fraction           | 0.294    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.75    |\n",
      "|    agent/train/explained_variance      | 0.916    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0384  |\n",
      "|    agent/train/n_updates               | 130      |\n",
      "|    agent/train/policy_gradient_loss    | -0.0128  |\n",
      "|    agent/train/std                     | 0.955    |\n",
      "|    agent/train/value_loss              | 0.0181   |\n",
      "|    preferences/entropy                 | 0.201    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.653    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.14     |\n",
      "|    reward/epoch-0/train/loss           | 0.659    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.677    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.14     |\n",
      "|    reward/epoch-1/train/loss           | 0.606    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.716    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.141    |\n",
      "|    reward/epoch-2/train/loss           | 0.572    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.716    |\n",
      "|    final/train/gt_reward_loss          | 0.141    |\n",
      "|    final/train/loss                    | 0.572    |\n",
      "-----------------------------------------------------\n",
      "Collecting 170 fragments (8500 transitions)\n",
      "Sampling 425 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 370 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c711602217b4ef9a71db694bc1b990f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -60.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.76       |\n",
      "|    agent/time/fps                    | 1045        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 1           |\n",
      "|    agent/time/total_timesteps        | 32768       |\n",
      "|    agent/train/approx_kl             | 0.009796307 |\n",
      "|    agent/train/clip_fraction         | 0.323       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.72       |\n",
      "|    agent/train/explained_variance    | 0.955       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0413     |\n",
      "|    agent/train/n_updates             | 150         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0153     |\n",
      "|    agent/train/std                   | 0.94        |\n",
      "|    agent/train/value_loss            | 0.0105      |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -60.3        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.95        |\n",
      "|    agent/time/fps                    | 847          |\n",
      "|    agent/time/iterations             | 2            |\n",
      "|    agent/time/time_elapsed           | 4            |\n",
      "|    agent/time/total_timesteps        | 34816        |\n",
      "|    agent/train/approx_kl             | 0.0065186555 |\n",
      "|    agent/train/clip_fraction         | 0.274        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.72        |\n",
      "|    agent/train/explained_variance    | 0.824        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0557      |\n",
      "|    agent/train/n_updates             | 160          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00763     |\n",
      "|    agent/train/std                   | 0.94         |\n",
      "|    agent/train/value_loss            | 0.0107       |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -59.3        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.01        |\n",
      "|    agent/time/fps                    | 778          |\n",
      "|    agent/time/iterations             | 3            |\n",
      "|    agent/time/time_elapsed           | 7            |\n",
      "|    agent/time/total_timesteps        | 36864        |\n",
      "|    agent/train/approx_kl             | 0.0079522915 |\n",
      "|    agent/train/clip_fraction         | 0.308        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.67        |\n",
      "|    agent/train/explained_variance    | 0.846        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0528      |\n",
      "|    agent/train/n_updates             | 170          |\n",
      "|    agent/train/policy_gradient_loss  | -0.011       |\n",
      "|    agent/train/std                   | 0.919        |\n",
      "|    agent/train/value_loss            | 0.0124       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -58.2       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.04       |\n",
      "|    agent/time/fps                    | 753         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 10          |\n",
      "|    agent/time/total_timesteps        | 38912       |\n",
      "|    agent/train/approx_kl             | 0.008880626 |\n",
      "|    agent/train/clip_fraction         | 0.31        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.65       |\n",
      "|    agent/train/explained_variance    | 0.914       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0427     |\n",
      "|    agent/train/n_updates             | 180         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0114     |\n",
      "|    agent/train/std                   | 0.905       |\n",
      "|    agent/train/value_loss            | 0.0095      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -58.1       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.94       |\n",
      "|    agent/time/fps                    | 728         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 14          |\n",
      "|    agent/time/total_timesteps        | 40960       |\n",
      "|    agent/train/approx_kl             | 0.011128351 |\n",
      "|    agent/train/clip_fraction         | 0.32        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.61       |\n",
      "|    agent/train/explained_variance    | 0.905       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0548     |\n",
      "|    agent/train/n_updates             | 190         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0117     |\n",
      "|    agent/train/std                   | 0.889       |\n",
      "|    agent/train/value_loss            | 0.00962     |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -59.3    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -6.94    |\n",
      "|    agent/time/fps                      | 830      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 7.2      |\n",
      "|    agent/time/total_timesteps          | 3.69e+04 |\n",
      "|    agent/train/approx_kl               | 0.00895  |\n",
      "|    agent/train/clip_fraction           | 0.311    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.65    |\n",
      "|    agent/train/explained_variance      | 0.879    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0461  |\n",
      "|    agent/train/n_updates               | 180      |\n",
      "|    agent/train/policy_gradient_loss    | -0.0108  |\n",
      "|    agent/train/std                     | 0.908    |\n",
      "|    agent/train/value_loss              | 0.00971  |\n",
      "|    preferences/entropy                 | 0.163    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.683    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.148    |\n",
      "|    reward/epoch-0/train/loss           | 0.599    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.715    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.146    |\n",
      "|    reward/epoch-1/train/loss           | 0.57     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.722    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.151    |\n",
      "|    reward/epoch-2/train/loss           | 0.56     |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.722    |\n",
      "|    final/train/gt_reward_loss          | 0.151    |\n",
      "|    final/train/loss                    | 0.56     |\n",
      "-----------------------------------------------------\n",
      "Collecting 146 fragments (7300 transitions)\n",
      "Sampling 365 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 445 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d209506c406946a3baf35529c99ea53a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -58.3       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.88       |\n",
      "|    agent/time/fps                    | 1024        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 1           |\n",
      "|    agent/time/total_timesteps        | 43008       |\n",
      "|    agent/train/approx_kl             | 0.010252158 |\n",
      "|    agent/train/clip_fraction         | 0.343       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.6        |\n",
      "|    agent/train/explained_variance    | 0.906       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0246     |\n",
      "|    agent/train/n_updates             | 200         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0124     |\n",
      "|    agent/train/std                   | 0.887       |\n",
      "|    agent/train/value_loss            | 0.0064      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -59         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.08       |\n",
      "|    agent/time/fps                    | 832         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 4           |\n",
      "|    agent/time/total_timesteps        | 45056       |\n",
      "|    agent/train/approx_kl             | 0.008807008 |\n",
      "|    agent/train/clip_fraction         | 0.318       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.58       |\n",
      "|    agent/train/explained_variance    | 0.853       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0482     |\n",
      "|    agent/train/n_updates             | 210         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00984    |\n",
      "|    agent/train/std                   | 0.877       |\n",
      "|    agent/train/value_loss            | 0.0114      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -58.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.35       |\n",
      "|    agent/time/fps                    | 765         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 8           |\n",
      "|    agent/time/total_timesteps        | 47104       |\n",
      "|    agent/train/approx_kl             | 0.012840383 |\n",
      "|    agent/train/clip_fraction         | 0.356       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.53       |\n",
      "|    agent/train/explained_variance    | 0.874       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0187     |\n",
      "|    agent/train/n_updates             | 220         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0139     |\n",
      "|    agent/train/std                   | 0.854       |\n",
      "|    agent/train/value_loss            | 0.011       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -57.5       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.29       |\n",
      "|    agent/time/fps                    | 738         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 11          |\n",
      "|    agent/time/total_timesteps        | 49152       |\n",
      "|    agent/train/approx_kl             | 0.012638155 |\n",
      "|    agent/train/clip_fraction         | 0.376       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.48       |\n",
      "|    agent/train/explained_variance    | 0.892       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0229     |\n",
      "|    agent/train/n_updates             | 230         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0153     |\n",
      "|    agent/train/std                   | 0.836       |\n",
      "|    agent/train/value_loss            | 0.00911     |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 50         |\n",
      "|    agent/rollout/ep_rew_mean         | -56.3      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.21      |\n",
      "|    agent/time/fps                    | 724        |\n",
      "|    agent/time/iterations             | 5          |\n",
      "|    agent/time/time_elapsed           | 14         |\n",
      "|    agent/time/total_timesteps        | 51200      |\n",
      "|    agent/train/approx_kl             | 0.01049798 |\n",
      "|    agent/train/clip_fraction         | 0.37       |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -2.42      |\n",
      "|    agent/train/explained_variance    | 0.923      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | -0.0235    |\n",
      "|    agent/train/n_updates             | 240        |\n",
      "|    agent/train/policy_gradient_loss  | -0.0133    |\n",
      "|    agent/train/std                   | 0.808      |\n",
      "|    agent/train/value_loss            | 0.00863    |\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -57.9    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -7.16    |\n",
      "|    agent/time/fps                      | 817      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 7.6      |\n",
      "|    agent/time/total_timesteps          | 4.71e+04 |\n",
      "|    agent/train/approx_kl               | 0.0113   |\n",
      "|    agent/train/clip_fraction           | 0.357    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.47    |\n",
      "|    agent/train/explained_variance      | 0.882    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0293  |\n",
      "|    agent/train/n_updates               | 230      |\n",
      "|    agent/train/policy_gradient_loss    | -0.0131  |\n",
      "|    agent/train/std                     | 0.832    |\n",
      "|    agent/train/value_loss              | 0.00995  |\n",
      "|    preferences/entropy                 | 0.17     |\n",
      "|    reward/epoch-0/train/accuracy       | 0.737    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.147    |\n",
      "|    reward/epoch-0/train/loss           | 0.556    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.737    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.148    |\n",
      "|    reward/epoch-1/train/loss           | 0.531    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.744    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.146    |\n",
      "|    reward/epoch-2/train/loss           | 0.523    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.744    |\n",
      "|    final/train/gt_reward_loss          | 0.146    |\n",
      "|    final/train/loss                    | 0.523    |\n",
      "-----------------------------------------------------\n",
      "Collecting 126 fragments (6300 transitions)\n",
      "Sampling 315 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 510 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e3d247ec524587afae79a63d155423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -55.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.16       |\n",
      "|    agent/time/fps                    | 974         |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 2           |\n",
      "|    agent/time/total_timesteps        | 53248       |\n",
      "|    agent/train/approx_kl             | 0.011565978 |\n",
      "|    agent/train/clip_fraction         | 0.365       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.37       |\n",
      "|    agent/train/explained_variance    | 0.867       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0331     |\n",
      "|    agent/train/n_updates             | 250         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0131     |\n",
      "|    agent/train/std                   | 0.785       |\n",
      "|    agent/train/value_loss            | 0.00951     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -54.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.28       |\n",
      "|    agent/time/fps                    | 816         |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 5           |\n",
      "|    agent/time/total_timesteps        | 55296       |\n",
      "|    agent/train/approx_kl             | 0.012720885 |\n",
      "|    agent/train/clip_fraction         | 0.377       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.31       |\n",
      "|    agent/train/explained_variance    | 0.957       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0376     |\n",
      "|    agent/train/n_updates             | 260         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0103     |\n",
      "|    agent/train/std                   | 0.767       |\n",
      "|    agent/train/value_loss            | 0.0065      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -53.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.1        |\n",
      "|    agent/time/fps                    | 767         |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 8           |\n",
      "|    agent/time/total_timesteps        | 57344       |\n",
      "|    agent/train/approx_kl             | 0.011234921 |\n",
      "|    agent/train/clip_fraction         | 0.364       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.27       |\n",
      "|    agent/train/explained_variance    | 0.94        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.035      |\n",
      "|    agent/train/n_updates             | 270         |\n",
      "|    agent/train/policy_gradient_loss  | -0.013      |\n",
      "|    agent/train/std                   | 0.748       |\n",
      "|    agent/train/value_loss            | 0.00771     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -52.1       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.88       |\n",
      "|    agent/time/fps                    | 732         |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 11          |\n",
      "|    agent/time/total_timesteps        | 59392       |\n",
      "|    agent/train/approx_kl             | 0.013980525 |\n",
      "|    agent/train/clip_fraction         | 0.373       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.2        |\n",
      "|    agent/train/explained_variance    | 0.943       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0447     |\n",
      "|    agent/train/n_updates             | 280         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0128     |\n",
      "|    agent/train/std                   | 0.725       |\n",
      "|    agent/train/value_loss            | 0.0088      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -51.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.69       |\n",
      "|    agent/time/fps                    | 717         |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 14          |\n",
      "|    agent/time/total_timesteps        | 61440       |\n",
      "|    agent/train/approx_kl             | 0.015870415 |\n",
      "|    agent/train/clip_fraction         | 0.38        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.15       |\n",
      "|    agent/train/explained_variance    | 0.942       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0485     |\n",
      "|    agent/train/n_updates             | 290         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0128     |\n",
      "|    agent/train/std                   | 0.703       |\n",
      "|    agent/train/value_loss            | 0.00777     |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -53.6    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -7.02    |\n",
      "|    agent/time/fps                      | 801      |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 8        |\n",
      "|    agent/time/total_timesteps          | 5.73e+04 |\n",
      "|    agent/train/approx_kl               | 0.0145   |\n",
      "|    agent/train/clip_fraction           | 0.382    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.2     |\n",
      "|    agent/train/explained_variance      | 0.946    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0382  |\n",
      "|    agent/train/n_updates               | 280      |\n",
      "|    agent/train/policy_gradient_loss    | -0.0123  |\n",
      "|    agent/train/std                     | 0.724    |\n",
      "|    agent/train/value_loss              | 0.00752  |\n",
      "|    preferences/entropy                 | 0.19     |\n",
      "|    reward/epoch-0/train/accuracy       | 0.726    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.16     |\n",
      "|    reward/epoch-0/train/loss           | 0.532    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.737    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.159    |\n",
      "|    reward/epoch-1/train/loss           | 0.521    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.733    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.16     |\n",
      "|    reward/epoch-2/train/loss           | 0.514    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.733    |\n",
      "|    final/train/gt_reward_loss          | 0.16     |\n",
      "|    final/train/loss                    | 0.514    |\n",
      "-----------------------------------------------------\n",
      "[{'reward_loss': 0.5310595929622649, 'reward_accuracy': 0.7390624992549418}, {'reward_loss': 0.5257573835551739, 'reward_accuracy': 0.7429687492549419}, {'reward_loss': 0.5290716495364904, 'reward_accuracy': 0.7490885406732558}, {'reward_loss': 0.4379456136375665, 'reward_accuracy': 0.8082031235098838}, {'reward_loss': 0.5097689516842363, 'reward_accuracy': 0.7347656264901161}, {'reward_loss': 0.49668692424893385, 'reward_accuracy': 0.7610677070915699}, {'reward_loss': 0.5595093220472337, 'reward_accuracy': 0.7180989570915698}, {'reward_loss': 0.516863377764821, 'reward_accuracy': 0.7434895820915698}, {'reward_loss': 0.5358229354023935, 'reward_accuracy': 0.717968750745058}, {'reward_loss': 0.5144221894443035, 'reward_accuracy': 0.7332031242549419}]\n"
     ]
    }
   ],
   "source": [
    "pairwise_group_comparison_result = intantiate_and_train(False)    \n",
    "\n",
    "print(pairwise_group_comparison_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Active selection fragmenter based on random fragmenter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_comparisons_3.train(\n",
    "    total_timesteps=50_000,\n",
    "    total_comparisons=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we trained the reward network using the preference comparisons algorithm, we can wrap our environment with that learned reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.rewards.reward_wrapper import RewardVecEnvWrapper\n",
    "\n",
    "learned_reward_venv = RewardVecEnvWrapper(venv, reward_net.predict_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train an agent that sees only the shaped, learned reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7a1b8839c4c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner = PPO(\n",
    "    seed=0,\n",
    "    policy=FeedForward32Policy,\n",
    "    policy_kwargs=dict(\n",
    "        features_extractor_class=NormalizeFeaturesExtractor,\n",
    "        features_extractor_kwargs=dict(normalize_class=RunningNorm),\n",
    "    ),\n",
    "    env=learned_reward_venv,\n",
    "    batch_size=64,\n",
    "    ent_coef=0.01,\n",
    "    n_epochs=10,\n",
    "    n_steps=2048 // learned_reward_venv.num_envs,\n",
    "    clip_range=0.1,\n",
    "    gae_lambda=0.95,\n",
    "    gamma=0.97,\n",
    "    learning_rate=2e-3,\n",
    ")\n",
    "learner.learn(100_000)  # Note: set to 100_000 to train a proficient expert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can evaluate it using the original reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: -11 +/- 0\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "n_eval_episodes = 100\n",
    "reward_mean, reward_std = evaluate_policy(learner.policy, venv, n_eval_episodes)\n",
    "reward_stderr = reward_std / np.sqrt(n_eval_episodes)\n",
    "print(f\"Reward: {reward_mean:.0f} +/- {reward_stderr:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('imitation_ppo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/lib/python3.8/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /imitation/docs/tutorials/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-0.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-0.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-1.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-1.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-2.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-2.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-2.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-3.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-3.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-3.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-4.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-4.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-4.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-5.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-5.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-5.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-6.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-6.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-6.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-7.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-7.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-7.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-8.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-8.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-8.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-9.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-9.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-9.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-10.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-10.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-10.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-11.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-11.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-11.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-12.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-12.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-12.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-13.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-13.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-13.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-14.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-14.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-14.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-15.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-15.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-15.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-16.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-16.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-16.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-17.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-17.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-17.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-18.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-18.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-18.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-19.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-19.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-19.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-20.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-20.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-20.mp4\n"
     ]
    }
   ],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"Reacher-v4\", render_mode='rgb_array')\n",
    "env = RecordVideo(env, './evaluation_videos', name_prefix=\"reacher\", episode_trigger=lambda x: x % 1 == 0) \n",
    "\n",
    "# Run the model in the environment\n",
    "obs, info = env.reset()\n",
    "for _ in range(1000):\n",
    "        action, _states = learner.predict(obs, deterministic=True)\n",
    "        obs, reward, _ ,done, info = env.step(action)\n",
    "        if done:\n",
    "            obs, info = env.reset()\n",
    "            \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "439158cd89905785fcc749928062ade7bfccc3f087fab145e5671f895c635937"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
