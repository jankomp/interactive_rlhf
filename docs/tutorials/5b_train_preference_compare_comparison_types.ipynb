{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[download this notebook here](https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/5_train_preference_comparisons.ipynb)\n",
    "# We want to compare pairwise comparison with groupwise comparison\n",
    "\n",
    "We will use synthetic feedback based on the true reward function of the reacher environment to evaluate and compare pairwise comparison and pairwise group comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up the preference comparisons algorithm, we first need to set up a lot of its internals beforehand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.2, Python 3.8.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from imitation.algorithms import preference_comparisons\n",
    "from imitation.rewards.reward_nets import BasicRewardNet, RewardEnsemble\n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.util.util import make_vec_env\n",
    "from imitation.policies.base import FeedForward32Policy, NormalizeFeaturesExtractor\n",
    "from imitation.regularization.regularizers import LpRegularizer\n",
    "from imitation.regularization.updaters import IntervalParamScaler\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "import numpy as np\n",
    "from imitation.util import logger\n",
    "import stable_baselines3.common.logger as sb_logger\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "def intantiate_and_train(pairwise):\n",
    "    venv = make_vec_env(\"Reacher-v4\", rng=rng, render_mode='rgb_array', n_envs=8)\n",
    "\n",
    "    reward_net_members = [BasicRewardNet(venv.observation_space, venv.action_space, normalize_input_layer=RunningNorm) for _ in range(5)]\n",
    "    reward_net = RewardEnsemble(venv.observation_space, venv.action_space, reward_net_members)\n",
    "\n",
    "    preference_model = preference_comparisons.PreferenceModel(reward_net)\n",
    "    # reward_trainer = preference_comparisons.BasicRewardTrainer(\n",
    "    #     preference_model=preference_model,\n",
    "    #     loss=preference_comparisons.CrossEntropyRewardLoss(),\n",
    "    #     epochs=3,\n",
    "    #     rng=rng,\n",
    "    # )\n",
    "\n",
    "\n",
    "    # Create a lambda updater\n",
    "    scaling_factor = 0.1\n",
    "    tolerable_interval = (0.9, 1.1) \n",
    "    lambda_updater = IntervalParamScaler(scaling_factor, tolerable_interval)\n",
    "    # Create a RegularizerFactory\n",
    "    regularizer_factory = LpRegularizer.create(initial_lambda=0.1, lambda_updater=lambda_updater, p=2, val_split=0.1)\n",
    "\n",
    "    reward_trainer = preference_comparisons.EnsembleTrainer(\n",
    "        preference_model,\n",
    "        loss=preference_comparisons.CrossEntropyRewardLoss(),\n",
    "        rng=rng,\n",
    "        epochs=5,\n",
    "        batch_size = 4,\n",
    "        minibatch_size = 2,\n",
    "        # lr: float = 1e-3,\n",
    "        # custom_logger: Optional[imit_logger.HierarchicalLogger] = None,\n",
    "        regularizer_factory = regularizer_factory,\n",
    "    )\n",
    "    if pairwise:\n",
    "        base_fragmenter = preference_comparisons.RandomFragmenter(\n",
    "            warning_threshold=0,\n",
    "            rng=rng,\n",
    "        )\n",
    "        fragmenter = preference_comparisons.ActiveSelectionFragmenter(\n",
    "                preference_model,\n",
    "                base_fragmenter,\n",
    "                2.0,\n",
    "        )\n",
    "        gatherer = preference_comparisons.SyntheticGatherer(rng=rng)\n",
    "    else:\n",
    "        fragmenter = preference_comparisons.AbsoluteUncertaintyFragmenter(\n",
    "            preference_model,\n",
    "            2.0,\n",
    "            rng=rng,\n",
    "        )\n",
    "        gatherer = preference_comparisons.SyntheticGathererForGroupComparisons(rng=rng)\n",
    "    # Several hyperparameters (reward_epochs, ppo_clip_range, ppo_ent_coef,\n",
    "    # ppo_gae_lambda, ppo_n_epochs, discount_factor, use_sde, sde_sample_freq,\n",
    "    # ppo_lr, exploration_frac, num_iterations, initial_comparison_frac,\n",
    "    # initial_epoch_multiplier, query_schedule) used in this example have been\n",
    "    # approximately fine-tuned to reach a reasonable level of performance.\n",
    "    agent = PPO(\n",
    "        policy=FeedForward32Policy,\n",
    "        policy_kwargs=dict(\n",
    "            features_extractor_class=NormalizeFeaturesExtractor,\n",
    "            features_extractor_kwargs=dict(normalize_class=RunningNorm),\n",
    "        ),\n",
    "        env=venv,\n",
    "        seed=0,\n",
    "        n_steps=2048 // venv.num_envs,\n",
    "        batch_size=64,\n",
    "        ent_coef=0.01,\n",
    "        learning_rate=2e-3,\n",
    "        clip_range=0.1,\n",
    "        gae_lambda=0.95,\n",
    "        gamma=0.97,\n",
    "        n_epochs=10,\n",
    "        tensorboard_log=\"tensorboard_logs/\",\n",
    "    )\n",
    "\n",
    "    trajectory_generator = preference_comparisons.AgentTrainer(\n",
    "        algorithm=agent,\n",
    "        reward_fn=reward_net,\n",
    "        venv=venv,\n",
    "        rng=rng,\n",
    "        exploration_frac=0.05,\n",
    "    )\n",
    "\n",
    "    default_logger = sb_logger.Logger(folder='/logs', output_formats='stdout,log,csv,tensorboard')\n",
    "    custom_logger = logger.HierarchicalLogger(default_logger=default_logger)\n",
    "\n",
    "    pref_comparisons = preference_comparisons.PreferenceComparisons(\n",
    "        trajectory_generator,\n",
    "        reward_net,\n",
    "        num_iterations=5,  # Set to 60 for better performance\n",
    "        fragmenter=fragmenter,\n",
    "        preference_gatherer=gatherer,\n",
    "        reward_trainer=reward_trainer,\n",
    "        fragment_length=50,\n",
    "        transition_oversampling=1,\n",
    "        initial_comparison_frac=0.1,\n",
    "        allow_variable_horizon=False,\n",
    "        initial_epoch_multiplier=4,\n",
    "        query_schedule=\"hyperbolic\",\n",
    "        custom_logger=custom_logger,\n",
    "    )\n",
    "\n",
    "    return pref_comparisons.train(\n",
    "        total_timesteps=100_000,\n",
    "        total_comparisons=1000,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classical pairwise comparison (baseline):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query schedule: [100, 254, 204, 170, 145, 127]\n",
      "Fewer transitions available than needed for desired number of fragment pairs. Some transitions will appear multiple times.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c14ef8f920f4e0d87842edbc670c802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949f86910e204c7e923141e02e277a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8c079554b04124b6c20071cfcc9de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0175965ff4e498ca7e5758a6ae1b18b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "452a5145d0fd4c0a97ee8f62827601d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "| raw/                                 |          |\n",
      "|    agent/rollout/ep_len_mean         | 50       |\n",
      "|    agent/rollout/ep_rew_mean         | -61      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 5.4      |\n",
      "|    agent/time/fps                    | 2891     |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 2048     |\n",
      "---------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61.4        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 1.24         |\n",
      "|    agent/time/fps                    | 1668         |\n",
      "|    agent/time/iterations             | 2            |\n",
      "|    agent/time/time_elapsed           | 2            |\n",
      "|    agent/time/total_timesteps        | 4096         |\n",
      "|    agent/train/approx_kl             | 0.0039963517 |\n",
      "|    agent/train/clip_fraction         | 0.196        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.86        |\n",
      "|    agent/train/explained_variance    | -0.0224      |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.0262       |\n",
      "|    agent/train/n_updates             | 10           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00886     |\n",
      "|    agent/train/std                   | 1            |\n",
      "|    agent/train/value_loss            | 0.194        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -2.64       |\n",
      "|    agent/time/fps                    | 1479        |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 4           |\n",
      "|    agent/time/total_timesteps        | 6144        |\n",
      "|    agent/train/approx_kl             | 0.005579002 |\n",
      "|    agent/train/clip_fraction         | 0.207       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.85       |\n",
      "|    agent/train/explained_variance    | 0.447       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0123     |\n",
      "|    agent/train/n_updates             | 20          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00952    |\n",
      "|    agent/train/std                   | 1.01        |\n",
      "|    agent/train/value_loss            | 0.108       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.87       |\n",
      "|    agent/time/fps                    | 1399        |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 5           |\n",
      "|    agent/time/total_timesteps        | 8192        |\n",
      "|    agent/train/approx_kl             | 0.004795705 |\n",
      "|    agent/train/clip_fraction         | 0.233       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.86       |\n",
      "|    agent/train/explained_variance    | 0.587       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0214     |\n",
      "|    agent/train/n_updates             | 30          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0134     |\n",
      "|    agent/train/std                   | 1.01        |\n",
      "|    agent/train/value_loss            | 0.0845      |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -62.4        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.46        |\n",
      "|    agent/time/fps                    | 1354         |\n",
      "|    agent/time/iterations             | 5            |\n",
      "|    agent/time/time_elapsed           | 7            |\n",
      "|    agent/time/total_timesteps        | 10240        |\n",
      "|    agent/train/approx_kl             | 0.0060104528 |\n",
      "|    agent/train/clip_fraction         | 0.254        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.85        |\n",
      "|    agent/train/explained_variance    | 0.507        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0126      |\n",
      "|    agent/train/n_updates             | 40           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0149      |\n",
      "|    agent/train/std                   | 1            |\n",
      "|    agent/train/value_loss            | 0.0848       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.5       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.28       |\n",
      "|    agent/time/fps                    | 1311        |\n",
      "|    agent/time/iterations             | 6           |\n",
      "|    agent/time/time_elapsed           | 9           |\n",
      "|    agent/time/total_timesteps        | 12288       |\n",
      "|    agent/train/approx_kl             | 0.007816275 |\n",
      "|    agent/train/clip_fraction         | 0.3         |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.82       |\n",
      "|    agent/train/explained_variance    | 0.546       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.00852     |\n",
      "|    agent/train/n_updates             | 50          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0137     |\n",
      "|    agent/train/std                   | 0.989       |\n",
      "|    agent/train/value_loss            | 0.0945      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -62         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -2.83       |\n",
      "|    agent/time/fps                    | 1287        |\n",
      "|    agent/time/iterations             | 7           |\n",
      "|    agent/time/time_elapsed           | 11          |\n",
      "|    agent/time/total_timesteps        | 14336       |\n",
      "|    agent/train/approx_kl             | 0.008006171 |\n",
      "|    agent/train/clip_fraction         | 0.322       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.81       |\n",
      "|    agent/train/explained_variance    | 0.544       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0313     |\n",
      "|    agent/train/n_updates             | 60          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0157     |\n",
      "|    agent/train/std                   | 0.986       |\n",
      "|    agent/train/value_loss            | 0.045       |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 50         |\n",
      "|    agent/rollout/ep_rew_mean         | -61.8      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -2.46      |\n",
      "|    agent/time/fps                    | 1275       |\n",
      "|    agent/time/iterations             | 8          |\n",
      "|    agent/time/time_elapsed           | 12         |\n",
      "|    agent/time/total_timesteps        | 16384      |\n",
      "|    agent/train/approx_kl             | 0.00850228 |\n",
      "|    agent/train/clip_fraction         | 0.325      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -2.79      |\n",
      "|    agent/train/explained_variance    | 0.582      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | -0.00754   |\n",
      "|    agent/train/n_updates             | 70         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0162    |\n",
      "|    agent/train/std                   | 0.969      |\n",
      "|    agent/train/value_loss            | 0.054      |\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.9       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -2.03       |\n",
      "|    agent/time/fps                    | 1264        |\n",
      "|    agent/time/iterations             | 9           |\n",
      "|    agent/time/time_elapsed           | 14          |\n",
      "|    agent/time/total_timesteps        | 18432       |\n",
      "|    agent/train/approx_kl             | 0.009820123 |\n",
      "|    agent/train/clip_fraction         | 0.304       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.76       |\n",
      "|    agent/train/explained_variance    | 0.539       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0202     |\n",
      "|    agent/train/n_updates             | 80          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0152     |\n",
      "|    agent/train/std                   | 0.965       |\n",
      "|    agent/train/value_loss            | 0.0437      |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -62.4        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -1.61        |\n",
      "|    agent/time/fps                    | 1257         |\n",
      "|    agent/time/iterations             | 10           |\n",
      "|    agent/time/time_elapsed           | 16           |\n",
      "|    agent/time/total_timesteps        | 20480        |\n",
      "|    agent/train/approx_kl             | 0.0076505113 |\n",
      "|    agent/train/clip_fraction         | 0.305        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.74        |\n",
      "|    agent/train/explained_variance    | 0.649        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0422      |\n",
      "|    agent/train/n_updates             | 90           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0138      |\n",
      "|    agent/train/std                   | 0.952        |\n",
      "|    agent/train/value_loss            | 0.029        |\n",
      "-------------------------------------------------------\n",
      "Fewer transitions available than needed for desired number of fragment pairs. Some transitions will appear multiple times.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfbea91bc8374985853a7c8959f8ffec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "213e3f6f32204073a35073b72dd3e224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2bd15ed4834f45a7e6f13663b1bfb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e3ad14c8a7746a78eb49acc1fab8aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5a204426bb42689b77d8f4e7281e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -62.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -1.5        |\n",
      "|    agent/time/fps                    | 3004        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 22528       |\n",
      "|    agent/train/approx_kl             | 0.010715319 |\n",
      "|    agent/train/clip_fraction         | 0.327       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.72       |\n",
      "|    agent/train/explained_variance    | 0.813       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0341     |\n",
      "|    agent/train/n_updates             | 100         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0131     |\n",
      "|    agent/train/std                   | 0.939       |\n",
      "|    agent/train/value_loss            | 0.0315      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -62.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -2.19       |\n",
      "|    agent/time/fps                    | 1669        |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 2           |\n",
      "|    agent/time/total_timesteps        | 24576       |\n",
      "|    agent/train/approx_kl             | 0.006928853 |\n",
      "|    agent/train/clip_fraction         | 0.265       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.69       |\n",
      "|    agent/train/explained_variance    | 0.582       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0264     |\n",
      "|    agent/train/n_updates             | 110         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0095     |\n",
      "|    agent/train/std                   | 0.932       |\n",
      "|    agent/train/value_loss            | 0.0285      |\n",
      "------------------------------------------------------\n",
      "----------------------------------------------------\n",
      "| raw/                                 |           |\n",
      "|    agent/rollout/ep_len_mean         | 50        |\n",
      "|    agent/rollout/ep_rew_mean         | -61.9     |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -2.84     |\n",
      "|    agent/time/fps                    | 1466      |\n",
      "|    agent/time/iterations             | 3         |\n",
      "|    agent/time/time_elapsed           | 4         |\n",
      "|    agent/time/total_timesteps        | 26624     |\n",
      "|    agent/train/approx_kl             | 0.0084577 |\n",
      "|    agent/train/clip_fraction         | 0.297     |\n",
      "|    agent/train/clip_range            | 0.1       |\n",
      "|    agent/train/entropy_loss          | -2.68     |\n",
      "|    agent/train/explained_variance    | 0.862     |\n",
      "|    agent/train/learning_rate         | 0.002     |\n",
      "|    agent/train/loss                  | -0.0274   |\n",
      "|    agent/train/n_updates             | 120       |\n",
      "|    agent/train/policy_gradient_loss  | -0.00873  |\n",
      "|    agent/train/std                   | 0.927     |\n",
      "|    agent/train/value_loss            | 0.0177    |\n",
      "----------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -2.77       |\n",
      "|    agent/time/fps                    | 1377        |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 5           |\n",
      "|    agent/time/total_timesteps        | 28672       |\n",
      "|    agent/train/approx_kl             | 0.007908668 |\n",
      "|    agent/train/clip_fraction         | 0.308       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.67       |\n",
      "|    agent/train/explained_variance    | 0.848       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0254     |\n",
      "|    agent/train/n_updates             | 130         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0118     |\n",
      "|    agent/train/std                   | 0.925       |\n",
      "|    agent/train/value_loss            | 0.0177      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -2.75       |\n",
      "|    agent/time/fps                    | 1331        |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 7           |\n",
      "|    agent/time/total_timesteps        | 30720       |\n",
      "|    agent/train/approx_kl             | 0.008089357 |\n",
      "|    agent/train/clip_fraction         | 0.288       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.67       |\n",
      "|    agent/train/explained_variance    | 0.893       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0401     |\n",
      "|    agent/train/n_updates             | 140         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0114     |\n",
      "|    agent/train/std                   | 0.92        |\n",
      "|    agent/train/value_loss            | 0.0102      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -60.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -2.62       |\n",
      "|    agent/time/fps                    | 1303        |\n",
      "|    agent/time/iterations             | 6           |\n",
      "|    agent/time/time_elapsed           | 9           |\n",
      "|    agent/time/total_timesteps        | 32768       |\n",
      "|    agent/train/approx_kl             | 0.007581448 |\n",
      "|    agent/train/clip_fraction         | 0.305       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.63       |\n",
      "|    agent/train/explained_variance    | 0.914       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0411     |\n",
      "|    agent/train/n_updates             | 150         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0116     |\n",
      "|    agent/train/std                   | 0.903       |\n",
      "|    agent/train/value_loss            | 0.00735     |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -59.8        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -2.43        |\n",
      "|    agent/time/fps                    | 1284         |\n",
      "|    agent/time/iterations             | 7            |\n",
      "|    agent/time/time_elapsed           | 11           |\n",
      "|    agent/time/total_timesteps        | 34816        |\n",
      "|    agent/train/approx_kl             | 0.0075448207 |\n",
      "|    agent/train/clip_fraction         | 0.307        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.6         |\n",
      "|    agent/train/explained_variance    | 0.956        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0394      |\n",
      "|    agent/train/n_updates             | 160          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0116      |\n",
      "|    agent/train/std                   | 0.888        |\n",
      "|    agent/train/value_loss            | 0.00368      |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -60.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -2.21       |\n",
      "|    agent/time/fps                    | 1269        |\n",
      "|    agent/time/iterations             | 8           |\n",
      "|    agent/time/time_elapsed           | 12          |\n",
      "|    agent/time/total_timesteps        | 36864       |\n",
      "|    agent/train/approx_kl             | 0.011016951 |\n",
      "|    agent/train/clip_fraction         | 0.344       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.56       |\n",
      "|    agent/train/explained_variance    | 0.932       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.044      |\n",
      "|    agent/train/n_updates             | 170         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0162     |\n",
      "|    agent/train/std                   | 0.865       |\n",
      "|    agent/train/value_loss            | 0.00516     |\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 50         |\n",
      "|    agent/rollout/ep_rew_mean         | -59.6      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -2.29      |\n",
      "|    agent/time/fps                    | 1258       |\n",
      "|    agent/time/iterations             | 9          |\n",
      "|    agent/time/time_elapsed           | 14         |\n",
      "|    agent/time/total_timesteps        | 38912      |\n",
      "|    agent/train/approx_kl             | 0.01063581 |\n",
      "|    agent/train/clip_fraction         | 0.336      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -2.52      |\n",
      "|    agent/train/explained_variance    | 0.931      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | -0.0409    |\n",
      "|    agent/train/n_updates             | 180        |\n",
      "|    agent/train/policy_gradient_loss  | -0.0115    |\n",
      "|    agent/train/std                   | 0.849      |\n",
      "|    agent/train/value_loss            | 0.0047     |\n",
      "-----------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -59.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -2.25       |\n",
      "|    agent/time/fps                    | 1249        |\n",
      "|    agent/time/iterations             | 10          |\n",
      "|    agent/time/time_elapsed           | 16          |\n",
      "|    agent/time/total_timesteps        | 40960       |\n",
      "|    agent/train/approx_kl             | 0.010764062 |\n",
      "|    agent/train/clip_fraction         | 0.338       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.47       |\n",
      "|    agent/train/explained_variance    | 0.92        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0251     |\n",
      "|    agent/train/n_updates             | 190         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0123     |\n",
      "|    agent/train/std                   | 0.828       |\n",
      "|    agent/train/value_loss            | 0.00458     |\n",
      "------------------------------------------------------\n",
      "Fewer transitions available than needed for desired number of fragment pairs. Some transitions will appear multiple times.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda274ea4d0e4d509f2547e7aadb7801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c2a45051b0429a9c4adfdc53785b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b6a7c38a884d91a6ca8dc1dd686f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4143a39ef145d182d94f802713a772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ffddb2707ab4f06b2be1f9103256152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -59.1        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -2.24        |\n",
      "|    agent/time/fps                    | 2794         |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 43008        |\n",
      "|    agent/train/approx_kl             | 0.0111877145 |\n",
      "|    agent/train/clip_fraction         | 0.342        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.43        |\n",
      "|    agent/train/explained_variance    | 0.941        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0347      |\n",
      "|    agent/train/n_updates             | 200          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00961     |\n",
      "|    agent/train/std                   | 0.814        |\n",
      "|    agent/train/value_loss            | 0.00433      |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -58.6        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -2.69        |\n",
      "|    agent/time/fps                    | 1620         |\n",
      "|    agent/time/iterations             | 2            |\n",
      "|    agent/time/time_elapsed           | 2            |\n",
      "|    agent/time/total_timesteps        | 45056        |\n",
      "|    agent/train/approx_kl             | 0.0074225003 |\n",
      "|    agent/train/clip_fraction         | 0.288        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.39        |\n",
      "|    agent/train/explained_variance    | 0.879        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0373      |\n",
      "|    agent/train/n_updates             | 210          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00683     |\n",
      "|    agent/train/std                   | 0.799        |\n",
      "|    agent/train/value_loss            | 0.00964      |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 50         |\n",
      "|    agent/rollout/ep_rew_mean         | -57.4      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.06      |\n",
      "|    agent/time/fps                    | 1422       |\n",
      "|    agent/time/iterations             | 3          |\n",
      "|    agent/time/time_elapsed           | 4          |\n",
      "|    agent/time/total_timesteps        | 47104      |\n",
      "|    agent/train/approx_kl             | 0.00901822 |\n",
      "|    agent/train/clip_fraction         | 0.355      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -2.33      |\n",
      "|    agent/train/explained_variance    | 0.936      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | -0.0455    |\n",
      "|    agent/train/n_updates             | 220        |\n",
      "|    agent/train/policy_gradient_loss  | -0.0112    |\n",
      "|    agent/train/std                   | 0.774      |\n",
      "|    agent/train/value_loss            | 0.00666    |\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 50         |\n",
      "|    agent/rollout/ep_rew_mean         | -55.9      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.19      |\n",
      "|    agent/time/fps                    | 1331       |\n",
      "|    agent/time/iterations             | 4          |\n",
      "|    agent/time/time_elapsed           | 6          |\n",
      "|    agent/time/total_timesteps        | 49152      |\n",
      "|    agent/train/approx_kl             | 0.01154626 |\n",
      "|    agent/train/clip_fraction         | 0.345      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -2.26      |\n",
      "|    agent/train/explained_variance    | 0.943      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | -0.0402    |\n",
      "|    agent/train/n_updates             | 230        |\n",
      "|    agent/train/policy_gradient_loss  | -0.0112    |\n",
      "|    agent/train/std                   | 0.754      |\n",
      "|    agent/train/value_loss            | 0.00631    |\n",
      "-----------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -54.3       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.06       |\n",
      "|    agent/time/fps                    | 1283        |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 7           |\n",
      "|    agent/time/total_timesteps        | 51200       |\n",
      "|    agent/train/approx_kl             | 0.012029929 |\n",
      "|    agent/train/clip_fraction         | 0.368       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.2        |\n",
      "|    agent/train/explained_variance    | 0.955       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0133     |\n",
      "|    agent/train/n_updates             | 240         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0117     |\n",
      "|    agent/train/std                   | 0.728       |\n",
      "|    agent/train/value_loss            | 0.00452     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -53.1       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -2.94       |\n",
      "|    agent/time/fps                    | 1258        |\n",
      "|    agent/time/iterations             | 6           |\n",
      "|    agent/time/time_elapsed           | 9           |\n",
      "|    agent/time/total_timesteps        | 53248       |\n",
      "|    agent/train/approx_kl             | 0.011923527 |\n",
      "|    agent/train/clip_fraction         | 0.366       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.13       |\n",
      "|    agent/train/explained_variance    | 0.947       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0231     |\n",
      "|    agent/train/n_updates             | 250         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0132     |\n",
      "|    agent/train/std                   | 0.708       |\n",
      "|    agent/train/value_loss            | 0.0051      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -52.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -2.75       |\n",
      "|    agent/time/fps                    | 1224        |\n",
      "|    agent/time/iterations             | 7           |\n",
      "|    agent/time/time_elapsed           | 11          |\n",
      "|    agent/time/total_timesteps        | 55296       |\n",
      "|    agent/train/approx_kl             | 0.012918416 |\n",
      "|    agent/train/clip_fraction         | 0.379       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.07       |\n",
      "|    agent/train/explained_variance    | 0.95        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0342     |\n",
      "|    agent/train/n_updates             | 260         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0131     |\n",
      "|    agent/train/std                   | 0.685       |\n",
      "|    agent/train/value_loss            | 0.00529     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -51.3       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -2.7        |\n",
      "|    agent/time/fps                    | 1198        |\n",
      "|    agent/time/iterations             | 8           |\n",
      "|    agent/time/time_elapsed           | 13          |\n",
      "|    agent/time/total_timesteps        | 57344       |\n",
      "|    agent/train/approx_kl             | 0.014166615 |\n",
      "|    agent/train/clip_fraction         | 0.402       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.98       |\n",
      "|    agent/train/explained_variance    | 0.97        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0352     |\n",
      "|    agent/train/n_updates             | 270         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0153     |\n",
      "|    agent/train/std                   | 0.659       |\n",
      "|    agent/train/value_loss            | 0.00261     |\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -49.1       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -2.72       |\n",
      "|    agent/time/fps                    | 1180        |\n",
      "|    agent/time/iterations             | 9           |\n",
      "|    agent/time/time_elapsed           | 15          |\n",
      "|    agent/time/total_timesteps        | 59392       |\n",
      "|    agent/train/approx_kl             | 0.012834331 |\n",
      "|    agent/train/clip_fraction         | 0.388       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.9        |\n",
      "|    agent/train/explained_variance    | 0.965       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0206     |\n",
      "|    agent/train/n_updates             | 280         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0109     |\n",
      "|    agent/train/std                   | 0.632       |\n",
      "|    agent/train/value_loss            | 0.00403     |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -47.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -2.65       |\n",
      "|    agent/time/fps                    | 1165        |\n",
      "|    agent/time/iterations             | 10          |\n",
      "|    agent/time/time_elapsed           | 17          |\n",
      "|    agent/time/total_timesteps        | 61440       |\n",
      "|    agent/train/approx_kl             | 0.010491494 |\n",
      "|    agent/train/clip_fraction         | 0.362       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.82       |\n",
      "|    agent/train/explained_variance    | 0.963       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0384     |\n",
      "|    agent/train/n_updates             | 290         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00885    |\n",
      "|    agent/train/std                   | 0.613       |\n",
      "|    agent/train/value_loss            | 0.00381     |\n",
      "------------------------------------------------------\n",
      "Fewer transitions available than needed for desired number of fragment pairs. Some transitions will appear multiple times.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5e61ff60d7463481d21063330b4a82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2100198a7b834867ab6dae2a5a959966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd601453607b46c3879bfe8e37d42771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a991094533a44a2afb34dce29baa81e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a25b9500b342d3a76b168a979aed01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 50         |\n",
      "|    agent/rollout/ep_rew_mean         | -45.9      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -2.57      |\n",
      "|    agent/time/fps                    | 2812       |\n",
      "|    agent/time/iterations             | 1          |\n",
      "|    agent/time/time_elapsed           | 0          |\n",
      "|    agent/time/total_timesteps        | 63488      |\n",
      "|    agent/train/approx_kl             | 0.01291349 |\n",
      "|    agent/train/clip_fraction         | 0.394      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -1.75      |\n",
      "|    agent/train/explained_variance    | 0.97       |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | -0.0393    |\n",
      "|    agent/train/n_updates             | 300        |\n",
      "|    agent/train/policy_gradient_loss  | -0.0117    |\n",
      "|    agent/train/std                   | 0.592      |\n",
      "|    agent/train/value_loss            | 0.00352    |\n",
      "-----------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -45.1       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -4.48       |\n",
      "|    agent/time/fps                    | 1571        |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 2           |\n",
      "|    agent/time/total_timesteps        | 65536       |\n",
      "|    agent/train/approx_kl             | 0.010625571 |\n",
      "|    agent/train/clip_fraction         | 0.363       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.68       |\n",
      "|    agent/train/explained_variance    | 0.452       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.000581   |\n",
      "|    agent/train/n_updates             | 310         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0091     |\n",
      "|    agent/train/std                   | 0.574       |\n",
      "|    agent/train/value_loss            | 0.0902      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -43.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.27       |\n",
      "|    agent/time/fps                    | 1387        |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 4           |\n",
      "|    agent/time/total_timesteps        | 67584       |\n",
      "|    agent/train/approx_kl             | 0.017201338 |\n",
      "|    agent/train/clip_fraction         | 0.401       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.6        |\n",
      "|    agent/train/explained_variance    | 0.704       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0132     |\n",
      "|    agent/train/n_updates             | 320         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0121     |\n",
      "|    agent/train/std                   | 0.558       |\n",
      "|    agent/train/value_loss            | 0.0551      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -42.5       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.84       |\n",
      "|    agent/time/fps                    | 1290        |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 6           |\n",
      "|    agent/time/total_timesteps        | 69632       |\n",
      "|    agent/train/approx_kl             | 0.018382646 |\n",
      "|    agent/train/clip_fraction         | 0.442       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.52       |\n",
      "|    agent/train/explained_variance    | 0.79        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0338     |\n",
      "|    agent/train/n_updates             | 330         |\n",
      "|    agent/train/policy_gradient_loss  | -0.014      |\n",
      "|    agent/train/std                   | 0.537       |\n",
      "|    agent/train/value_loss            | 0.0483      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -40.3       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.59       |\n",
      "|    agent/time/fps                    | 1247        |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 8           |\n",
      "|    agent/time/total_timesteps        | 71680       |\n",
      "|    agent/train/approx_kl             | 0.014947064 |\n",
      "|    agent/train/clip_fraction         | 0.429       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.44       |\n",
      "|    agent/train/explained_variance    | 0.803       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0147     |\n",
      "|    agent/train/n_updates             | 340         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0127     |\n",
      "|    agent/train/std                   | 0.517       |\n",
      "|    agent/train/value_loss            | 0.0367      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -38.1       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.14       |\n",
      "|    agent/time/fps                    | 1220        |\n",
      "|    agent/time/iterations             | 6           |\n",
      "|    agent/time/time_elapsed           | 10          |\n",
      "|    agent/time/total_timesteps        | 73728       |\n",
      "|    agent/train/approx_kl             | 0.014603978 |\n",
      "|    agent/train/clip_fraction         | 0.446       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.34       |\n",
      "|    agent/train/explained_variance    | 0.815       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.02       |\n",
      "|    agent/train/n_updates             | 350         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0148     |\n",
      "|    agent/train/std                   | 0.496       |\n",
      "|    agent/train/value_loss            | 0.0303      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -36.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.65       |\n",
      "|    agent/time/fps                    | 1198        |\n",
      "|    agent/time/iterations             | 7           |\n",
      "|    agent/time/time_elapsed           | 11          |\n",
      "|    agent/time/total_timesteps        | 75776       |\n",
      "|    agent/train/approx_kl             | 0.013779503 |\n",
      "|    agent/train/clip_fraction         | 0.445       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.25       |\n",
      "|    agent/train/explained_variance    | 0.87        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.00959    |\n",
      "|    agent/train/n_updates             | 360         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0137     |\n",
      "|    agent/train/std                   | 0.474       |\n",
      "|    agent/train/value_loss            | 0.0245      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -35.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.33       |\n",
      "|    agent/time/fps                    | 1184        |\n",
      "|    agent/time/iterations             | 8           |\n",
      "|    agent/time/time_elapsed           | 13          |\n",
      "|    agent/time/total_timesteps        | 77824       |\n",
      "|    agent/train/approx_kl             | 0.020590058 |\n",
      "|    agent/train/clip_fraction         | 0.448       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.15       |\n",
      "|    agent/train/explained_variance    | 0.837       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.00361     |\n",
      "|    agent/train/n_updates             | 370         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0139     |\n",
      "|    agent/train/std                   | 0.453       |\n",
      "|    agent/train/value_loss            | 0.0224      |\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -34.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.08       |\n",
      "|    agent/time/fps                    | 1175        |\n",
      "|    agent/time/iterations             | 9           |\n",
      "|    agent/time/time_elapsed           | 15          |\n",
      "|    agent/time/total_timesteps        | 79872       |\n",
      "|    agent/train/approx_kl             | 0.017065307 |\n",
      "|    agent/train/clip_fraction         | 0.47        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.06       |\n",
      "|    agent/train/explained_variance    | 0.871       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0382     |\n",
      "|    agent/train/n_updates             | 380         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0149     |\n",
      "|    agent/train/std                   | 0.434       |\n",
      "|    agent/train/value_loss            | 0.0257      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -32         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -4.76       |\n",
      "|    agent/time/fps                    | 1165        |\n",
      "|    agent/time/iterations             | 10          |\n",
      "|    agent/time/time_elapsed           | 17          |\n",
      "|    agent/time/total_timesteps        | 81920       |\n",
      "|    agent/train/approx_kl             | 0.015470797 |\n",
      "|    agent/train/clip_fraction         | 0.447       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.97       |\n",
      "|    agent/train/explained_variance    | 0.88        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.00912    |\n",
      "|    agent/train/n_updates             | 390         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0123     |\n",
      "|    agent/train/std                   | 0.416       |\n",
      "|    agent/train/value_loss            | 0.0192      |\n",
      "------------------------------------------------------\n",
      "Fewer transitions available than needed for desired number of fragment pairs. Some transitions will appear multiple times.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0616a01d6e34dec994d94c422a8d809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a25903ead9f48799c073c3215df8fa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38effd1e1c9f4b269931c8e14e3b6491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e872ed167d4763b52bea0c587b4213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c0c15048dfb43f2b5c96faf0eb34383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -30.1       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -4.71       |\n",
      "|    agent/time/fps                    | 2890        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 83968       |\n",
      "|    agent/train/approx_kl             | 0.013990346 |\n",
      "|    agent/train/clip_fraction         | 0.452       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.881      |\n",
      "|    agent/train/explained_variance    | 0.875       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.00546     |\n",
      "|    agent/train/n_updates             | 400         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0144     |\n",
      "|    agent/train/std                   | 0.397       |\n",
      "|    agent/train/value_loss            | 0.0217      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -28.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -8.28       |\n",
      "|    agent/time/fps                    | 1551        |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 2           |\n",
      "|    agent/time/total_timesteps        | 86016       |\n",
      "|    agent/train/approx_kl             | 0.013497773 |\n",
      "|    agent/train/clip_fraction         | 0.392       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.83       |\n",
      "|    agent/train/explained_variance    | 0.378       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.11        |\n",
      "|    agent/train/n_updates             | 410         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0061     |\n",
      "|    agent/train/std                   | 0.384       |\n",
      "|    agent/train/value_loss            | 0.381       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -27.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -11.7       |\n",
      "|    agent/time/fps                    | 1358        |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 4           |\n",
      "|    agent/time/total_timesteps        | 88064       |\n",
      "|    agent/train/approx_kl             | 0.014204319 |\n",
      "|    agent/train/clip_fraction         | 0.42        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.746      |\n",
      "|    agent/train/explained_variance    | 0.664       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.0922      |\n",
      "|    agent/train/n_updates             | 420         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00867    |\n",
      "|    agent/train/std                   | 0.367       |\n",
      "|    agent/train/value_loss            | 0.206       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -26.3       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -12.4       |\n",
      "|    agent/time/fps                    | 1283        |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 6           |\n",
      "|    agent/time/total_timesteps        | 90112       |\n",
      "|    agent/train/approx_kl             | 0.017411489 |\n",
      "|    agent/train/clip_fraction         | 0.491       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.647      |\n",
      "|    agent/train/explained_variance    | 0.768       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.0536      |\n",
      "|    agent/train/n_updates             | 430         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0163     |\n",
      "|    agent/train/std                   | 0.349       |\n",
      "|    agent/train/value_loss            | 0.179       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -24.2       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -11.6       |\n",
      "|    agent/time/fps                    | 1240        |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 8           |\n",
      "|    agent/time/total_timesteps        | 92160       |\n",
      "|    agent/train/approx_kl             | 0.016744047 |\n",
      "|    agent/train/clip_fraction         | 0.484       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.548      |\n",
      "|    agent/train/explained_variance    | 0.768       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.016       |\n",
      "|    agent/train/n_updates             | 440         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0128     |\n",
      "|    agent/train/std                   | 0.329       |\n",
      "|    agent/train/value_loss            | 0.132       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -22.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -10.5       |\n",
      "|    agent/time/fps                    | 1210        |\n",
      "|    agent/time/iterations             | 6           |\n",
      "|    agent/time/time_elapsed           | 10          |\n",
      "|    agent/time/total_timesteps        | 94208       |\n",
      "|    agent/train/approx_kl             | 0.020342618 |\n",
      "|    agent/train/clip_fraction         | 0.477       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.454      |\n",
      "|    agent/train/explained_variance    | 0.84        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.0272      |\n",
      "|    agent/train/n_updates             | 450         |\n",
      "|    agent/train/policy_gradient_loss  | -0.01       |\n",
      "|    agent/train/std                   | 0.312       |\n",
      "|    agent/train/value_loss            | 0.0941      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 50         |\n",
      "|    agent/rollout/ep_rew_mean         | -21.1      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -9.75      |\n",
      "|    agent/time/fps                    | 1190       |\n",
      "|    agent/time/iterations             | 7          |\n",
      "|    agent/time/time_elapsed           | 12         |\n",
      "|    agent/time/total_timesteps        | 96256      |\n",
      "|    agent/train/approx_kl             | 0.01926938 |\n",
      "|    agent/train/clip_fraction         | 0.494      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -0.348     |\n",
      "|    agent/train/explained_variance    | 0.88       |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | 0.0301     |\n",
      "|    agent/train/n_updates             | 460        |\n",
      "|    agent/train/policy_gradient_loss  | -0.0143    |\n",
      "|    agent/train/std                   | 0.296      |\n",
      "|    agent/train/value_loss            | 0.1        |\n",
      "-----------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -20.1       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -9.03       |\n",
      "|    agent/time/fps                    | 1177        |\n",
      "|    agent/time/iterations             | 8           |\n",
      "|    agent/time/time_elapsed           | 13          |\n",
      "|    agent/time/total_timesteps        | 98304       |\n",
      "|    agent/train/approx_kl             | 0.017148254 |\n",
      "|    agent/train/clip_fraction         | 0.482       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.251      |\n",
      "|    agent/train/explained_variance    | 0.895       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.00612     |\n",
      "|    agent/train/n_updates             | 470         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0127     |\n",
      "|    agent/train/std                   | 0.282       |\n",
      "|    agent/train/value_loss            | 0.0766      |\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -18.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -8.49       |\n",
      "|    agent/time/fps                    | 1167        |\n",
      "|    agent/time/iterations             | 9           |\n",
      "|    agent/time/time_elapsed           | 15          |\n",
      "|    agent/time/total_timesteps        | 100352      |\n",
      "|    agent/train/approx_kl             | 0.017419517 |\n",
      "|    agent/train/clip_fraction         | 0.483       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.166      |\n",
      "|    agent/train/explained_variance    | 0.808       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.00576    |\n",
      "|    agent/train/n_updates             | 480         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00842    |\n",
      "|    agent/train/std                   | 0.267       |\n",
      "|    agent/train/value_loss            | 0.0854      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -17.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.43       |\n",
      "|    agent/time/fps                    | 1159        |\n",
      "|    agent/time/iterations             | 10          |\n",
      "|    agent/time/time_elapsed           | 17          |\n",
      "|    agent/time/total_timesteps        | 102400      |\n",
      "|    agent/train/approx_kl             | 0.018386193 |\n",
      "|    agent/train/clip_fraction         | 0.494       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.0626     |\n",
      "|    agent/train/explained_variance    | 0.877       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.0327      |\n",
      "|    agent/train/n_updates             | 490         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00824    |\n",
      "|    agent/train/std                   | 0.254       |\n",
      "|    agent/train/value_loss            | 0.0801      |\n",
      "------------------------------------------------------\n",
      "Fewer transitions available than needed for desired number of fragment pairs. Some transitions will appear multiple times.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b59fbfcc174c0b8405191363887f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b6b1d36765b481399b78f3be93cf9d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08507a379f924a26909d9df850124d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833690393fcb4aa3b049484fcb383e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d251af12b2a44189733094a4e96a436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -16.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.93       |\n",
      "|    agent/time/fps                    | 2818        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 104448      |\n",
      "|    agent/train/approx_kl             | 0.017879821 |\n",
      "|    agent/train/clip_fraction         | 0.51        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | 0.0479      |\n",
      "|    agent/train/explained_variance    | 0.885       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0095     |\n",
      "|    agent/train/n_updates             | 500         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00993    |\n",
      "|    agent/train/std                   | 0.241       |\n",
      "|    agent/train/value_loss            | 0.0567      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -15.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.85       |\n",
      "|    agent/time/fps                    | 1552        |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 2           |\n",
      "|    agent/time/total_timesteps        | 106496      |\n",
      "|    agent/train/approx_kl             | 0.018745864 |\n",
      "|    agent/train/clip_fraction         | 0.488       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | 0.142       |\n",
      "|    agent/train/explained_variance    | 0.73        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.0202      |\n",
      "|    agent/train/n_updates             | 510         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00906    |\n",
      "|    agent/train/std                   | 0.228       |\n",
      "|    agent/train/value_loss            | 0.123       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -14.9       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -8          |\n",
      "|    agent/time/fps                    | 1359        |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 4           |\n",
      "|    agent/time/total_timesteps        | 108544      |\n",
      "|    agent/train/approx_kl             | 0.022309266 |\n",
      "|    agent/train/clip_fraction         | 0.509       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | 0.248       |\n",
      "|    agent/train/explained_variance    | 0.852       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.0573      |\n",
      "|    agent/train/n_updates             | 520         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00527    |\n",
      "|    agent/train/std                   | 0.217       |\n",
      "|    agent/train/value_loss            | 0.0943      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 50         |\n",
      "|    agent/rollout/ep_rew_mean         | -13.5      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -6.32      |\n",
      "|    agent/time/fps                    | 1268       |\n",
      "|    agent/time/iterations             | 4          |\n",
      "|    agent/time/time_elapsed           | 6          |\n",
      "|    agent/time/total_timesteps        | 110592     |\n",
      "|    agent/train/approx_kl             | 0.01908689 |\n",
      "|    agent/train/clip_fraction         | 0.504      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | 0.335      |\n",
      "|    agent/train/explained_variance    | 0.819      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | 0.0155     |\n",
      "|    agent/train/n_updates             | 530        |\n",
      "|    agent/train/policy_gradient_loss  | -0.00326   |\n",
      "|    agent/train/std                   | 0.208      |\n",
      "|    agent/train/value_loss            | 0.0926     |\n",
      "-----------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -12.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -5.19       |\n",
      "|    agent/time/fps                    | 1226        |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 8           |\n",
      "|    agent/time/total_timesteps        | 112640      |\n",
      "|    agent/train/approx_kl             | 0.017555058 |\n",
      "|    agent/train/clip_fraction         | 0.478       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | 0.421       |\n",
      "|    agent/train/explained_variance    | 0.802       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.00806     |\n",
      "|    agent/train/n_updates             | 540         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00662    |\n",
      "|    agent/train/std                   | 0.197       |\n",
      "|    agent/train/value_loss            | 0.076       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -12.2       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -4.3        |\n",
      "|    agent/time/fps                    | 1200        |\n",
      "|    agent/time/iterations             | 6           |\n",
      "|    agent/time/time_elapsed           | 10          |\n",
      "|    agent/time/total_timesteps        | 114688      |\n",
      "|    agent/train/approx_kl             | 0.016437305 |\n",
      "|    agent/train/clip_fraction         | 0.479       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | 0.501       |\n",
      "|    agent/train/explained_variance    | 0.858       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.0225      |\n",
      "|    agent/train/n_updates             | 550         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00149    |\n",
      "|    agent/train/std                   | 0.19        |\n",
      "|    agent/train/value_loss            | 0.0619      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -11.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.99       |\n",
      "|    agent/time/fps                    | 1186        |\n",
      "|    agent/time/iterations             | 7           |\n",
      "|    agent/time/time_elapsed           | 12          |\n",
      "|    agent/time/total_timesteps        | 116736      |\n",
      "|    agent/train/approx_kl             | 0.018941905 |\n",
      "|    agent/train/clip_fraction         | 0.495       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | 0.589       |\n",
      "|    agent/train/explained_variance    | 0.898       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.0275      |\n",
      "|    agent/train/n_updates             | 560         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00429    |\n",
      "|    agent/train/std                   | 0.181       |\n",
      "|    agent/train/value_loss            | 0.0544      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -11.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.77       |\n",
      "|    agent/time/fps                    | 1172        |\n",
      "|    agent/time/iterations             | 8           |\n",
      "|    agent/time/time_elapsed           | 13          |\n",
      "|    agent/time/total_timesteps        | 118784      |\n",
      "|    agent/train/approx_kl             | 0.023590194 |\n",
      "|    agent/train/clip_fraction         | 0.518       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | 0.681       |\n",
      "|    agent/train/explained_variance    | 0.892       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.0318      |\n",
      "|    agent/train/n_updates             | 570         |\n",
      "|    agent/train/policy_gradient_loss  | 0.00351     |\n",
      "|    agent/train/std                   | 0.173       |\n",
      "|    agent/train/value_loss            | 0.0622      |\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 50         |\n",
      "|    agent/rollout/ep_rew_mean         | -10.7      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -3.39      |\n",
      "|    agent/time/fps                    | 1161       |\n",
      "|    agent/time/iterations             | 9          |\n",
      "|    agent/time/time_elapsed           | 15         |\n",
      "|    agent/time/total_timesteps        | 120832     |\n",
      "|    agent/train/approx_kl             | 0.01875526 |\n",
      "|    agent/train/clip_fraction         | 0.492      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | 0.764      |\n",
      "|    agent/train/explained_variance    | 0.939      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | 0.00136    |\n",
      "|    agent/train/n_updates             | 580        |\n",
      "|    agent/train/policy_gradient_loss  | -0.000496  |\n",
      "|    agent/train/std                   | 0.165      |\n",
      "|    agent/train/value_loss            | 0.0442     |\n",
      "-----------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -10.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -2.82       |\n",
      "|    agent/time/fps                    | 1152        |\n",
      "|    agent/time/iterations             | 10          |\n",
      "|    agent/time/time_elapsed           | 17          |\n",
      "|    agent/time/total_timesteps        | 122880      |\n",
      "|    agent/train/approx_kl             | 0.024630746 |\n",
      "|    agent/train/clip_fraction         | 0.515       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | 0.846       |\n",
      "|    agent/train/explained_variance    | 0.947       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.0373      |\n",
      "|    agent/train/n_updates             | 590         |\n",
      "|    agent/train/policy_gradient_loss  | 0.00758     |\n",
      "|    agent/train/std                   | 0.159       |\n",
      "|    agent/train/value_loss            | 0.0405      |\n",
      "------------------------------------------------------\n",
      "{'reward_loss': 0.18626000849712682, 'reward_accuracy': 0.9277777777777777}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pairwise_comparison_result = intantiate_and_train(True)    \n",
    "\n",
    "print(pairwise_comparison_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pairwise Group comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query schedule: [100, 254, 204, 170, 145, 127]\n",
      "DBSCAN found 30 clusters. 30 groups were created.\n",
      "Before cleaning: Group 1 has 3 fragments, Group 2 has 3 fragments\n",
      "Group 1 has an average reward of -67.74492009480794, Group 2 has an average reward of -64.79341125488281\n",
      "Groups reward border is -66.26916567484537\n",
      "Group 1 has 2 fragments, Group 2 has 2 fragments\n",
      "Before cleaning: Group 1 has 5 fragments, Group 2 has 3 fragments\n",
      "Group 1 has an average reward of -61.35270538330078, Group 2 has an average reward of -62.25755182902018\n",
      "Groups reward border is -61.80512860616048\n",
      "Group 1 has 4 fragments, Group 2 has 2 fragments\n",
      "Before cleaning: Group 1 has 4 fragments, Group 2 has 7 fragments\n",
      "Group 1 has an average reward of -61.536930084228516, Group 2 has an average reward of -62.538499559674946\n",
      "Groups reward border is -62.03771482195173\n",
      "Group 1 has 3 fragments, Group 2 has 3 fragments\n",
      "Before cleaning: Group 1 has 5 fragments, Group 2 has 5 fragments\n",
      "Group 1 has an average reward of -62.154240417480466, Group 2 has an average reward of -60.05222015380859\n",
      "Groups reward border is -61.10323028564453\n",
      "Group 1 has 3 fragments, Group 2 has 3 fragments\n",
      "Before cleaning: Group 1 has 3 fragments, Group 2 has 3 fragments\n",
      "Group 1 has an average reward of -58.93485641479492, Group 2 has an average reward of -64.79341125488281\n",
      "Groups reward border is -61.86413383483887\n",
      "Group 1 has 3 fragments, Group 2 has 3 fragments\n",
      "Before cleaning: Group 1 has 4 fragments, Group 2 has 4 fragments\n",
      "Group 1 has an average reward of -60.89355754852295, Group 2 has an average reward of -60.76179790496826\n",
      "Groups reward border is -60.827677726745605\n",
      "Group 1 has 1 fragments, Group 2 has 3 fragments\n",
      "Before cleaning: Group 1 has 6 fragments, Group 2 has 5 fragments\n",
      "Group 1 has an average reward of -57.61456425984701, Group 2 has an average reward of -59.481407928466794\n",
      "Groups reward border is -58.5479860941569\n",
      "Group 1 has 3 fragments, Group 2 has 3 fragments\n",
      "Before cleaning: Group 1 has 3 fragments, Group 2 has 5 fragments\n",
      "Group 1 has an average reward of -58.93485641479492, Group 2 has an average reward of -63.550927734375\n",
      "Groups reward border is -61.24289207458496\n",
      "Group 1 has 3 fragments, Group 2 has 4 fragments\n",
      "Before cleaning: Group 1 has 5 fragments, Group 2 has 3 fragments\n",
      "Group 1 has an average reward of -60.981973266601564, Group 2 has an average reward of -62.25755182902018\n",
      "Groups reward border is -61.61976254781087\n",
      "Group 1 has 1 fragments, Group 2 has 2 fragments\n",
      "Before cleaning: Group 1 has 4 fragments, Group 2 has 10 fragments\n",
      "Group 1 has an average reward of -60.89355754852295, Group 2 has an average reward of -63.65387802124023\n",
      "Groups reward border is -62.27371778488159\n",
      "Group 1 has 3 fragments, Group 2 has 8 fragments\n",
      "Before cleaning: Group 1 has 5 fragments, Group 2 has 3 fragments\n",
      "Group 1 has an average reward of -60.05222015380859, Group 2 has an average reward of -63.434060414632164\n",
      "Groups reward border is -61.743140284220374\n",
      "Group 1 has 3 fragments, Group 2 has 2 fragments\n",
      "Before cleaning: Group 1 has 5 fragments, Group 2 has 3 fragments\n",
      "Group 1 has an average reward of -59.481407928466794, Group 2 has an average reward of -63.434060414632164\n",
      "Groups reward border is -61.457734171549475\n",
      "Group 1 has 3 fragments, Group 2 has 2 fragments\n",
      "Before cleaning: Group 1 has 5 fragments, Group 2 has 5 fragments\n",
      "Group 1 has an average reward of -61.35270538330078, Group 2 has an average reward of -61.31829147338867\n",
      "Groups reward border is -61.33549842834472\n",
      "Group 1 has 2 fragments, Group 2 has 3 fragments\n",
      "Before cleaning: Group 1 has 3 fragments, Group 2 has 4 fragments\n",
      "Group 1 has an average reward of -62.20096969604492, Group 2 has an average reward of -60.76179790496826\n",
      "Groups reward border is -61.48138380050659\n",
      "Group 1 has 2 fragments, Group 2 has 3 fragments\n",
      "Before cleaning: Group 1 has 5 fragments, Group 2 has 4 fragments\n",
      "Group 1 has an average reward of -62.154240417480466, Group 2 has an average reward of -60.89355754852295\n",
      "Groups reward border is -61.52389898300171\n",
      "Group 1 has 2 fragments, Group 2 has 3 fragments\n",
      "Before cleaning: Group 1 has 8 fragments, Group 2 has 5 fragments\n",
      "Group 1 has an average reward of -61.83048915863037, Group 2 has an average reward of -59.33667984008789\n",
      "Groups reward border is -60.583584499359134\n",
      "Group 1 has 3 fragments, Group 2 has 3 fragments\n",
      "Before cleaning: Group 1 has 4 fragments, Group 2 has 3 fragments\n",
      "Group 1 has an average reward of -60.76179790496826, Group 2 has an average reward of -62.20096969604492\n",
      "Groups reward border is -61.48138380050659\n",
      "Group 1 has 3 fragments, Group 2 has 2 fragments\n",
      "Before cleaning: Group 1 has 3 fragments, Group 2 has 3 fragments\n",
      "Group 1 has an average reward of -62.25755182902018, Group 2 has an average reward of -63.434060414632164\n",
      "Groups reward border is -62.84580612182617\n",
      "Group 1 has 1 fragments, Group 2 has 2 fragments\n",
      "Before cleaning: Group 1 has 5 fragments, Group 2 has 5 fragments\n",
      "Group 1 has an average reward of -59.481407928466794, Group 2 has an average reward of -61.31829147338867\n",
      "Groups reward border is -60.39984970092773\n",
      "Group 1 has 2 fragments, Group 2 has 3 fragments\n",
      "The system did 19 group comparisons.\n",
      "This amounts to a total of 103 trajectories in comparisons\n",
      "Generated 1900 preferences\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c3b7d3f74b4188b6e2f58414bdd26a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5bd7b2024ab46d09cd2bc0c2ff8664c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b5cd15658148f3934ef616e51bfe0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101924e5bb09411f80eef8a1dd0153ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e001c7e4688c42a3a62ef77a05b71ab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "| raw/                                 |          |\n",
      "|    agent/rollout/ep_len_mean         | 50       |\n",
      "|    agent/rollout/ep_rew_mean         | -61.1    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.69    |\n",
      "|    agent/time/fps                    | 2726     |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 2048     |\n",
      "---------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61.2        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -11.6        |\n",
      "|    agent/time/fps                    | 1784         |\n",
      "|    agent/time/iterations             | 2            |\n",
      "|    agent/time/time_elapsed           | 2            |\n",
      "|    agent/time/total_timesteps        | 4096         |\n",
      "|    agent/train/approx_kl             | 0.0029042535 |\n",
      "|    agent/train/clip_fraction         | 0.13         |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.84        |\n",
      "|    agent/train/explained_variance    | 0.000133     |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.692        |\n",
      "|    agent/train/n_updates             | 10           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00325     |\n",
      "|    agent/train/std                   | 0.999        |\n",
      "|    agent/train/value_loss            | 2.63         |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -14.8       |\n",
      "|    agent/time/fps                    | 1580        |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 3           |\n",
      "|    agent/time/total_timesteps        | 6144        |\n",
      "|    agent/train/approx_kl             | 0.004159358 |\n",
      "|    agent/train/clip_fraction         | 0.172       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.83       |\n",
      "|    agent/train/explained_variance    | 0.492       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.192       |\n",
      "|    agent/train/n_updates             | 20          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00689    |\n",
      "|    agent/train/std                   | 0.997       |\n",
      "|    agent/train/value_loss            | 0.897       |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -15.3        |\n",
      "|    agent/time/fps                    | 1491         |\n",
      "|    agent/time/iterations             | 4            |\n",
      "|    agent/time/time_elapsed           | 5            |\n",
      "|    agent/time/total_timesteps        | 8192         |\n",
      "|    agent/train/approx_kl             | 0.0051905145 |\n",
      "|    agent/train/clip_fraction         | 0.224        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.84        |\n",
      "|    agent/train/explained_variance    | 0.49         |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.194        |\n",
      "|    agent/train/n_updates             | 30           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0104      |\n",
      "|    agent/train/std                   | 1            |\n",
      "|    agent/train/value_loss            | 0.708        |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61.3        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -14.7        |\n",
      "|    agent/time/fps                    | 1418         |\n",
      "|    agent/time/iterations             | 5            |\n",
      "|    agent/time/time_elapsed           | 7            |\n",
      "|    agent/time/total_timesteps        | 10240        |\n",
      "|    agent/train/approx_kl             | 0.0054488713 |\n",
      "|    agent/train/clip_fraction         | 0.247        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.84        |\n",
      "|    agent/train/explained_variance    | 0.523        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.181        |\n",
      "|    agent/train/n_updates             | 40           |\n",
      "|    agent/train/policy_gradient_loss  | -0.012       |\n",
      "|    agent/train/std                   | 0.997        |\n",
      "|    agent/train/value_loss            | 0.565        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -60.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -14.4       |\n",
      "|    agent/time/fps                    | 1367        |\n",
      "|    agent/time/iterations             | 6           |\n",
      "|    agent/time/time_elapsed           | 8           |\n",
      "|    agent/time/total_timesteps        | 12288       |\n",
      "|    agent/train/approx_kl             | 0.007848547 |\n",
      "|    agent/train/clip_fraction         | 0.275       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.82       |\n",
      "|    agent/train/explained_variance    | 0.687       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.123       |\n",
      "|    agent/train/n_updates             | 50          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0124     |\n",
      "|    agent/train/std                   | 0.99        |\n",
      "|    agent/train/value_loss            | 0.483       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -13.9       |\n",
      "|    agent/time/fps                    | 1331        |\n",
      "|    agent/time/iterations             | 7           |\n",
      "|    agent/time/time_elapsed           | 10          |\n",
      "|    agent/time/total_timesteps        | 14336       |\n",
      "|    agent/train/approx_kl             | 0.009278469 |\n",
      "|    agent/train/clip_fraction         | 0.29        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.83       |\n",
      "|    agent/train/explained_variance    | 0.671       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.045       |\n",
      "|    agent/train/n_updates             | 60          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0138     |\n",
      "|    agent/train/std                   | 0.998       |\n",
      "|    agent/train/value_loss            | 0.289       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.5       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -13.3       |\n",
      "|    agent/time/fps                    | 1287        |\n",
      "|    agent/time/iterations             | 8           |\n",
      "|    agent/time/time_elapsed           | 12          |\n",
      "|    agent/time/total_timesteps        | 16384       |\n",
      "|    agent/train/approx_kl             | 0.007304611 |\n",
      "|    agent/train/clip_fraction         | 0.285       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.83       |\n",
      "|    agent/train/explained_variance    | 0.662       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.0345      |\n",
      "|    agent/train/n_updates             | 70          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0111     |\n",
      "|    agent/train/std                   | 0.998       |\n",
      "|    agent/train/value_loss            | 0.25        |\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61.6        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -12.8        |\n",
      "|    agent/time/fps                    | 1264         |\n",
      "|    agent/time/iterations             | 9            |\n",
      "|    agent/time/time_elapsed           | 14           |\n",
      "|    agent/time/total_timesteps        | 18432        |\n",
      "|    agent/train/approx_kl             | 0.0099897375 |\n",
      "|    agent/train/clip_fraction         | 0.286        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.84        |\n",
      "|    agent/train/explained_variance    | 0.758        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.00869      |\n",
      "|    agent/train/n_updates             | 80           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0124      |\n",
      "|    agent/train/std                   | 1.01         |\n",
      "|    agent/train/value_loss            | 0.194        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -62.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -12.6       |\n",
      "|    agent/time/fps                    | 1254        |\n",
      "|    agent/time/iterations             | 10          |\n",
      "|    agent/time/time_elapsed           | 16          |\n",
      "|    agent/time/total_timesteps        | 20480       |\n",
      "|    agent/train/approx_kl             | 0.009029688 |\n",
      "|    agent/train/clip_fraction         | 0.284       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.85       |\n",
      "|    agent/train/explained_variance    | 0.773       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.0242      |\n",
      "|    agent/train/n_updates             | 90          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0123     |\n",
      "|    agent/train/std                   | 1           |\n",
      "|    agent/train/value_loss            | 0.188       |\n",
      "------------------------------------------------------\n",
      "DBSCAN found 47 clusters. 47 groups were created.\n",
      "Before cleaning: Group 1 has 4 fragments, Group 2 has 7 fragments\n",
      "Group 1 has an average reward of -58.144161224365234, Group 2 has an average reward of -62.57195227486746\n",
      "Groups reward border is -60.35805674961635\n",
      "Group 1 has 3 fragments, Group 2 has 6 fragments\n",
      "Before cleaning: Group 1 has 5 fragments, Group 2 has 9 fragments\n",
      "Group 1 has an average reward of -61.39701156616211, Group 2 has an average reward of -61.37612448798286\n",
      "Groups reward border is -61.386568027072485\n",
      "Group 1 has 3 fragments, Group 2 has 5 fragments\n",
      "Before cleaning: Group 1 has 13 fragments, Group 2 has 9 fragments\n",
      "Group 1 has an average reward of -60.30224286592924, Group 2 has an average reward of -62.43291261461046\n",
      "Groups reward border is -61.36757774026985\n",
      "Group 1 has 7 fragments, Group 2 has 5 fragments\n",
      "Before cleaning: Group 1 has 5 fragments, Group 2 has 9 fragments\n",
      "Group 1 has an average reward of -58.02933578491211, Group 2 has an average reward of -63.57056596544054\n",
      "Groups reward border is -60.79995087517632\n",
      "Group 1 has 3 fragments, Group 2 has 7 fragments\n",
      "Before cleaning: Group 1 has 9 fragments, Group 2 has 18 fragments\n",
      "Group 1 has an average reward of -62.43291261461046, Group 2 has an average reward of -61.19578446282281\n",
      "Groups reward border is -61.81434853871663\n",
      "Group 1 has 5 fragments, Group 2 has 9 fragments\n",
      "Before cleaning: Group 1 has 21 fragments, Group 2 has 18 fragments\n",
      "Group 1 has an average reward of -61.29563522338867, Group 2 has an average reward of -61.19578446282281\n",
      "Groups reward border is -61.24570984310574\n",
      "Group 1 has 13 fragments, Group 2 has 9 fragments\n",
      "Before cleaning: Group 1 has 30 fragments, Group 2 has 10 fragments\n",
      "Group 1 has an average reward of -60.30288632710775, Group 2 has an average reward of -61.89519004821777\n",
      "Groups reward border is -61.09903818766276\n",
      "Group 1 has 17 fragments, Group 2 has 6 fragments\n",
      "Before cleaning: Group 1 has 5 fragments, Group 2 has 9 fragments\n",
      "Group 1 has an average reward of -60.409593963623045, Group 2 has an average reward of -63.57056596544054\n",
      "Groups reward border is -61.99007996453179\n",
      "Group 1 has 3 fragments, Group 2 has 7 fragments\n",
      "Before cleaning: Group 1 has 7 fragments, Group 2 has 4 fragments\n",
      "Group 1 has an average reward of -62.57249995640346, Group 2 has an average reward of -66.0355453491211\n",
      "Groups reward border is -64.30402265276228\n",
      "Group 1 has 4 fragments, Group 2 has 4 fragments\n",
      "Before cleaning: Group 1 has 30 fragments, Group 2 has 9 fragments\n",
      "Group 1 has an average reward of -63.510386021931964, Group 2 has an average reward of -66.19647894965277\n",
      "Groups reward border is -64.85343248579237\n",
      "Group 1 has 16 fragments, Group 2 has 5 fragments\n",
      "Before cleaning: Group 1 has 11 fragments, Group 2 has 4 fragments\n",
      "Group 1 has an average reward of -59.58826307816939, Group 2 has an average reward of -68.74852752685547\n",
      "Groups reward border is -64.16839530251244\n",
      "Group 1 has 10 fragments, Group 2 has 4 fragments\n",
      "Before cleaning: Group 1 has 5 fragments, Group 2 has 4 fragments\n",
      "Group 1 has an average reward of -59.97292785644531, Group 2 has an average reward of -64.30132293701172\n",
      "Groups reward border is -62.137125396728514\n",
      "Group 1 has 3 fragments, Group 2 has 3 fragments\n",
      "Before cleaning: Group 1 has 9 fragments, Group 2 has 3 fragments\n",
      "Group 1 has an average reward of -59.06367831759982, Group 2 has an average reward of -63.73432286580404\n",
      "Groups reward border is -61.399000591701935\n",
      "Group 1 has 6 fragments, Group 2 has 2 fragments\n",
      "Before cleaning: Group 1 has 5 fragments, Group 2 has 3 fragments\n",
      "Group 1 has an average reward of -61.39701156616211, Group 2 has an average reward of -59.798301696777344\n",
      "Groups reward border is -60.597656631469725\n",
      "Group 1 has 4 fragments, Group 2 has 2 fragments\n",
      "Before cleaning: Group 1 has 5 fragments, Group 2 has 18 fragments\n",
      "Group 1 has an average reward of -64.17356414794922, Group 2 has an average reward of -61.19578446282281\n",
      "Groups reward border is -62.68467430538601\n",
      "Group 1 has 3 fragments, Group 2 has 11 fragments\n",
      "Before cleaning: Group 1 has 6 fragments, Group 2 has 10 fragments\n",
      "Group 1 has an average reward of -64.51775042215984, Group 2 has an average reward of -61.89519004821777\n",
      "Groups reward border is -63.2064702351888\n",
      "Group 1 has 3 fragments, Group 2 has 7 fragments\n",
      "Before cleaning: Group 1 has 6 fragments, Group 2 has 9 fragments\n",
      "Group 1 has an average reward of -62.26276715596517, Group 2 has an average reward of -59.06367831759982\n",
      "Groups reward border is -60.6632227367825\n",
      "Group 1 has 4 fragments, Group 2 has 6 fragments\n",
      "Before cleaning: Group 1 has 13 fragments, Group 2 has 5 fragments\n",
      "Group 1 has an average reward of -60.30224286592924, Group 2 has an average reward of -64.17356414794922\n",
      "Groups reward border is -62.23790350693923\n",
      "Group 1 has 9 fragments, Group 2 has 3 fragments\n",
      "Before cleaning: Group 1 has 30 fragments, Group 2 has 14 fragments\n",
      "Group 1 has an average reward of -60.30288632710775, Group 2 has an average reward of -60.44030353001186\n",
      "Groups reward border is -60.3715949285598\n",
      "Group 1 has 16 fragments, Group 2 has 6 fragments\n",
      "Before cleaning: Group 1 has 42 fragments, Group 2 has 21 fragments\n",
      "Group 1 has an average reward of -60.44693529038202, Group 2 has an average reward of -61.29563522338867\n",
      "Groups reward border is -60.87128525688534\n",
      "Group 1 has 23 fragments, Group 2 has 14 fragments\n",
      "The system did 20 group comparisons.\n",
      "This amounts to a total of 276 trajectories in comparisons\n",
      "Generated 2275 preferences\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e5fca7fc33b4d17a6f0fad69ac1459b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41500ed8d4514f2a83c5f4e9f746a770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58bd26c7200a464eb382f76d7a314585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f09be86b5b84fc2b2b02bed4957de57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c72cd996ecb4a75b89241d1c357d9e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 50         |\n",
      "|    agent/rollout/ep_rew_mean         | -63        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -12        |\n",
      "|    agent/time/fps                    | 2969       |\n",
      "|    agent/time/iterations             | 1          |\n",
      "|    agent/time/time_elapsed           | 0          |\n",
      "|    agent/time/total_timesteps        | 22528      |\n",
      "|    agent/train/approx_kl             | 0.00866675 |\n",
      "|    agent/train/clip_fraction         | 0.333      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -2.85      |\n",
      "|    agent/train/explained_variance    | 0.804      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | 0.00194    |\n",
      "|    agent/train/n_updates             | 100        |\n",
      "|    agent/train/policy_gradient_loss  | -0.0167    |\n",
      "|    agent/train/std                   | 1          |\n",
      "|    agent/train/value_loss            | 0.133      |\n",
      "-----------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -63.5        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -19.5        |\n",
      "|    agent/time/fps                    | 1882         |\n",
      "|    agent/time/iterations             | 2            |\n",
      "|    agent/time/time_elapsed           | 2            |\n",
      "|    agent/time/total_timesteps        | 24576        |\n",
      "|    agent/train/approx_kl             | 0.0036136897 |\n",
      "|    agent/train/clip_fraction         | 0.167        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.85        |\n",
      "|    agent/train/explained_variance    | 0.0138       |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.887        |\n",
      "|    agent/train/n_updates             | 110          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00785     |\n",
      "|    agent/train/std                   | 1.01         |\n",
      "|    agent/train/value_loss            | 2.8          |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -63.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -26.7       |\n",
      "|    agent/time/fps                    | 1688        |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 3           |\n",
      "|    agent/time/total_timesteps        | 26624       |\n",
      "|    agent/train/approx_kl             | 0.004678938 |\n",
      "|    agent/train/clip_fraction         | 0.182       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.83       |\n",
      "|    agent/train/explained_variance    | 0.486       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.443       |\n",
      "|    agent/train/n_updates             | 120         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00975    |\n",
      "|    agent/train/std                   | 0.999       |\n",
      "|    agent/train/value_loss            | 1.52        |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 50         |\n",
      "|    agent/rollout/ep_rew_mean         | -63.7      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -29        |\n",
      "|    agent/time/fps                    | 1603       |\n",
      "|    agent/time/iterations             | 4          |\n",
      "|    agent/time/time_elapsed           | 5          |\n",
      "|    agent/time/total_timesteps        | 28672      |\n",
      "|    agent/train/approx_kl             | 0.00758113 |\n",
      "|    agent/train/clip_fraction         | 0.259      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -2.83      |\n",
      "|    agent/train/explained_variance    | 0.627      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | 0.243      |\n",
      "|    agent/train/n_updates             | 130        |\n",
      "|    agent/train/policy_gradient_loss  | -0.0128    |\n",
      "|    agent/train/std                   | 0.993      |\n",
      "|    agent/train/value_loss            | 1.06       |\n",
      "-----------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -62.9       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -28.8       |\n",
      "|    agent/time/fps                    | 1558        |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 6           |\n",
      "|    agent/time/total_timesteps        | 30720       |\n",
      "|    agent/train/approx_kl             | 0.009270422 |\n",
      "|    agent/train/clip_fraction         | 0.28        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.81       |\n",
      "|    agent/train/explained_variance    | 0.715       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.621       |\n",
      "|    agent/train/n_updates             | 140         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0141     |\n",
      "|    agent/train/std                   | 0.985       |\n",
      "|    agent/train/value_loss            | 1.13        |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -28         |\n",
      "|    agent/time/fps                    | 1513        |\n",
      "|    agent/time/iterations             | 6           |\n",
      "|    agent/time/time_elapsed           | 8           |\n",
      "|    agent/time/total_timesteps        | 32768       |\n",
      "|    agent/train/approx_kl             | 0.008386297 |\n",
      "|    agent/train/clip_fraction         | 0.303       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.77       |\n",
      "|    agent/train/explained_variance    | 0.686       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.622       |\n",
      "|    agent/train/n_updates             | 150         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0119     |\n",
      "|    agent/train/std                   | 0.965       |\n",
      "|    agent/train/value_loss            | 0.949       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -60.3       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -26.8       |\n",
      "|    agent/time/fps                    | 1469        |\n",
      "|    agent/time/iterations             | 7           |\n",
      "|    agent/time/time_elapsed           | 9           |\n",
      "|    agent/time/total_timesteps        | 34816       |\n",
      "|    agent/train/approx_kl             | 0.009039214 |\n",
      "|    agent/train/clip_fraction         | 0.319       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.74       |\n",
      "|    agent/train/explained_variance    | 0.823       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.15        |\n",
      "|    agent/train/n_updates             | 160         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0133     |\n",
      "|    agent/train/std                   | 0.947       |\n",
      "|    agent/train/value_loss            | 0.692       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -60.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -25         |\n",
      "|    agent/time/fps                    | 1438        |\n",
      "|    agent/time/iterations             | 8           |\n",
      "|    agent/time/time_elapsed           | 11          |\n",
      "|    agent/time/total_timesteps        | 36864       |\n",
      "|    agent/train/approx_kl             | 0.009100733 |\n",
      "|    agent/train/clip_fraction         | 0.31        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.71       |\n",
      "|    agent/train/explained_variance    | 0.714       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.316       |\n",
      "|    agent/train/n_updates             | 170         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0148     |\n",
      "|    agent/train/std                   | 0.94        |\n",
      "|    agent/train/value_loss            | 0.902       |\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -59.9       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -24.6       |\n",
      "|    agent/time/fps                    | 1410        |\n",
      "|    agent/time/iterations             | 9           |\n",
      "|    agent/time/time_elapsed           | 13          |\n",
      "|    agent/time/total_timesteps        | 38912       |\n",
      "|    agent/train/approx_kl             | 0.013437507 |\n",
      "|    agent/train/clip_fraction         | 0.377       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.66       |\n",
      "|    agent/train/explained_variance    | 0.76        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.167       |\n",
      "|    agent/train/n_updates             | 180         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0163     |\n",
      "|    agent/train/std                   | 0.916       |\n",
      "|    agent/train/value_loss            | 0.705       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -59.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -23.9       |\n",
      "|    agent/time/fps                    | 1391        |\n",
      "|    agent/time/iterations             | 10          |\n",
      "|    agent/time/time_elapsed           | 14          |\n",
      "|    agent/time/total_timesteps        | 40960       |\n",
      "|    agent/train/approx_kl             | 0.016127743 |\n",
      "|    agent/train/clip_fraction         | 0.383       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.61       |\n",
      "|    agent/train/explained_variance    | 0.885       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.144       |\n",
      "|    agent/train/n_updates             | 190         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0152     |\n",
      "|    agent/train/std                   | 0.897       |\n",
      "|    agent/train/value_loss            | 0.409       |\n",
      "------------------------------------------------------\n",
      "DBSCAN found 34 clusters. 34 groups were created.\n",
      "Before cleaning: Group 1 has 4 fragments, Group 2 has 4 fragments\n",
      "Group 1 has an average reward of -61.1455135345459, Group 2 has an average reward of -63.63259792327881\n",
      "Groups reward border is -62.38905572891235\n",
      "Group 1 has 3 fragments, Group 2 has 3 fragments\n",
      "Before cleaning: Group 1 has 7 fragments, Group 2 has 4 fragments\n",
      "Group 1 has an average reward of -60.02379281180246, Group 2 has an average reward of -61.1455135345459\n",
      "Groups reward border is -60.584653173174175\n",
      "Group 1 has 3 fragments, Group 2 has 3 fragments\n",
      "Before cleaning: Group 1 has 9 fragments, Group 2 has 13 fragments\n",
      "Group 1 has an average reward of -62.16958575778537, Group 2 has an average reward of -58.80068911038912\n",
      "Groups reward border is -60.48513743408725\n",
      "Group 1 has 6 fragments, Group 2 has 10 fragments\n",
      "Before cleaning: Group 1 has 39 fragments, Group 2 has 4 fragments\n",
      "Group 1 has an average reward of -58.95023394853641, Group 2 has an average reward of -60.71520709991455\n",
      "Groups reward border is -59.83272052422548\n",
      "Group 1 has 21 fragments, Group 2 has 2 fragments\n",
      "Before cleaning: Group 1 has 4 fragments, Group 2 has 8 fragments\n",
      "Group 1 has an average reward of -60.71520709991455, Group 2 has an average reward of -61.062049865722656\n",
      "Groups reward border is -60.8886284828186\n",
      "Group 1 has 2 fragments, Group 2 has 3 fragments\n",
      "Before cleaning: Group 1 has 8 fragments, Group 2 has 7 fragments\n",
      "Group 1 has an average reward of -59.86550521850586, Group 2 has an average reward of -60.02379281180246\n",
      "Groups reward border is -59.944649015154155\n",
      "Group 1 has 3 fragments, Group 2 has 4 fragments\n",
      "Before cleaning: Group 1 has 10 fragments, Group 2 has 8 fragments\n",
      "Group 1 has an average reward of -65.81664848327637, Group 2 has an average reward of -59.86550521850586\n",
      "Groups reward border is -62.84107685089111\n",
      "Group 1 has 7 fragments, Group 2 has 6 fragments\n",
      "Before cleaning: Group 1 has 7 fragments, Group 2 has 39 fragments\n",
      "Group 1 has an average reward of -60.02379281180246, Group 2 has an average reward of -58.95023394853641\n",
      "Groups reward border is -59.48701338016943\n",
      "Group 1 has 4 fragments, Group 2 has 19 fragments\n",
      "Before cleaning: Group 1 has 4 fragments, Group 2 has 6 fragments\n",
      "Group 1 has an average reward of -63.67438888549805, Group 2 has an average reward of -62.683572133382164\n",
      "Groups reward border is -63.17898050944011\n",
      "Group 1 has 2 fragments, Group 2 has 3 fragments\n",
      "Before cleaning: Group 1 has 4 fragments, Group 2 has 7 fragments\n",
      "Group 1 has an average reward of -60.71520709991455, Group 2 has an average reward of -60.02379281180246\n",
      "Groups reward border is -60.3694999558585\n",
      "Group 1 has 2 fragments, Group 2 has 3 fragments\n",
      "Before cleaning: Group 1 has 4 fragments, Group 2 has 10 fragments\n",
      "Group 1 has an average reward of -63.67438888549805, Group 2 has an average reward of -60.975089645385744\n",
      "Groups reward border is -62.32473926544189\n",
      "Group 1 has 2 fragments, Group 2 has 5 fragments\n",
      "Before cleaning: Group 1 has 3 fragments, Group 2 has 4 fragments\n",
      "Group 1 has an average reward of -58.52546183268229, Group 2 has an average reward of -54.33981704711914\n",
      "Groups reward border is -56.43263943990071\n",
      "Group 1 has 2 fragments, Group 2 has 4 fragments\n",
      "Before cleaning: Group 1 has 8 fragments, Group 2 has 3 fragments\n",
      "Group 1 has an average reward of -61.062049865722656, Group 2 has an average reward of -61.248497009277344\n",
      "Groups reward border is -61.1552734375\n",
      "Group 1 has 5 fragments, Group 2 has 2 fragments\n",
      "Before cleaning: Group 1 has 10 fragments, Group 2 has 13 fragments\n",
      "Group 1 has an average reward of -65.81664848327637, Group 2 has an average reward of -58.80068911038912\n",
      "Groups reward border is -62.30866879683275\n",
      "Group 1 has 7 fragments, Group 2 has 11 fragments\n",
      "Before cleaning: Group 1 has 3 fragments, Group 2 has 4 fragments\n",
      "Group 1 has an average reward of -61.248497009277344, Group 2 has an average reward of -63.67438888549805\n",
      "Groups reward border is -62.461442947387695\n",
      "Group 1 has 3 fragments, Group 2 has 2 fragments\n",
      "Before cleaning: Group 1 has 3 fragments, Group 2 has 10 fragments\n",
      "Group 1 has an average reward of -61.248497009277344, Group 2 has an average reward of -61.03561019897461\n",
      "Groups reward border is -61.14205360412598\n",
      "Group 1 has 2 fragments, Group 2 has 6 fragments\n",
      "Before cleaning: Group 1 has 45 fragments, Group 2 has 10 fragments\n",
      "Group 1 has an average reward of -64.15547171698677, Group 2 has an average reward of -61.566358184814455\n",
      "Groups reward border is -62.86091495090061\n",
      "Group 1 has 31 fragments, Group 2 has 7 fragments\n",
      "Before cleaning: Group 1 has 7 fragments, Group 2 has 10 fragments\n",
      "Group 1 has an average reward of -63.71577835083008, Group 2 has an average reward of -65.81664848327637\n",
      "Groups reward border is -64.76621341705322\n",
      "Group 1 has 5 fragments, Group 2 has 6 fragments\n",
      "The system did 18 group comparisons.\n",
      "This amounts to a total of 209 trajectories in comparisons\n",
      "Generated 1946 preferences\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8ff8a9fc054fc18afa530a9888e1e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a0cba4910146ddae4f71b1b8dc1c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc421f7726324fd4a22d25947bfbf9cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1bac86094542dca4a01936111bc6b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96bd8d102e454269a68ea84607d8a770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -58.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -22.9       |\n",
      "|    agent/time/fps                    | 3090        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 43008       |\n",
      "|    agent/train/approx_kl             | 0.011339644 |\n",
      "|    agent/train/clip_fraction         | 0.364       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.58       |\n",
      "|    agent/train/explained_variance    | 0.887       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.0511      |\n",
      "|    agent/train/n_updates             | 200         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0131     |\n",
      "|    agent/train/std                   | 0.886       |\n",
      "|    agent/train/value_loss            | 0.39        |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -57.3       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -27         |\n",
      "|    agent/time/fps                    | 1910        |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 2           |\n",
      "|    agent/time/total_timesteps        | 45056       |\n",
      "|    agent/train/approx_kl             | 0.011230161 |\n",
      "|    agent/train/clip_fraction         | 0.33        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.53       |\n",
      "|    agent/train/explained_variance    | 0.808       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.291       |\n",
      "|    agent/train/n_updates             | 210         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0112     |\n",
      "|    agent/train/std                   | 0.865       |\n",
      "|    agent/train/value_loss            | 0.854       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -56.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -31.7       |\n",
      "|    agent/time/fps                    | 1706        |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 3           |\n",
      "|    agent/time/total_timesteps        | 47104       |\n",
      "|    agent/train/approx_kl             | 0.011320914 |\n",
      "|    agent/train/clip_fraction         | 0.37        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.46       |\n",
      "|    agent/train/explained_variance    | 0.814       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.168       |\n",
      "|    agent/train/n_updates             | 220         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0117     |\n",
      "|    agent/train/std                   | 0.835       |\n",
      "|    agent/train/value_loss            | 0.631       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -54.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -33.6       |\n",
      "|    agent/time/fps                    | 1619        |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 5           |\n",
      "|    agent/time/total_timesteps        | 49152       |\n",
      "|    agent/train/approx_kl             | 0.013414853 |\n",
      "|    agent/train/clip_fraction         | 0.373       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.39       |\n",
      "|    agent/train/explained_variance    | 0.869       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.225       |\n",
      "|    agent/train/n_updates             | 230         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0132     |\n",
      "|    agent/train/std                   | 0.802       |\n",
      "|    agent/train/value_loss            | 0.576       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -52.3       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -32.3       |\n",
      "|    agent/time/fps                    | 1572        |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 6           |\n",
      "|    agent/time/total_timesteps        | 51200       |\n",
      "|    agent/train/approx_kl             | 0.013046596 |\n",
      "|    agent/train/clip_fraction         | 0.407       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.29       |\n",
      "|    agent/train/explained_variance    | 0.913       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.0998      |\n",
      "|    agent/train/n_updates             | 240         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0138     |\n",
      "|    agent/train/std                   | 0.771       |\n",
      "|    agent/train/value_loss            | 0.391       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -50.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -30.8       |\n",
      "|    agent/time/fps                    | 1515        |\n",
      "|    agent/time/iterations             | 6           |\n",
      "|    agent/time/time_elapsed           | 8           |\n",
      "|    agent/time/total_timesteps        | 53248       |\n",
      "|    agent/train/approx_kl             | 0.013761989 |\n",
      "|    agent/train/clip_fraction         | 0.404       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.21       |\n",
      "|    agent/train/explained_variance    | 0.878       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.0766      |\n",
      "|    agent/train/n_updates             | 250         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0145     |\n",
      "|    agent/train/std                   | 0.738       |\n",
      "|    agent/train/value_loss            | 0.393       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -49.9       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -29.7       |\n",
      "|    agent/time/fps                    | 1473        |\n",
      "|    agent/time/iterations             | 7           |\n",
      "|    agent/time/time_elapsed           | 9           |\n",
      "|    agent/time/total_timesteps        | 55296       |\n",
      "|    agent/train/approx_kl             | 0.014725201 |\n",
      "|    agent/train/clip_fraction         | 0.416       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.12       |\n",
      "|    agent/train/explained_variance    | 0.94        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.0413      |\n",
      "|    agent/train/n_updates             | 260         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0151     |\n",
      "|    agent/train/std                   | 0.709       |\n",
      "|    agent/train/value_loss            | 0.245       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -48.1       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -29.2       |\n",
      "|    agent/time/fps                    | 1443        |\n",
      "|    agent/time/iterations             | 8           |\n",
      "|    agent/time/time_elapsed           | 11          |\n",
      "|    agent/time/total_timesteps        | 57344       |\n",
      "|    agent/train/approx_kl             | 0.013506836 |\n",
      "|    agent/train/clip_fraction         | 0.424       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.02       |\n",
      "|    agent/train/explained_variance    | 0.951       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.0408      |\n",
      "|    agent/train/n_updates             | 270         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0162     |\n",
      "|    agent/train/std                   | 0.679       |\n",
      "|    agent/train/value_loss            | 0.275       |\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -45.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -28.8       |\n",
      "|    agent/time/fps                    | 1419        |\n",
      "|    agent/time/iterations             | 9           |\n",
      "|    agent/time/time_elapsed           | 12          |\n",
      "|    agent/time/total_timesteps        | 59392       |\n",
      "|    agent/train/approx_kl             | 0.020747326 |\n",
      "|    agent/train/clip_fraction         | 0.424       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.94       |\n",
      "|    agent/train/explained_variance    | 0.933       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.0869      |\n",
      "|    agent/train/n_updates             | 280         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0156     |\n",
      "|    agent/train/std                   | 0.653       |\n",
      "|    agent/train/value_loss            | 0.269       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -43.9       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -27.6       |\n",
      "|    agent/time/fps                    | 1388        |\n",
      "|    agent/time/iterations             | 10          |\n",
      "|    agent/time/time_elapsed           | 14          |\n",
      "|    agent/time/total_timesteps        | 61440       |\n",
      "|    agent/train/approx_kl             | 0.012096291 |\n",
      "|    agent/train/clip_fraction         | 0.394       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.85       |\n",
      "|    agent/train/explained_variance    | 0.948       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.0367      |\n",
      "|    agent/train/n_updates             | 290         |\n",
      "|    agent/train/policy_gradient_loss  | -0.013      |\n",
      "|    agent/train/std                   | 0.626       |\n",
      "|    agent/train/value_loss            | 0.214       |\n",
      "------------------------------------------------------\n",
      "DBSCAN found 25 clusters. 25 groups were created.\n",
      "Before cleaning: Group 1 has 5 fragments, Group 2 has 5 fragments\n",
      "Group 1 has an average reward of -47.682454681396486, Group 2 has an average reward of -50.84972915649414\n",
      "Groups reward border is -49.266091918945314\n",
      "Group 1 has 4 fragments, Group 2 has 3 fragments\n",
      "Before cleaning: Group 1 has 3 fragments, Group 2 has 5 fragments\n",
      "Group 1 has an average reward of -53.49494425455729, Group 2 has an average reward of -47.682454681396486\n",
      "Groups reward border is -50.58869946797689\n",
      "Group 1 has 2 fragments, Group 2 has 4 fragments\n",
      "Before cleaning: Group 1 has 9 fragments, Group 2 has 11 fragments\n",
      "Group 1 has an average reward of -51.64339913262261, Group 2 has an average reward of -50.91273082386363\n",
      "Groups reward border is -51.27806497824312\n",
      "Group 1 has 3 fragments, Group 2 has 6 fragments\n",
      "Before cleaning: Group 1 has 3 fragments, Group 2 has 6 fragments\n",
      "Group 1 has an average reward of -49.63198725382487, Group 2 has an average reward of -51.735032399495445\n",
      "Groups reward border is -50.683509826660156\n",
      "Group 1 has 1 fragments, Group 2 has 4 fragments\n",
      "Before cleaning: Group 1 has 3 fragments, Group 2 has 18 fragments\n",
      "Group 1 has an average reward of -49.63198725382487, Group 2 has an average reward of -47.59589640299479\n",
      "Groups reward border is -48.61394182840983\n",
      "Group 1 has 2 fragments, Group 2 has 12 fragments\n",
      "Before cleaning: Group 1 has 3 fragments, Group 2 has 29 fragments\n",
      "Group 1 has an average reward of -47.582297007242836, Group 2 has an average reward of -48.4132247135557\n",
      "Groups reward border is -47.99776086039927\n",
      "Group 1 has 2 fragments, Group 2 has 14 fragments\n",
      "Before cleaning: Group 1 has 5 fragments, Group 2 has 29 fragments\n",
      "Group 1 has an average reward of -47.682454681396486, Group 2 has an average reward of -48.4132247135557\n",
      "Groups reward border is -48.04783969747609\n",
      "Group 1 has 3 fragments, Group 2 has 14 fragments\n",
      "Before cleaning: Group 1 has 11 fragments, Group 2 has 8 fragments\n",
      "Group 1 has an average reward of -50.91273082386363, Group 2 has an average reward of -49.56420373916626\n",
      "Groups reward border is -50.23846728151494\n",
      "Group 1 has 8 fragments, Group 2 has 5 fragments\n",
      "Before cleaning: Group 1 has 8 fragments, Group 2 has 67 fragments\n",
      "Group 1 has an average reward of -45.57709550857544, Group 2 has an average reward of -45.898235548788044\n",
      "Groups reward border is -45.737665528681745\n",
      "Group 1 has 5 fragments, Group 2 has 30 fragments\n",
      "Before cleaning: Group 1 has 12 fragments, Group 2 has 29 fragments\n",
      "Group 1 has an average reward of -50.61490948994955, Group 2 has an average reward of -48.4132247135557\n",
      "Groups reward border is -49.51406710175262\n",
      "Group 1 has 7 fragments, Group 2 has 16 fragments\n",
      "Before cleaning: Group 1 has 9 fragments, Group 2 has 8 fragments\n",
      "Group 1 has an average reward of -51.64339913262261, Group 2 has an average reward of -49.56420373916626\n",
      "Groups reward border is -50.60380143589444\n",
      "Group 1 has 4 fragments, Group 2 has 5 fragments\n",
      "Before cleaning: Group 1 has 3 fragments, Group 2 has 67 fragments\n",
      "Group 1 has an average reward of -49.22677866617838, Group 2 has an average reward of -45.898235548788044\n",
      "Groups reward border is -47.56250710748321\n",
      "Group 1 has 2 fragments, Group 2 has 41 fragments\n",
      "The system did 12 group comparisons.\n",
      "This amounts to a total of 197 trajectories in comparisons\n",
      "Generated 1317 preferences\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b605221ced1b427088200fc414fde69c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab6cf1d45bcb47daa77935cc3bffce2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad98602fec04ca3a09423cb950ac8e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b69d76b381254a68a3727d53febbc76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84bcc32c4d01469f9c5d09e5f899ddf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -42.5       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -27.4       |\n",
      "|    agent/time/fps                    | 2868        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 63488       |\n",
      "|    agent/train/approx_kl             | 0.015341112 |\n",
      "|    agent/train/clip_fraction         | 0.408       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.77       |\n",
      "|    agent/train/explained_variance    | 0.948       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.097       |\n",
      "|    agent/train/n_updates             | 300         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0114     |\n",
      "|    agent/train/std                   | 0.601       |\n",
      "|    agent/train/value_loss            | 0.272       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -41.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -34.5       |\n",
      "|    agent/time/fps                    | 1876        |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 2           |\n",
      "|    agent/time/total_timesteps        | 65536       |\n",
      "|    agent/train/approx_kl             | 0.010187773 |\n",
      "|    agent/train/clip_fraction         | 0.324       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.69       |\n",
      "|    agent/train/explained_variance    | 0.535       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.355       |\n",
      "|    agent/train/n_updates             | 310         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00895    |\n",
      "|    agent/train/std                   | 0.578       |\n",
      "|    agent/train/value_loss            | 1.55        |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -39.9       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -41         |\n",
      "|    agent/time/fps                    | 1685        |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 3           |\n",
      "|    agent/time/total_timesteps        | 67584       |\n",
      "|    agent/train/approx_kl             | 0.013043201 |\n",
      "|    agent/train/clip_fraction         | 0.381       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.59       |\n",
      "|    agent/train/explained_variance    | 0.715       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.32        |\n",
      "|    agent/train/n_updates             | 320         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0131     |\n",
      "|    agent/train/std                   | 0.551       |\n",
      "|    agent/train/value_loss            | 1.14        |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 50         |\n",
      "|    agent/rollout/ep_rew_mean         | -38.5      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -42.6      |\n",
      "|    agent/time/fps                    | 1593       |\n",
      "|    agent/time/iterations             | 4          |\n",
      "|    agent/time/time_elapsed           | 5          |\n",
      "|    agent/time/total_timesteps        | 69632      |\n",
      "|    agent/train/approx_kl             | 0.01216899 |\n",
      "|    agent/train/clip_fraction         | 0.376      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -1.51      |\n",
      "|    agent/train/explained_variance    | 0.744      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | 0.251      |\n",
      "|    agent/train/n_updates             | 330        |\n",
      "|    agent/train/policy_gradient_loss  | -0.0108    |\n",
      "|    agent/train/std                   | 0.527      |\n",
      "|    agent/train/value_loss            | 0.903      |\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 50         |\n",
      "|    agent/rollout/ep_rew_mean         | -36.3      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -41.6      |\n",
      "|    agent/time/fps                    | 1549       |\n",
      "|    agent/time/iterations             | 5          |\n",
      "|    agent/time/time_elapsed           | 6          |\n",
      "|    agent/time/total_timesteps        | 71680      |\n",
      "|    agent/train/approx_kl             | 0.01569169 |\n",
      "|    agent/train/clip_fraction         | 0.457      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -1.41      |\n",
      "|    agent/train/explained_variance    | 0.69       |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | 0.23       |\n",
      "|    agent/train/n_updates             | 340        |\n",
      "|    agent/train/policy_gradient_loss  | -0.0128    |\n",
      "|    agent/train/std                   | 0.502      |\n",
      "|    agent/train/value_loss            | 0.707      |\n",
      "-----------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -33.9        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -40          |\n",
      "|    agent/time/fps                    | 1466         |\n",
      "|    agent/time/iterations             | 6            |\n",
      "|    agent/time/time_elapsed           | 8            |\n",
      "|    agent/time/total_timesteps        | 73728        |\n",
      "|    agent/train/approx_kl             | 0.0152184935 |\n",
      "|    agent/train/clip_fraction         | 0.444        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.31        |\n",
      "|    agent/train/explained_variance    | 0.723        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.206        |\n",
      "|    agent/train/n_updates             | 350          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0163      |\n",
      "|    agent/train/std                   | 0.478        |\n",
      "|    agent/train/value_loss            | 0.53         |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -31.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -38         |\n",
      "|    agent/time/fps                    | 1382        |\n",
      "|    agent/time/iterations             | 7           |\n",
      "|    agent/time/time_elapsed           | 10          |\n",
      "|    agent/time/total_timesteps        | 75776       |\n",
      "|    agent/train/approx_kl             | 0.018140515 |\n",
      "|    agent/train/clip_fraction         | 0.468       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.21       |\n",
      "|    agent/train/explained_variance    | 0.672       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.202       |\n",
      "|    agent/train/n_updates             | 360         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0169     |\n",
      "|    agent/train/std                   | 0.454       |\n",
      "|    agent/train/value_loss            | 0.583       |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -30.9        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -36.2        |\n",
      "|    agent/time/fps                    | 1357         |\n",
      "|    agent/time/iterations             | 8            |\n",
      "|    agent/time/time_elapsed           | 12           |\n",
      "|    agent/time/total_timesteps        | 77824        |\n",
      "|    agent/train/approx_kl             | 0.0152023025 |\n",
      "|    agent/train/clip_fraction         | 0.48         |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.11        |\n",
      "|    agent/train/explained_variance    | 0.659        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.114        |\n",
      "|    agent/train/n_updates             | 370          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0146      |\n",
      "|    agent/train/std                   | 0.431        |\n",
      "|    agent/train/value_loss            | 0.415        |\n",
      "-------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -29.2       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -35.6       |\n",
      "|    agent/time/fps                    | 1344        |\n",
      "|    agent/time/iterations             | 9           |\n",
      "|    agent/time/time_elapsed           | 13          |\n",
      "|    agent/time/total_timesteps        | 79872       |\n",
      "|    agent/train/approx_kl             | 0.017822845 |\n",
      "|    agent/train/clip_fraction         | 0.451       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.01       |\n",
      "|    agent/train/explained_variance    | 0.615       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.0987      |\n",
      "|    agent/train/n_updates             | 380         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0127     |\n",
      "|    agent/train/std                   | 0.407       |\n",
      "|    agent/train/value_loss            | 0.488       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -27         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -34.3       |\n",
      "|    agent/time/fps                    | 1334        |\n",
      "|    agent/time/iterations             | 10          |\n",
      "|    agent/time/time_elapsed           | 15          |\n",
      "|    agent/time/total_timesteps        | 81920       |\n",
      "|    agent/train/approx_kl             | 0.016808787 |\n",
      "|    agent/train/clip_fraction         | 0.443       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.91       |\n",
      "|    agent/train/explained_variance    | 0.703       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.169       |\n",
      "|    agent/train/n_updates             | 390         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0117     |\n",
      "|    agent/train/std                   | 0.388       |\n",
      "|    agent/train/value_loss            | 0.458       |\n",
      "------------------------------------------------------\n",
      "DBSCAN found 17 clusters. 17 groups were created.\n",
      "Before cleaning: Group 1 has 45 fragments, Group 2 has 58 fragments\n",
      "Group 1 has an average reward of -30.447476874457465, Group 2 has an average reward of -30.34737465299409\n",
      "Groups reward border is -30.397425763725778\n",
      "Group 1 has 22 fragments, Group 2 has 27 fragments\n",
      "Before cleaning: Group 1 has 9 fragments, Group 2 has 10 fragments\n",
      "Group 1 has an average reward of -26.974497689141167, Group 2 has an average reward of -30.525727653503417\n",
      "Groups reward border is -28.75011267132229\n",
      "Group 1 has 6 fragments, Group 2 has 6 fragments\n",
      "Before cleaning: Group 1 has 6 fragments, Group 2 has 11 fragments\n",
      "Group 1 has an average reward of -27.399195671081543, Group 2 has an average reward of -32.8310397755016\n",
      "Groups reward border is -30.115117723291572\n",
      "Group 1 has 5 fragments, Group 2 has 7 fragments\n",
      "Before cleaning: Group 1 has 25 fragments, Group 2 has 5 fragments\n",
      "Group 1 has an average reward of -34.50820144653321, Group 2 has an average reward of -24.055361938476562\n",
      "Groups reward border is -29.281781692504886\n",
      "Group 1 has 20 fragments, Group 2 has 5 fragments\n",
      "Before cleaning: Group 1 has 9 fragments, Group 2 has 5 fragments\n",
      "Group 1 has an average reward of -26.974497689141167, Group 2 has an average reward of -32.07028007507324\n",
      "Groups reward border is -29.522388882107204\n",
      "Group 1 has 7 fragments, Group 2 has 3 fragments\n",
      "Before cleaning: Group 1 has 45 fragments, Group 2 has 32 fragments\n",
      "Group 1 has an average reward of -30.447476874457465, Group 2 has an average reward of -30.41956228017807\n",
      "Groups reward border is -30.433519577317767\n",
      "Group 1 has 21 fragments, Group 2 has 16 fragments\n",
      "The system did 6 group comparisons.\n",
      "This amounts to a total of 145 trajectories in comparisons\n",
      "Generated 1330 preferences\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05282919bb9a4d31ae042601e3d3fdd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5a1477413f4d6d9dcfa79bd308dec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "028c045b530a412fbc494fd387d1e68b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116ece8413824a3fa5d8165ee23fc455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d683b497f2c4a0682eb46ad0b504fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -24.9       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -33.5       |\n",
      "|    agent/time/fps                    | 2954        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 83968       |\n",
      "|    agent/train/approx_kl             | 0.019372905 |\n",
      "|    agent/train/clip_fraction         | 0.484       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.797      |\n",
      "|    agent/train/explained_variance    | 0.619       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.13        |\n",
      "|    agent/train/n_updates             | 400         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0164     |\n",
      "|    agent/train/std                   | 0.366       |\n",
      "|    agent/train/value_loss            | 0.445       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -23.5       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -33.6       |\n",
      "|    agent/time/fps                    | 1916        |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 2           |\n",
      "|    agent/time/total_timesteps        | 86016       |\n",
      "|    agent/train/approx_kl             | 0.016366199 |\n",
      "|    agent/train/clip_fraction         | 0.428       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.709      |\n",
      "|    agent/train/explained_variance    | 0.604       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.191       |\n",
      "|    agent/train/n_updates             | 410         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0116     |\n",
      "|    agent/train/std                   | 0.35        |\n",
      "|    agent/train/value_loss            | 0.54        |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 50         |\n",
      "|    agent/rollout/ep_rew_mean         | -22.1      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -33.7      |\n",
      "|    agent/time/fps                    | 1711       |\n",
      "|    agent/time/iterations             | 3          |\n",
      "|    agent/time/time_elapsed           | 3          |\n",
      "|    agent/time/total_timesteps        | 88064      |\n",
      "|    agent/train/approx_kl             | 0.01627401 |\n",
      "|    agent/train/clip_fraction         | 0.486      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -0.604     |\n",
      "|    agent/train/explained_variance    | 0.689      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | 0.212      |\n",
      "|    agent/train/n_updates             | 420        |\n",
      "|    agent/train/policy_gradient_loss  | -0.0115    |\n",
      "|    agent/train/std                   | 0.332      |\n",
      "|    agent/train/value_loss            | 0.475      |\n",
      "-----------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -20.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -31         |\n",
      "|    agent/time/fps                    | 1623        |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 5           |\n",
      "|    agent/time/total_timesteps        | 90112       |\n",
      "|    agent/train/approx_kl             | 0.017681722 |\n",
      "|    agent/train/clip_fraction         | 0.495       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.499      |\n",
      "|    agent/train/explained_variance    | 0.808       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.194       |\n",
      "|    agent/train/n_updates             | 430         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0115     |\n",
      "|    agent/train/std                   | 0.315       |\n",
      "|    agent/train/value_loss            | 0.467       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -19.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -29.5       |\n",
      "|    agent/time/fps                    | 1543        |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 6           |\n",
      "|    agent/time/total_timesteps        | 92160       |\n",
      "|    agent/train/approx_kl             | 0.017422136 |\n",
      "|    agent/train/clip_fraction         | 0.46        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.401      |\n",
      "|    agent/train/explained_variance    | 0.808       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.177       |\n",
      "|    agent/train/n_updates             | 440         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0125     |\n",
      "|    agent/train/std                   | 0.298       |\n",
      "|    agent/train/value_loss            | 0.557       |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 50         |\n",
      "|    agent/rollout/ep_rew_mean         | -18.8      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -27.9      |\n",
      "|    agent/time/fps                    | 1485       |\n",
      "|    agent/time/iterations             | 6          |\n",
      "|    agent/time/time_elapsed           | 8          |\n",
      "|    agent/time/total_timesteps        | 94208      |\n",
      "|    agent/train/approx_kl             | 0.01813654 |\n",
      "|    agent/train/clip_fraction         | 0.44       |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -0.307     |\n",
      "|    agent/train/explained_variance    | 0.793      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | 0.157      |\n",
      "|    agent/train/n_updates             | 450        |\n",
      "|    agent/train/policy_gradient_loss  | -0.0106    |\n",
      "|    agent/train/std                   | 0.284      |\n",
      "|    agent/train/value_loss            | 0.416      |\n",
      "-----------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -17.6       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -27         |\n",
      "|    agent/time/fps                    | 1426        |\n",
      "|    agent/time/iterations             | 7           |\n",
      "|    agent/time/time_elapsed           | 10          |\n",
      "|    agent/time/total_timesteps        | 96256       |\n",
      "|    agent/train/approx_kl             | 0.021098474 |\n",
      "|    agent/train/clip_fraction         | 0.477       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -0.207      |\n",
      "|    agent/train/explained_variance    | 0.856       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.124       |\n",
      "|    agent/train/n_updates             | 460         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00859    |\n",
      "|    agent/train/std                   | 0.271       |\n",
      "|    agent/train/value_loss            | 0.338       |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 50         |\n",
      "|    agent/rollout/ep_rew_mean         | -16.6      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -25.5      |\n",
      "|    agent/time/fps                    | 1379       |\n",
      "|    agent/time/iterations             | 8          |\n",
      "|    agent/time/time_elapsed           | 11         |\n",
      "|    agent/time/total_timesteps        | 98304      |\n",
      "|    agent/train/approx_kl             | 0.01523772 |\n",
      "|    agent/train/clip_fraction         | 0.483      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -0.104     |\n",
      "|    agent/train/explained_variance    | 0.907      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | 0.135      |\n",
      "|    agent/train/n_updates             | 470        |\n",
      "|    agent/train/policy_gradient_loss  | -0.0124    |\n",
      "|    agent/train/std                   | 0.258      |\n",
      "|    agent/train/value_loss            | 0.32       |\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 50         |\n",
      "|    agent/rollout/ep_rew_mean         | -15.4      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -24.4      |\n",
      "|    agent/time/fps                    | 1338       |\n",
      "|    agent/time/iterations             | 9          |\n",
      "|    agent/time/time_elapsed           | 13         |\n",
      "|    agent/time/total_timesteps        | 100352     |\n",
      "|    agent/train/approx_kl             | 0.01871545 |\n",
      "|    agent/train/clip_fraction         | 0.482      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -0.0167    |\n",
      "|    agent/train/explained_variance    | 0.881      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | 0.0634     |\n",
      "|    agent/train/n_updates             | 480        |\n",
      "|    agent/train/policy_gradient_loss  | -0.00856   |\n",
      "|    agent/train/std                   | 0.246      |\n",
      "|    agent/train/value_loss            | 0.328      |\n",
      "-----------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -14.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -22.8       |\n",
      "|    agent/time/fps                    | 1317        |\n",
      "|    agent/time/iterations             | 10          |\n",
      "|    agent/time/time_elapsed           | 15          |\n",
      "|    agent/time/total_timesteps        | 102400      |\n",
      "|    agent/train/approx_kl             | 0.018219844 |\n",
      "|    agent/train/clip_fraction         | 0.474       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | 0.0755      |\n",
      "|    agent/train/explained_variance    | 0.896       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.12        |\n",
      "|    agent/train/n_updates             | 490         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00845    |\n",
      "|    agent/train/std                   | 0.233       |\n",
      "|    agent/train/value_loss            | 0.296       |\n",
      "------------------------------------------------------\n",
      "DBSCAN found 17 clusters. 17 groups were created.\n",
      "Before cleaning: Group 1 has 4 fragments, Group 2 has 4 fragments\n",
      "Group 1 has an average reward of -19.857810735702515, Group 2 has an average reward of -11.583773136138916\n",
      "Groups reward border is -15.720791935920715\n",
      "Group 1 has 4 fragments, Group 2 has 4 fragments\n",
      "Before cleaning: Group 1 has 4 fragments, Group 2 has 8 fragments\n",
      "Group 1 has an average reward of -19.857810735702515, Group 2 has an average reward of -16.472768187522888\n",
      "Groups reward border is -18.1652894616127\n",
      "Group 1 has 3 fragments, Group 2 has 6 fragments\n",
      "Before cleaning: Group 1 has 29 fragments, Group 2 has 8 fragments\n",
      "Group 1 has an average reward of -16.416814902733112, Group 2 has an average reward of -16.472768187522888\n",
      "Groups reward border is -16.444791545128\n",
      "Group 1 has 16 fragments, Group 2 has 3 fragments\n",
      "Before cleaning: Group 1 has 3 fragments, Group 2 has 3 fragments\n",
      "Group 1 has an average reward of -16.47534465789795, Group 2 has an average reward of -19.471497853597004\n",
      "Groups reward border is -17.973421255747475\n",
      "Group 1 has 2 fragments, Group 2 has 3 fragments\n",
      "Before cleaning: Group 1 has 29 fragments, Group 2 has 4 fragments\n",
      "Group 1 has an average reward of -16.416814902733112, Group 2 has an average reward of -11.583773136138916\n",
      "Groups reward border is -14.000294019436014\n",
      "Group 1 has 23 fragments, Group 2 has 4 fragments\n",
      "Before cleaning: Group 1 has 22 fragments, Group 2 has 21 fragments\n",
      "Group 1 has an average reward of -18.081192536787555, Group 2 has an average reward of -17.227343740917387\n",
      "Groups reward border is -17.654268138852473\n",
      "Group 1 has 9 fragments, Group 2 has 13 fragments\n",
      "Before cleaning: Group 1 has 9 fragments, Group 2 has 3 fragments\n",
      "Group 1 has an average reward of -16.36643081241184, Group 2 has an average reward of -19.471497853597004\n",
      "Groups reward border is -17.91896433300442\n",
      "Group 1 has 7 fragments, Group 2 has 3 fragments\n",
      "Before cleaning: Group 1 has 4 fragments, Group 2 has 28 fragments\n",
      "Group 1 has an average reward of -11.583773136138916, Group 2 has an average reward of -13.44502341747284\n",
      "Groups reward border is -12.514398276805878\n",
      "Group 1 has 3 fragments, Group 2 has 15 fragments\n",
      "Before cleaning: Group 1 has 8 fragments, Group 2 has 21 fragments\n",
      "Group 1 has an average reward of -16.727583169937134, Group 2 has an average reward of -17.227343740917387\n",
      "Groups reward border is -16.977463455427262\n",
      "Group 1 has 4 fragments, Group 2 has 10 fragments\n",
      "The system did 9 group comparisons.\n",
      "This amounts to a total of 132 trajectories in comparisons\n",
      "Generated 949 preferences\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494cba2b2be241d989da1051da60aeb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6ccd39b78e4219b1f1288ca42d55bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412effb921334101a672a05ffc0568be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095ecfc270da411197795559d20f0f68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc45969b1ef4164a7937416002af032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -14.1       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -24.7       |\n",
      "|    agent/time/fps                    | 2895        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 104448      |\n",
      "|    agent/train/approx_kl             | 0.018814774 |\n",
      "|    agent/train/clip_fraction         | 0.49        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | 0.18        |\n",
      "|    agent/train/explained_variance    | 0.902       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.0984      |\n",
      "|    agent/train/n_updates             | 500         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00627    |\n",
      "|    agent/train/std                   | 0.222       |\n",
      "|    agent/train/value_loss            | 0.256       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -13.5       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -25.9       |\n",
      "|    agent/time/fps                    | 1672        |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 2           |\n",
      "|    agent/time/total_timesteps        | 106496      |\n",
      "|    agent/train/approx_kl             | 0.022332748 |\n",
      "|    agent/train/clip_fraction         | 0.503       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | 0.268       |\n",
      "|    agent/train/explained_variance    | 0.829       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.105       |\n",
      "|    agent/train/n_updates             | 510         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00276    |\n",
      "|    agent/train/std                   | 0.211       |\n",
      "|    agent/train/value_loss            | 0.399       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -13         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -27         |\n",
      "|    agent/time/fps                    | 1461        |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 4           |\n",
      "|    agent/time/total_timesteps        | 108544      |\n",
      "|    agent/train/approx_kl             | 0.019450657 |\n",
      "|    agent/train/clip_fraction         | 0.513       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | 0.373       |\n",
      "|    agent/train/explained_variance    | 0.929       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.121       |\n",
      "|    agent/train/n_updates             | 520         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00641    |\n",
      "|    agent/train/std                   | 0.201       |\n",
      "|    agent/train/value_loss            | 0.245       |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 50         |\n",
      "|    agent/rollout/ep_rew_mean         | -12        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -23.7      |\n",
      "|    agent/time/fps                    | 1378       |\n",
      "|    agent/time/iterations             | 4          |\n",
      "|    agent/time/time_elapsed           | 5          |\n",
      "|    agent/time/total_timesteps        | 110592     |\n",
      "|    agent/train/approx_kl             | 0.01819947 |\n",
      "|    agent/train/clip_fraction         | 0.494      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | 0.464      |\n",
      "|    agent/train/explained_variance    | 0.89       |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | 0.102      |\n",
      "|    agent/train/n_updates             | 530        |\n",
      "|    agent/train/policy_gradient_loss  | -0.000568  |\n",
      "|    agent/train/std                   | 0.193      |\n",
      "|    agent/train/value_loss            | 0.25       |\n",
      "-----------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -11.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -21.3       |\n",
      "|    agent/time/fps                    | 1332        |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 7           |\n",
      "|    agent/time/total_timesteps        | 112640      |\n",
      "|    agent/train/approx_kl             | 0.020086173 |\n",
      "|    agent/train/clip_fraction         | 0.484       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | 0.543       |\n",
      "|    agent/train/explained_variance    | 0.941       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.0726      |\n",
      "|    agent/train/n_updates             | 540         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00397    |\n",
      "|    agent/train/std                   | 0.184       |\n",
      "|    agent/train/value_loss            | 0.249       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -10.9       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -20.5       |\n",
      "|    agent/time/fps                    | 1293        |\n",
      "|    agent/time/iterations             | 6           |\n",
      "|    agent/time/time_elapsed           | 9           |\n",
      "|    agent/time/total_timesteps        | 114688      |\n",
      "|    agent/train/approx_kl             | 0.022177942 |\n",
      "|    agent/train/clip_fraction         | 0.471       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | 0.617       |\n",
      "|    agent/train/explained_variance    | 0.937       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.0915      |\n",
      "|    agent/train/n_updates             | 550         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00199    |\n",
      "|    agent/train/std                   | 0.177       |\n",
      "|    agent/train/value_loss            | 0.214       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -10.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -19.7       |\n",
      "|    agent/time/fps                    | 1273        |\n",
      "|    agent/time/iterations             | 7           |\n",
      "|    agent/time/time_elapsed           | 11          |\n",
      "|    agent/time/total_timesteps        | 116736      |\n",
      "|    agent/train/approx_kl             | 0.017113976 |\n",
      "|    agent/train/clip_fraction         | 0.495       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | 0.71        |\n",
      "|    agent/train/explained_variance    | 0.939       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.0226      |\n",
      "|    agent/train/n_updates             | 560         |\n",
      "|    agent/train/policy_gradient_loss  | 0.00115     |\n",
      "|    agent/train/std                   | 0.169       |\n",
      "|    agent/train/value_loss            | 0.184       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -9.99       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -18.9       |\n",
      "|    agent/time/fps                    | 1258        |\n",
      "|    agent/time/iterations             | 8           |\n",
      "|    agent/time/time_elapsed           | 13          |\n",
      "|    agent/time/total_timesteps        | 118784      |\n",
      "|    agent/train/approx_kl             | 0.026449583 |\n",
      "|    agent/train/clip_fraction         | 0.504       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | 0.791       |\n",
      "|    agent/train/explained_variance    | 0.953       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.0788      |\n",
      "|    agent/train/n_updates             | 570         |\n",
      "|    agent/train/policy_gradient_loss  | 0.00512     |\n",
      "|    agent/train/std                   | 0.162       |\n",
      "|    agent/train/value_loss            | 0.15        |\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -9.73       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -18.2       |\n",
      "|    agent/time/fps                    | 1247        |\n",
      "|    agent/time/iterations             | 9           |\n",
      "|    agent/time/time_elapsed           | 14          |\n",
      "|    agent/time/total_timesteps        | 120832      |\n",
      "|    agent/train/approx_kl             | 0.023953147 |\n",
      "|    agent/train/clip_fraction         | 0.54        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | 0.899       |\n",
      "|    agent/train/explained_variance    | 0.919       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.042       |\n",
      "|    agent/train/n_updates             | 580         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00025    |\n",
      "|    agent/train/std                   | 0.155       |\n",
      "|    agent/train/value_loss            | 0.154       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -9.72       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -17.6       |\n",
      "|    agent/time/fps                    | 1236        |\n",
      "|    agent/time/iterations             | 10          |\n",
      "|    agent/time/time_elapsed           | 16          |\n",
      "|    agent/time/total_timesteps        | 122880      |\n",
      "|    agent/train/approx_kl             | 0.026907107 |\n",
      "|    agent/train/clip_fraction         | 0.528       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | 0.969       |\n",
      "|    agent/train/explained_variance    | 0.961       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.0529      |\n",
      "|    agent/train/n_updates             | 590         |\n",
      "|    agent/train/policy_gradient_loss  | 0.00147     |\n",
      "|    agent/train/std                   | 0.149       |\n",
      "|    agent/train/value_loss            | 0.141       |\n",
      "------------------------------------------------------\n",
      "{'reward_loss': 0.010784060014295055, 'reward_accuracy': 0.9971186828264365}\n"
     ]
    }
   ],
   "source": [
    "pairwise_group_comparison_result = intantiate_and_train(False)    \n",
    "\n",
    "print(pairwise_group_comparison_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Active selection fragmenter based on random fragmenter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_comparisons_3.train(\n",
    "    total_timesteps=50_000,\n",
    "    total_comparisons=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we trained the reward network using the preference comparisons algorithm, we can wrap our environment with that learned reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.rewards.reward_wrapper import RewardVecEnvWrapper\n",
    "\n",
    "learned_reward_venv = RewardVecEnvWrapper(venv, reward_net.predict_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train an agent that sees only the shaped, learned reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7a1b8839c4c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner = PPO(\n",
    "    seed=0,\n",
    "    policy=FeedForward32Policy,\n",
    "    policy_kwargs=dict(\n",
    "        features_extractor_class=NormalizeFeaturesExtractor,\n",
    "        features_extractor_kwargs=dict(normalize_class=RunningNorm),\n",
    "    ),\n",
    "    env=learned_reward_venv,\n",
    "    batch_size=64,\n",
    "    ent_coef=0.01,\n",
    "    n_epochs=10,\n",
    "    n_steps=2048 // learned_reward_venv.num_envs,\n",
    "    clip_range=0.1,\n",
    "    gae_lambda=0.95,\n",
    "    gamma=0.97,\n",
    "    learning_rate=2e-3,\n",
    ")\n",
    "learner.learn(100_000)  # Note: set to 100_000 to train a proficient expert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can evaluate it using the original reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: -11 +/- 0\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "n_eval_episodes = 100\n",
    "reward_mean, reward_std = evaluate_policy(learner.policy, venv, n_eval_episodes)\n",
    "reward_stderr = reward_std / np.sqrt(n_eval_episodes)\n",
    "print(f\"Reward: {reward_mean:.0f} +/- {reward_stderr:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('imitation_ppo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/lib/python3.8/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /imitation/docs/tutorials/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-0.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-0.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-1.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-1.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-2.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-2.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-2.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-3.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-3.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-3.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-4.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-4.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-4.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-5.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-5.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-5.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-6.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-6.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-6.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-7.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-7.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-7.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-8.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-8.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-8.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-9.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-9.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-9.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-10.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-10.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-10.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-11.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-11.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-11.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-12.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-12.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-12.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-13.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-13.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-13.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-14.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-14.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-14.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-15.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-15.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-15.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-16.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-16.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-16.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-17.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-17.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-17.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-18.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-18.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-18.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-19.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-19.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-19.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-20.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-20.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-20.mp4\n"
     ]
    }
   ],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"Reacher-v4\", render_mode='rgb_array')\n",
    "env = RecordVideo(env, './evaluation_videos', name_prefix=\"reacher\", episode_trigger=lambda x: x % 1 == 0) \n",
    "\n",
    "# Run the model in the environment\n",
    "obs, info = env.reset()\n",
    "for _ in range(1000):\n",
    "        action, _states = learner.predict(obs, deterministic=True)\n",
    "        obs, reward, _ ,done, info = env.step(action)\n",
    "        if done:\n",
    "            obs, info = env.reset()\n",
    "            \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "439158cd89905785fcc749928062ade7bfccc3f087fab145e5671f895c635937"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
