{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[download this notebook here](https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/5_train_preference_comparisons.ipynb)\n",
    "# Learning a Reward Function using Preference Comparisons\n",
    "\n",
    "The preference comparisons algorithm learns a reward function by comparing trajectory segments to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up the preference comparisons algorithm, we first need to set up a lot of its internals beforehand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.2, Python 3.8.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from imitation.algorithms import preference_comparisons\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.util.util import make_vec_env\n",
    "from imitation.policies.base import FeedForward32Policy, NormalizeFeaturesExtractor\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "venv = make_vec_env(\"Reacher-v4\", rng=rng, render_mode='rgb_array', n_envs=1)\n",
    "\n",
    "reward_net = BasicRewardNet(\n",
    "    venv.observation_space, venv.action_space, normalize_input_layer=RunningNorm\n",
    ")\n",
    "\n",
    "fragmenter = preference_comparisons.RandomFragmenter(\n",
    "    warning_threshold=0,\n",
    "    rng=rng,\n",
    ")\n",
    "#gatherer = preference_comparisons.SyntheticGatherer(rng=rng)\n",
    "gatherer = preference_comparisons.HumanGatherer(rng=rng)\n",
    "preference_model = preference_comparisons.PreferenceModel(reward_net)\n",
    "reward_trainer = preference_comparisons.BasicRewardTrainer(\n",
    "    preference_model=preference_model,\n",
    "    loss=preference_comparisons.CrossEntropyRewardLoss(),\n",
    "    epochs=3,\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "\n",
    "# Several hyperparameters (reward_epochs, ppo_clip_range, ppo_ent_coef,\n",
    "# ppo_gae_lambda, ppo_n_epochs, discount_factor, use_sde, sde_sample_freq,\n",
    "# ppo_lr, exploration_frac, num_iterations, initial_comparison_frac,\n",
    "# initial_epoch_multiplier, query_schedule) used in this example have been\n",
    "# approximately fine-tuned to reach a reasonable level of performance.\n",
    "agent = PPO(\n",
    "    policy=FeedForward32Policy,\n",
    "    policy_kwargs=dict(\n",
    "        features_extractor_class=NormalizeFeaturesExtractor,\n",
    "        features_extractor_kwargs=dict(normalize_class=RunningNorm),\n",
    "    ),\n",
    "    env=venv,\n",
    "    seed=0,\n",
    "    n_steps=2048 // venv.num_envs,\n",
    "    batch_size=64,\n",
    "    ent_coef=0.01,\n",
    "    learning_rate=2e-3,\n",
    "    clip_range=0.1,\n",
    "    gae_lambda=0.95,\n",
    "    gamma=0.97,\n",
    "    n_epochs=10,\n",
    ")\n",
    "\n",
    "trajectory_generator = preference_comparisons.AgentTrainerWithVideoBuffering(\n",
    "    algorithm=agent,\n",
    "    reward_fn=reward_net,\n",
    "    venv=venv,\n",
    "    rng=rng,\n",
    "    exploration_frac=0.05,\n",
    "    video_folder='./training_videos',\n",
    "    video_length=50,\n",
    "    name_prefix='rl-video'\n",
    ")\n",
    "\n",
    "\n",
    "pref_comparisons = preference_comparisons.PreferenceComparisons(\n",
    "    trajectory_generator,\n",
    "    reward_net,\n",
    "    num_iterations=5,  # Set to 60 for better performance\n",
    "    fragmenter=fragmenter,\n",
    "    preference_gatherer=gatherer,\n",
    "    reward_trainer=reward_trainer,\n",
    "    fragment_length=50,\n",
    "    transition_oversampling=1,\n",
    "    initial_comparison_frac=0.1,\n",
    "    allow_variable_horizon=False,\n",
    "    initial_epoch_multiplier=4,\n",
    "    query_schedule=\"hyperbolic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can start training the reward model. Note that we need to specify the total timesteps that the agent should be trained and how many fragment comparisons should be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query schedule: [50, 127, 102, 85, 73, 63]\n",
      "Collecting 100 fragments (5000 transitions)\n",
      "Requested 4750 transitions but only 0 in buffer. Sampling 4750 additional transitions.\n",
      "Sampling 250 exploratory transitions.\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-0-to-step-50.mp4\n",
      "Video path found in step 49: /imitation/docs/tutorials/training_videos/rl-video-step-100-to-step-150.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-100-to-step-150.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-150-to-step-200.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-200-to-step-250.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-250-to-step-300.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-300-to-step-350.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-350-to-step-400.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-400-to-step-450.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-450-to-step-500.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-500-to-step-550.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-550-to-step-600.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-600-to-step-650.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-650-to-step-700.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-700-to-step-750.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-750-to-step-800.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-800-to-step-850.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-850-to-step-900.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-900-to-step-950.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-950-to-step-1000.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-1000-to-step-1050.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-1050-to-step-1100.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-1100-to-step-1150.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-1150-to-step-1200.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-1200-to-step-1250.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-1250-to-step-1300.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-1300-to-step-1350.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-1350-to-step-1400.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-1400-to-step-1450.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-1450-to-step-1500.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-1500-to-step-1550.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-1550-to-step-1600.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-1600-to-step-1650.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-1650-to-step-1700.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-1700-to-step-1750.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-1750-to-step-1800.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-1800-to-step-1850.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-1850-to-step-1900.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-1900-to-step-1950.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-1950-to-step-2000.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-2000-to-step-2050.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-2050-to-step-2100.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-2100-to-step-2150.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-2150-to-step-2200.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-2200-to-step-2250.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-2250-to-step-2300.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-2300-to-step-2350.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-2350-to-step-2400.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-2400-to-step-2450.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-2450-to-step-2500.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-2500-to-step-2550.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-2550-to-step-2600.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-2600-to-step-2650.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-2650-to-step-2700.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-2700-to-step-2750.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-2750-to-step-2800.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-2800-to-step-2850.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-2850-to-step-2900.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-2900-to-step-2950.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-2950-to-step-3000.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-3000-to-step-3050.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-3050-to-step-3100.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-3100-to-step-3150.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-3150-to-step-3200.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-3200-to-step-3250.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-3250-to-step-3300.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-3300-to-step-3350.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-3350-to-step-3400.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-3400-to-step-3450.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-3450-to-step-3500.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-3500-to-step-3550.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-3550-to-step-3600.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-3600-to-step-3650.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-3650-to-step-3700.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-3700-to-step-3750.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-3750-to-step-3800.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-3800-to-step-3850.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-3850-to-step-3900.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-3900-to-step-3950.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-3950-to-step-4000.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-4000-to-step-4050.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-4050-to-step-4100.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-4100-to-step-4150.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-4150-to-step-4200.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-4200-to-step-4250.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-4250-to-step-4300.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-4300-to-step-4350.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-4350-to-step-4400.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-4400-to-step-4450.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-4450-to-step-4500.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-4500-to-step-4550.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-4550-to-step-4600.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-4600-to-step-4650.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-4650-to-step-4700.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-4700-to-step-4750.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-4750-to-step-4800.mp4\n",
      "Video path found in step 49: /imitation/docs/tutorials/training_videos/rl-video-step-4850-to-step-4900.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-4850-to-step-4900.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-4900-to-step-4950.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-4950-to-step-5000.mp4\n",
      "Creating fragment pairs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering preferences\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-1400-to-step-1450.mp4\n",
      "Video path found in step 0: /imitation/docs/tutorials/training_videos/rl-video-step-1100-to-step-1150.mp4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpref_comparisons\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50_000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_comparisons\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/imitation/src/imitation/algorithms/preference_comparisons.py:1881\u001b[0m, in \u001b[0;36mPreferenceComparisons.train\u001b[0;34m(self, total_timesteps, total_comparisons, callback)\u001b[0m\n\u001b[1;32m   1879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39maccumulate_means(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreferences\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1880\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGathering preferences\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1881\u001b[0m     preferences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreference_gatherer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfragments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mpush(fragments, preferences)\n\u001b[1;32m   1883\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset now contains \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m comparisons\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/imitation/src/imitation/algorithms/preference_comparisons.py:1074\u001b[0m, in \u001b[0;36mHumanGatherer.__call__\u001b[0;34m(self, fragment_pairs)\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBoth fragments in pair \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must have a video_path for human feedback. Frag1 path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_path1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Frag2 path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_path2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1073\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay_videos(video_path1, video_path2)\n\u001b[0;32m-> 1074\u001b[0m     feedback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_human_feedback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1075\u001b[0m     preferences\u001b[38;5;241m.\u001b[39mappend(feedback)\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(preferences, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m/imitation/src/imitation/algorithms/preference_comparisons.py:1044\u001b[0m, in \u001b[0;36mHumanGatherer.get_human_feedback\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1042\u001b[0m waiting_for_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m waiting_for_input:\n\u001b[0;32m-> 1044\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1045\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m event\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m pygame\u001b[38;5;241m.\u001b[39mQUIT:\n\u001b[1;32m   1046\u001b[0m             waiting_for_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pref_comparisons.train(\n",
    "    total_timesteps=50_000,\n",
    "    total_comparisons=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we trained the reward network using the preference comparisons algorithm, we can wrap our environment with that learned reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.rewards.reward_wrapper import RewardVecEnvWrapper\n",
    "\n",
    "learned_reward_venv = RewardVecEnvWrapper(venv, reward_net.predict_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train an agent that sees only the shaped, learned reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7a1b8839c4c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner = PPO(\n",
    "    seed=0,\n",
    "    policy=FeedForward32Policy,\n",
    "    policy_kwargs=dict(\n",
    "        features_extractor_class=NormalizeFeaturesExtractor,\n",
    "        features_extractor_kwargs=dict(normalize_class=RunningNorm),\n",
    "    ),\n",
    "    env=learned_reward_venv,\n",
    "    batch_size=64,\n",
    "    ent_coef=0.01,\n",
    "    n_epochs=10,\n",
    "    n_steps=2048 // learned_reward_venv.num_envs,\n",
    "    clip_range=0.1,\n",
    "    gae_lambda=0.95,\n",
    "    gamma=0.97,\n",
    "    learning_rate=2e-3,\n",
    ")\n",
    "learner.learn(100_000)  # Note: set to 100_000 to train a proficient expert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can evaluate it using the original reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: -11 +/- 0\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "n_eval_episodes = 100\n",
    "reward_mean, reward_std = evaluate_policy(learner.policy, venv, n_eval_episodes)\n",
    "reward_stderr = reward_std / np.sqrt(n_eval_episodes)\n",
    "print(f\"Reward: {reward_mean:.0f} +/- {reward_stderr:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('imitation_ppo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/lib/python3.8/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /imitation/docs/tutorials/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-0.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-0.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-1.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-1.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-2.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-2.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-2.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-3.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-3.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-3.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-4.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-4.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-4.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-5.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-5.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-5.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-6.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-6.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-6.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-7.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-7.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-7.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-8.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-8.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-8.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-9.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-9.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-9.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-10.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-10.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-10.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-11.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-11.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-11.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-12.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-12.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-12.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-13.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-13.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-13.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-14.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-14.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-14.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-15.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-15.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-15.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-16.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-16.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-16.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-17.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-17.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-17.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-18.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-18.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-18.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-19.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-19.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-19.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-20.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-20.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-20.mp4\n"
     ]
    }
   ],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"Reacher-v4\", render_mode='rgb_array')\n",
    "env = RecordVideo(env, './evaluation_videos', name_prefix=\"reacher\", episode_trigger=lambda x: x % 1 == 0) \n",
    "\n",
    "# Run the model in the environment\n",
    "obs, info = env.reset()\n",
    "for _ in range(1000):\n",
    "        action, _states = learner.predict(obs, deterministic=True)\n",
    "        obs, reward, _ ,done, info = env.step(action)\n",
    "        if done:\n",
    "            obs, info = env.reset()\n",
    "            \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "439158cd89905785fcc749928062ade7bfccc3f087fab145e5671f895c635937"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
