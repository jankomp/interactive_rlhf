{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[download this notebook here](https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/5_train_preference_comparisons.ipynb)\n",
    "# Learning a Reward Function using Preference Comparisons\n",
    "\n",
    "The preference comparisons algorithm learns a reward function by comparing trajectory segments to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up the preference comparisons algorithm, we first need to set up a lot of its internals beforehand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from imitation.algorithms import preference_comparisons\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.util.util import make_vec_env\n",
    "from imitation.policies.base import FeedForward32Policy, NormalizeFeaturesExtractor\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "venv = make_vec_env(\"Reacher-v4\", rng=rng)\n",
    "\n",
    "reward_net = BasicRewardNet(\n",
    "    venv.observation_space, venv.action_space, normalize_input_layer=RunningNorm\n",
    ")\n",
    "\n",
    "fragmenter = preference_comparisons.RandomFragmenter(\n",
    "    warning_threshold=0,\n",
    "    rng=rng,\n",
    ")\n",
    "gatherer = preference_comparisons.SyntheticGatherer(rng=rng)\n",
    "preference_model = preference_comparisons.PreferenceModel(reward_net)\n",
    "reward_trainer = preference_comparisons.BasicRewardTrainer(\n",
    "    preference_model=preference_model,\n",
    "    loss=preference_comparisons.CrossEntropyRewardLoss(),\n",
    "    epochs=3,\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "\n",
    "# Several hyperparameters (reward_epochs, ppo_clip_range, ppo_ent_coef,\n",
    "# ppo_gae_lambda, ppo_n_epochs, discount_factor, use_sde, sde_sample_freq,\n",
    "# ppo_lr, exploration_frac, num_iterations, initial_comparison_frac,\n",
    "# initial_epoch_multiplier, query_schedule) used in this example have been\n",
    "# approximately fine-tuned to reach a reasonable level of performance.\n",
    "agent = PPO(\n",
    "    policy=FeedForward32Policy,\n",
    "    policy_kwargs=dict(\n",
    "        features_extractor_class=NormalizeFeaturesExtractor,\n",
    "        features_extractor_kwargs=dict(normalize_class=RunningNorm),\n",
    "    ),\n",
    "    env=venv,\n",
    "    seed=0,\n",
    "    n_steps=2048 // venv.num_envs,\n",
    "    batch_size=64,\n",
    "    ent_coef=0.01,\n",
    "    learning_rate=2e-3,\n",
    "    clip_range=0.1,\n",
    "    gae_lambda=0.95,\n",
    "    gamma=0.97,\n",
    "    n_epochs=10,\n",
    ")\n",
    "\n",
    "trajectory_generator = preference_comparisons.AgentTrainer(\n",
    "    algorithm=agent,\n",
    "    reward_fn=reward_net,\n",
    "    venv=venv,\n",
    "    exploration_frac=0.05,\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "pref_comparisons = preference_comparisons.PreferenceComparisons(\n",
    "    trajectory_generator,\n",
    "    reward_net,\n",
    "    num_iterations=5,  # Set to 60 for better performance\n",
    "    fragmenter=fragmenter,\n",
    "    preference_gatherer=gatherer,\n",
    "    reward_trainer=reward_trainer,\n",
    "    fragment_length=50,\n",
    "    transition_oversampling=1,\n",
    "    initial_comparison_frac=0.1,\n",
    "    allow_variable_horizon=False,\n",
    "    initial_epoch_multiplier=4,\n",
    "    query_schedule=\"hyperbolic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can start training the reward model. Note that we need to specify the total timesteps that the agent should be trained and how many fragment comparisons should be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query schedule: [50, 127, 102, 85, 73, 63]\n",
      "Collecting 100 fragments (5000 transitions)\n",
      "Requested 4750 transitions but only 0 in buffer. Sampling 4750 additional transitions.\n",
      "Sampling 250 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 50 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a2c89bf7e1546d4835ebf3e486b245b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "---------------------------------------------------\n",
      "| raw/                                 |          |\n",
      "|    agent/rollout/ep_len_mean         | 50       |\n",
      "|    agent/rollout/ep_rew_mean         | -61.4    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -13.1    |\n",
      "|    agent/time/fps                    | 4965     |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 2048     |\n",
      "---------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -62          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -10.8        |\n",
      "|    agent/time/fps                    | 2541         |\n",
      "|    agent/time/iterations             | 2            |\n",
      "|    agent/time/time_elapsed           | 1            |\n",
      "|    agent/time/total_timesteps        | 4096         |\n",
      "|    agent/train/approx_kl             | 0.0032633715 |\n",
      "|    agent/train/clip_fraction         | 0.178        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.85        |\n",
      "|    agent/train/explained_variance    | -0.0976      |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.0966       |\n",
      "|    agent/train/n_updates             | 10           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00492     |\n",
      "|    agent/train/std                   | 1.01         |\n",
      "|    agent/train/value_loss            | 0.418        |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -62.2        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -8.99        |\n",
      "|    agent/time/fps                    | 2197         |\n",
      "|    agent/time/iterations             | 3            |\n",
      "|    agent/time/time_elapsed           | 2            |\n",
      "|    agent/time/total_timesteps        | 6144         |\n",
      "|    agent/train/approx_kl             | 0.0027890848 |\n",
      "|    agent/train/clip_fraction         | 0.137        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.86        |\n",
      "|    agent/train/explained_variance    | 0.127        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.00775     |\n",
      "|    agent/train/n_updates             | 20           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00464     |\n",
      "|    agent/train/std                   | 1.01         |\n",
      "|    agent/train/value_loss            | 0.186        |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -62.4        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.65        |\n",
      "|    agent/time/fps                    | 2031         |\n",
      "|    agent/time/iterations             | 4            |\n",
      "|    agent/time/time_elapsed           | 4            |\n",
      "|    agent/time/total_timesteps        | 8192         |\n",
      "|    agent/train/approx_kl             | 0.0049793813 |\n",
      "|    agent/train/clip_fraction         | 0.204        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.87        |\n",
      "|    agent/train/explained_variance    | 0.553        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.0031       |\n",
      "|    agent/train/n_updates             | 30           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00942     |\n",
      "|    agent/train/std                   | 1.02         |\n",
      "|    agent/train/value_loss            | 0.1          |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61.8        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.61        |\n",
      "|    agent/time/fps                    | 1977         |\n",
      "|    agent/time/iterations             | 5            |\n",
      "|    agent/time/time_elapsed           | 5            |\n",
      "|    agent/time/total_timesteps        | 10240        |\n",
      "|    agent/train/approx_kl             | 0.0049889185 |\n",
      "|    agent/train/clip_fraction         | 0.213        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.86        |\n",
      "|    agent/train/explained_variance    | 0.693        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0256      |\n",
      "|    agent/train/n_updates             | 40           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00828     |\n",
      "|    agent/train/std                   | 1.01         |\n",
      "|    agent/train/value_loss            | 0.053        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                   |          |\n",
      "|    agent/rollout/ep_len_mean            | 50       |\n",
      "|    agent/rollout/ep_rew_mean            | -62      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean    | -9.63    |\n",
      "|    agent/time/fps                       | 2.74e+03 |\n",
      "|    agent/time/iterations                | 3        |\n",
      "|    agent/time/time_elapsed              | 2.4      |\n",
      "|    agent/time/total_timesteps           | 6.14e+03 |\n",
      "|    agent/train/approx_kl                | 0.00412  |\n",
      "|    agent/train/clip_fraction            | 0.192    |\n",
      "|    agent/train/clip_range               | 0.1      |\n",
      "|    agent/train/entropy_loss             | -2.86    |\n",
      "|    agent/train/explained_variance       | 0.392    |\n",
      "|    agent/train/learning_rate            | 0.002    |\n",
      "|    agent/train/loss                     | 0.00771  |\n",
      "|    agent/train/n_updates                | 30       |\n",
      "|    agent/train/policy_gradient_loss     | -0.00764 |\n",
      "|    agent/train/std                      | 1.01     |\n",
      "|    agent/train/value_loss               | 0.162    |\n",
      "|    preferences/entropy                  | 0.19     |\n",
      "|    reward/epoch-0/train/accuracy        | 0.655    |\n",
      "|    reward/epoch-0/train/gt_reward_loss  | 0.255    |\n",
      "|    reward/epoch-0/train/loss            | 0.755    |\n",
      "|    reward/epoch-1/train/accuracy        | 0.712    |\n",
      "|    reward/epoch-1/train/gt_reward_loss  | 0.335    |\n",
      "|    reward/epoch-1/train/loss            | 0.799    |\n",
      "|    reward/epoch-10/train/accuracy       | 0.799    |\n",
      "|    reward/epoch-10/train/gt_reward_loss | 0.294    |\n",
      "|    reward/epoch-10/train/loss           | 0.532    |\n",
      "|    reward/epoch-11/train/accuracy       | 0.847    |\n",
      "|    reward/epoch-11/train/gt_reward_loss | 0.23     |\n",
      "|    reward/epoch-11/train/loss           | 0.459    |\n",
      "|    reward/epoch-2/train/accuracy        | 0.76     |\n",
      "|    reward/epoch-2/train/gt_reward_loss  | 0.244    |\n",
      "|    reward/epoch-2/train/loss            | 0.645    |\n",
      "|    reward/epoch-3/train/accuracy        | 0.717    |\n",
      "|    reward/epoch-3/train/gt_reward_loss  | 0.234    |\n",
      "|    reward/epoch-3/train/loss            | 0.607    |\n",
      "|    reward/epoch-4/train/accuracy        | 0.689    |\n",
      "|    reward/epoch-4/train/gt_reward_loss  | 0.298    |\n",
      "|    reward/epoch-4/train/loss            | 0.596    |\n",
      "|    reward/epoch-5/train/accuracy        | 0.696    |\n",
      "|    reward/epoch-5/train/gt_reward_loss  | 0.332    |\n",
      "|    reward/epoch-5/train/loss            | 0.62     |\n",
      "|    reward/epoch-6/train/accuracy        | 0.741    |\n",
      "|    reward/epoch-6/train/gt_reward_loss  | 0.26     |\n",
      "|    reward/epoch-6/train/loss            | 0.538    |\n",
      "|    reward/epoch-7/train/accuracy        | 0.708    |\n",
      "|    reward/epoch-7/train/gt_reward_loss  | 0.315    |\n",
      "|    reward/epoch-7/train/loss            | 0.542    |\n",
      "|    reward/epoch-8/train/accuracy        | 0.727    |\n",
      "|    reward/epoch-8/train/gt_reward_loss  | 0.303    |\n",
      "|    reward/epoch-8/train/loss            | 0.544    |\n",
      "|    reward/epoch-9/train/accuracy        | 0.807    |\n",
      "|    reward/epoch-9/train/gt_reward_loss  | 0.246    |\n",
      "|    reward/epoch-9/train/loss            | 0.511    |\n",
      "| reward/                                 |          |\n",
      "|    final/train/accuracy                 | 0.847    |\n",
      "|    final/train/gt_reward_loss           | 0.23     |\n",
      "|    final/train/loss                     | 0.459    |\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting 254 fragments (12700 transitions)\n",
      "Requested 12065 transitions but only 10000 in buffer. Sampling 2065 additional transitions.\n",
      "Sampling 635 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 177 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca08c0354abd4baf9dbc493c0e900e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -62.1        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -7.71        |\n",
      "|    agent/time/fps                    | 5205         |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 12288        |\n",
      "|    agent/train/approx_kl             | 0.0045902636 |\n",
      "|    agent/train/clip_fraction         | 0.226        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.84        |\n",
      "|    agent/train/explained_variance    | 0.683        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0277      |\n",
      "|    agent/train/n_updates             | 50           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0109      |\n",
      "|    agent/train/std                   | 1            |\n",
      "|    agent/train/value_loss            | 0.0506       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.8       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -8.3        |\n",
      "|    agent/time/fps                    | 2407        |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 1           |\n",
      "|    agent/time/total_timesteps        | 14336       |\n",
      "|    agent/train/approx_kl             | 0.005556846 |\n",
      "|    agent/train/clip_fraction         | 0.222       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.84       |\n",
      "|    agent/train/explained_variance    | 0.476       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0112     |\n",
      "|    agent/train/n_updates             | 60          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0103     |\n",
      "|    agent/train/std                   | 1           |\n",
      "|    agent/train/value_loss            | 0.0867      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -62.3       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -8.85       |\n",
      "|    agent/time/fps                    | 1960        |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 3           |\n",
      "|    agent/time/total_timesteps        | 16384       |\n",
      "|    agent/train/approx_kl             | 0.006014134 |\n",
      "|    agent/train/clip_fraction         | 0.252       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.85       |\n",
      "|    agent/train/explained_variance    | 0.596       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.00259     |\n",
      "|    agent/train/n_updates             | 70          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0113     |\n",
      "|    agent/train/std                   | 1           |\n",
      "|    agent/train/value_loss            | 0.068       |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -62.1        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -9.11        |\n",
      "|    agent/time/fps                    | 1827         |\n",
      "|    agent/time/iterations             | 4            |\n",
      "|    agent/time/time_elapsed           | 4            |\n",
      "|    agent/time/total_timesteps        | 18432        |\n",
      "|    agent/train/approx_kl             | 0.0064067477 |\n",
      "|    agent/train/clip_fraction         | 0.253        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.85        |\n",
      "|    agent/train/explained_variance    | 0.74         |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0153      |\n",
      "|    agent/train/n_updates             | 80           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00896     |\n",
      "|    agent/train/std                   | 1            |\n",
      "|    agent/train/value_loss            | 0.0394       |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61.5        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -8.88        |\n",
      "|    agent/time/fps                    | 1755         |\n",
      "|    agent/time/iterations             | 5            |\n",
      "|    agent/time/time_elapsed           | 5            |\n",
      "|    agent/time/total_timesteps        | 20480        |\n",
      "|    agent/train/approx_kl             | 0.0073244306 |\n",
      "|    agent/train/clip_fraction         | 0.282        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.84        |\n",
      "|    agent/train/explained_variance    | 0.761        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.02        |\n",
      "|    agent/train/n_updates             | 90           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0138      |\n",
      "|    agent/train/std                   | 1            |\n",
      "|    agent/train/value_loss            | 0.0348       |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -62      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -8.57    |\n",
      "|    agent/time/fps                      | 2.63e+03 |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 2.6      |\n",
      "|    agent/time/total_timesteps          | 1.64e+04 |\n",
      "|    agent/train/approx_kl               | 0.00645  |\n",
      "|    agent/train/clip_fraction           | 0.26     |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.84    |\n",
      "|    agent/train/explained_variance      | 0.681    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0165  |\n",
      "|    agent/train/n_updates               | 80       |\n",
      "|    agent/train/policy_gradient_loss    | -0.0118  |\n",
      "|    agent/train/std                     | 1        |\n",
      "|    agent/train/value_loss              | 0.0514   |\n",
      "|    preferences/entropy                 | 0.163    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.614    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.244    |\n",
      "|    reward/epoch-0/train/loss           | 0.66     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.669    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.246    |\n",
      "|    reward/epoch-1/train/loss           | 0.596    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.723    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.237    |\n",
      "|    reward/epoch-2/train/loss           | 0.579    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.723    |\n",
      "|    final/train/gt_reward_loss          | 0.237    |\n",
      "|    final/train/loss                    | 0.579    |\n",
      "-----------------------------------------------------\n",
      "Collecting 204 fragments (10200 transitions)\n",
      "Sampling 510 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 279 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3817c8b89b6e4781b1e3509e220dcf2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61.6        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -9.1         |\n",
      "|    agent/time/fps                    | 5495         |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 22528        |\n",
      "|    agent/train/approx_kl             | 0.0069297645 |\n",
      "|    agent/train/clip_fraction         | 0.288        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.83        |\n",
      "|    agent/train/explained_variance    | 0.833        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0385      |\n",
      "|    agent/train/n_updates             | 100          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0145      |\n",
      "|    agent/train/std                   | 0.999        |\n",
      "|    agent/train/value_loss            | 0.028        |\n",
      "-------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -61.3        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -9.48        |\n",
      "|    agent/time/fps                    | 2425         |\n",
      "|    agent/time/iterations             | 2            |\n",
      "|    agent/time/time_elapsed           | 1            |\n",
      "|    agent/time/total_timesteps        | 24576        |\n",
      "|    agent/train/approx_kl             | 0.0074624745 |\n",
      "|    agent/train/clip_fraction         | 0.255        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.83        |\n",
      "|    agent/train/explained_variance    | 0.702        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0444      |\n",
      "|    agent/train/n_updates             | 110          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0105      |\n",
      "|    agent/train/std                   | 0.996        |\n",
      "|    agent/train/value_loss            | 0.0426       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61.5       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -9.96       |\n",
      "|    agent/time/fps                    | 2035        |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 3           |\n",
      "|    agent/time/total_timesteps        | 26624       |\n",
      "|    agent/train/approx_kl             | 0.007163154 |\n",
      "|    agent/train/clip_fraction         | 0.277       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.81       |\n",
      "|    agent/train/explained_variance    | 0.586       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.039      |\n",
      "|    agent/train/n_updates             | 120         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0123     |\n",
      "|    agent/train/std                   | 0.983       |\n",
      "|    agent/train/value_loss            | 0.0429      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 50         |\n",
      "|    agent/rollout/ep_rew_mean         | -61.3      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -9.79      |\n",
      "|    agent/time/fps                    | 1870       |\n",
      "|    agent/time/iterations             | 4          |\n",
      "|    agent/time/time_elapsed           | 4          |\n",
      "|    agent/time/total_timesteps        | 28672      |\n",
      "|    agent/train/approx_kl             | 0.00939008 |\n",
      "|    agent/train/clip_fraction         | 0.316      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -2.77      |\n",
      "|    agent/train/explained_variance    | 0.694      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | -0.0154    |\n",
      "|    agent/train/n_updates             | 130        |\n",
      "|    agent/train/policy_gradient_loss  | -0.0136    |\n",
      "|    agent/train/std                   | 0.962      |\n",
      "|    agent/train/value_loss            | 0.05       |\n",
      "-----------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -61         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -9.63       |\n",
      "|    agent/time/fps                    | 1793        |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 5           |\n",
      "|    agent/time/total_timesteps        | 30720       |\n",
      "|    agent/train/approx_kl             | 0.009167446 |\n",
      "|    agent/train/clip_fraction         | 0.343       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.74       |\n",
      "|    agent/train/explained_variance    | 0.677       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.035      |\n",
      "|    agent/train/n_updates             | 140         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0156     |\n",
      "|    agent/train/std                   | 0.953       |\n",
      "|    agent/train/value_loss            | 0.0314      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -61.3    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -9.59    |\n",
      "|    agent/time/fps                      | 2.72e+03 |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 2.6      |\n",
      "|    agent/time/total_timesteps          | 2.66e+04 |\n",
      "|    agent/train/approx_kl               | 0.00863  |\n",
      "|    agent/train/clip_fraction           | 0.301    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.77    |\n",
      "|    agent/train/explained_variance      | 0.697    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0333  |\n",
      "|    agent/train/n_updates               | 130      |\n",
      "|    agent/train/policy_gradient_loss    | -0.0126  |\n",
      "|    agent/train/std                     | 0.965    |\n",
      "|    agent/train/value_loss              | 0.0379   |\n",
      "|    preferences/entropy                 | 0.169    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.69     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.203    |\n",
      "|    reward/epoch-0/train/loss           | 0.581    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.731    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.201    |\n",
      "|    reward/epoch-1/train/loss           | 0.529    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.724    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.199    |\n",
      "|    reward/epoch-2/train/loss           | 0.504    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.724    |\n",
      "|    final/train/gt_reward_loss          | 0.199    |\n",
      "|    final/train/loss                    | 0.504    |\n",
      "-----------------------------------------------------\n",
      "Collecting 170 fragments (8500 transitions)\n",
      "Sampling 425 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 364 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b1f7ca512c1443fbf67039f62ee427d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -60.5       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -9.81       |\n",
      "|    agent/time/fps                    | 5499        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 32768       |\n",
      "|    agent/train/approx_kl             | 0.009955309 |\n",
      "|    agent/train/clip_fraction         | 0.314       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.71       |\n",
      "|    agent/train/explained_variance    | 0.828       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0329     |\n",
      "|    agent/train/n_updates             | 150         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0109     |\n",
      "|    agent/train/std                   | 0.933       |\n",
      "|    agent/train/value_loss            | 0.0228      |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -60.6        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -10.4        |\n",
      "|    agent/time/fps                    | 2430         |\n",
      "|    agent/time/iterations             | 2            |\n",
      "|    agent/time/time_elapsed           | 1            |\n",
      "|    agent/time/total_timesteps        | 34816        |\n",
      "|    agent/train/approx_kl             | 0.0067743976 |\n",
      "|    agent/train/clip_fraction         | 0.286        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.67        |\n",
      "|    agent/train/explained_variance    | 0.731        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0209      |\n",
      "|    agent/train/n_updates             | 160          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00709     |\n",
      "|    agent/train/std                   | 0.92         |\n",
      "|    agent/train/value_loss            | 0.0394       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -59.4       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -11.2       |\n",
      "|    agent/time/fps                    | 2032        |\n",
      "|    agent/time/iterations             | 3           |\n",
      "|    agent/time/time_elapsed           | 3           |\n",
      "|    agent/time/total_timesteps        | 36864       |\n",
      "|    agent/train/approx_kl             | 0.010640867 |\n",
      "|    agent/train/clip_fraction         | 0.327       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.63       |\n",
      "|    agent/train/explained_variance    | 0.834       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0363     |\n",
      "|    agent/train/n_updates             | 170         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0117     |\n",
      "|    agent/train/std                   | 0.899       |\n",
      "|    agent/train/value_loss            | 0.0243      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -58.1       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -10.9       |\n",
      "|    agent/time/fps                    | 1879        |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 4           |\n",
      "|    agent/time/total_timesteps        | 38912       |\n",
      "|    agent/train/approx_kl             | 0.008434538 |\n",
      "|    agent/train/clip_fraction         | 0.292       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.58       |\n",
      "|    agent/train/explained_variance    | 0.849       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0514     |\n",
      "|    agent/train/n_updates             | 180         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00766    |\n",
      "|    agent/train/std                   | 0.874       |\n",
      "|    agent/train/value_loss            | 0.0166      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -56.9       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -10.7       |\n",
      "|    agent/time/fps                    | 1797        |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 5           |\n",
      "|    agent/time/total_timesteps        | 40960       |\n",
      "|    agent/train/approx_kl             | 0.008746661 |\n",
      "|    agent/train/clip_fraction         | 0.334       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.53       |\n",
      "|    agent/train/explained_variance    | 0.94        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0458     |\n",
      "|    agent/train/n_updates             | 190         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0119     |\n",
      "|    agent/train/std                   | 0.858       |\n",
      "|    agent/train/value_loss            | 0.0119      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -59.1    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -10.6    |\n",
      "|    agent/time/fps                      | 2.73e+03 |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 2.6      |\n",
      "|    agent/time/total_timesteps          | 3.69e+04 |\n",
      "|    agent/train/approx_kl               | 0.00937  |\n",
      "|    agent/train/clip_fraction           | 0.317    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.58    |\n",
      "|    agent/train/explained_variance      | 0.851    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0383  |\n",
      "|    agent/train/n_updates               | 180      |\n",
      "|    agent/train/policy_gradient_loss    | -0.00996 |\n",
      "|    agent/train/std                     | 0.877    |\n",
      "|    agent/train/value_loss              | 0.0205   |\n",
      "|    preferences/entropy                 | 0.126    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.713    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.181    |\n",
      "|    reward/epoch-0/train/loss           | 0.527    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.747    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.204    |\n",
      "|    reward/epoch-1/train/loss           | 0.504    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.748    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.184    |\n",
      "|    reward/epoch-2/train/loss           | 0.493    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.748    |\n",
      "|    final/train/gt_reward_loss          | 0.184    |\n",
      "|    final/train/loss                    | 0.493    |\n",
      "-----------------------------------------------------\n",
      "Collecting 146 fragments (7300 transitions)\n",
      "Sampling 365 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 437 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36e5193ae22440bd99d5e6cd8a20a4c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -56.9        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -11.1        |\n",
      "|    agent/time/fps                    | 5431         |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 43008        |\n",
      "|    agent/train/approx_kl             | 0.0122620035 |\n",
      "|    agent/train/clip_fraction         | 0.347        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.48        |\n",
      "|    agent/train/explained_variance    | 0.899        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0369      |\n",
      "|    agent/train/n_updates             | 200          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0114      |\n",
      "|    agent/train/std                   | 0.835        |\n",
      "|    agent/train/value_loss            | 0.0102       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -57.1       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -11.5       |\n",
      "|    agent/time/fps                    | 2418        |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 1           |\n",
      "|    agent/time/total_timesteps        | 45056       |\n",
      "|    agent/train/approx_kl             | 0.009083658 |\n",
      "|    agent/train/clip_fraction         | 0.302       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.45       |\n",
      "|    agent/train/explained_variance    | 0.794       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0297     |\n",
      "|    agent/train/n_updates             | 210         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0103     |\n",
      "|    agent/train/std                   | 0.823       |\n",
      "|    agent/train/value_loss            | 0.0246      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 50         |\n",
      "|    agent/rollout/ep_rew_mean         | -56.1      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -11.9      |\n",
      "|    agent/time/fps                    | 2022       |\n",
      "|    agent/time/iterations             | 3          |\n",
      "|    agent/time/time_elapsed           | 3          |\n",
      "|    agent/time/total_timesteps        | 47104      |\n",
      "|    agent/train/approx_kl             | 0.01056332 |\n",
      "|    agent/train/clip_fraction         | 0.341      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -2.38      |\n",
      "|    agent/train/explained_variance    | 0.83       |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | -0.0315    |\n",
      "|    agent/train/n_updates             | 220        |\n",
      "|    agent/train/policy_gradient_loss  | -0.0134    |\n",
      "|    agent/train/std                   | 0.795      |\n",
      "|    agent/train/value_loss            | 0.0161     |\n",
      "-----------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -54.7       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -11.4       |\n",
      "|    agent/time/fps                    | 1870        |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 4           |\n",
      "|    agent/time/total_timesteps        | 49152       |\n",
      "|    agent/train/approx_kl             | 0.009046659 |\n",
      "|    agent/train/clip_fraction         | 0.35        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.33       |\n",
      "|    agent/train/explained_variance    | 0.884       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0477     |\n",
      "|    agent/train/n_updates             | 230         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0116     |\n",
      "|    agent/train/std                   | 0.777       |\n",
      "|    agent/train/value_loss            | 0.0132      |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -54.2        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -11.3        |\n",
      "|    agent/time/fps                    | 1790         |\n",
      "|    agent/time/iterations             | 5            |\n",
      "|    agent/time/time_elapsed           | 5            |\n",
      "|    agent/time/total_timesteps        | 51200        |\n",
      "|    agent/train/approx_kl             | 0.0113811735 |\n",
      "|    agent/train/clip_fraction         | 0.356        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.29        |\n",
      "|    agent/train/explained_variance    | 0.919        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.00789     |\n",
      "|    agent/train/n_updates             | 240          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0128      |\n",
      "|    agent/train/std                   | 0.756        |\n",
      "|    agent/train/value_loss            | 0.0122       |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -55.8    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -11.4    |\n",
      "|    agent/time/fps                      | 2.71e+03 |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 2.6      |\n",
      "|    agent/time/total_timesteps          | 4.71e+04 |\n",
      "|    agent/train/approx_kl               | 0.0101   |\n",
      "|    agent/train/clip_fraction           | 0.345    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.33    |\n",
      "|    agent/train/explained_variance      | 0.862    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0283  |\n",
      "|    agent/train/n_updates               | 230      |\n",
      "|    agent/train/policy_gradient_loss    | -0.012   |\n",
      "|    agent/train/std                     | 0.775    |\n",
      "|    agent/train/value_loss              | 0.0153   |\n",
      "|    preferences/entropy                 | 0.159    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.746    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.188    |\n",
      "|    reward/epoch-0/train/loss           | 0.499    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.762    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.185    |\n",
      "|    reward/epoch-1/train/loss           | 0.482    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.781    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.189    |\n",
      "|    reward/epoch-2/train/loss           | 0.464    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.781    |\n",
      "|    final/train/gt_reward_loss          | 0.189    |\n",
      "|    final/train/loss                    | 0.464    |\n",
      "-----------------------------------------------------\n",
      "Collecting 126 fragments (6300 transitions)\n",
      "Sampling 315 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 500 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d849f93e2edc4d1bb3843a2fd08bb76f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 10000 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -53.2       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -11.8       |\n",
      "|    agent/time/fps                    | 5251        |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 53248       |\n",
      "|    agent/train/approx_kl             | 0.010630243 |\n",
      "|    agent/train/clip_fraction         | 0.377       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.21       |\n",
      "|    agent/train/explained_variance    | 0.882       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0249     |\n",
      "|    agent/train/n_updates             | 250         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0122     |\n",
      "|    agent/train/std                   | 0.727       |\n",
      "|    agent/train/value_loss            | 0.0104      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -51.9       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -12.5       |\n",
      "|    agent/time/fps                    | 2347        |\n",
      "|    agent/time/iterations             | 2           |\n",
      "|    agent/time/time_elapsed           | 1           |\n",
      "|    agent/time/total_timesteps        | 55296       |\n",
      "|    agent/train/approx_kl             | 0.011239842 |\n",
      "|    agent/train/clip_fraction         | 0.335       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.15       |\n",
      "|    agent/train/explained_variance    | 0.709       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0186     |\n",
      "|    agent/train/n_updates             | 260         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00859    |\n",
      "|    agent/train/std                   | 0.709       |\n",
      "|    agent/train/value_loss            | 0.0313      |\n",
      "------------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 50           |\n",
      "|    agent/rollout/ep_rew_mean         | -50          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -13.2        |\n",
      "|    agent/time/fps                    | 1867         |\n",
      "|    agent/time/iterations             | 3            |\n",
      "|    agent/time/time_elapsed           | 3            |\n",
      "|    agent/time/total_timesteps        | 57344        |\n",
      "|    agent/train/approx_kl             | 0.0127732605 |\n",
      "|    agent/train/clip_fraction         | 0.37         |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -2.08        |\n",
      "|    agent/train/explained_variance    | 0.852        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | -0.0456      |\n",
      "|    agent/train/n_updates             | 270          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0145      |\n",
      "|    agent/train/std                   | 0.683        |\n",
      "|    agent/train/value_loss            | 0.0266       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -48.5       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -12.7       |\n",
      "|    agent/time/fps                    | 1752        |\n",
      "|    agent/time/iterations             | 4           |\n",
      "|    agent/time/time_elapsed           | 4           |\n",
      "|    agent/time/total_timesteps        | 59392       |\n",
      "|    agent/train/approx_kl             | 0.014201043 |\n",
      "|    agent/train/clip_fraction         | 0.384       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -2.01       |\n",
      "|    agent/train/explained_variance    | 0.86        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0292     |\n",
      "|    agent/train/n_updates             | 280         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0126     |\n",
      "|    agent/train/std                   | 0.657       |\n",
      "|    agent/train/value_loss            | 0.0185      |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 50          |\n",
      "|    agent/rollout/ep_rew_mean         | -47         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -12.6       |\n",
      "|    agent/time/fps                    | 1688        |\n",
      "|    agent/time/iterations             | 5           |\n",
      "|    agent/time/time_elapsed           | 6           |\n",
      "|    agent/time/total_timesteps        | 61440       |\n",
      "|    agent/train/approx_kl             | 0.009628369 |\n",
      "|    agent/train/clip_fraction         | 0.379       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.93       |\n",
      "|    agent/train/explained_variance    | 0.912       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | -0.0552     |\n",
      "|    agent/train/n_updates             | 290         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0126     |\n",
      "|    agent/train/std                   | 0.635       |\n",
      "|    agent/train/value_loss            | 0.0145      |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 50       |\n",
      "|    agent/rollout/ep_rew_mean           | -50.1    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -12.6    |\n",
      "|    agent/time/fps                      | 2.58e+03 |\n",
      "|    agent/time/iterations               | 3        |\n",
      "|    agent/time/time_elapsed             | 2.8      |\n",
      "|    agent/time/total_timesteps          | 5.73e+04 |\n",
      "|    agent/train/approx_kl               | 0.0119   |\n",
      "|    agent/train/clip_fraction           | 0.372    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -2.01    |\n",
      "|    agent/train/explained_variance      | 0.848    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | -0.0323  |\n",
      "|    agent/train/n_updates               | 280      |\n",
      "|    agent/train/policy_gradient_loss    | -0.0115  |\n",
      "|    agent/train/std                     | 0.658    |\n",
      "|    agent/train/value_loss              | 0.0204   |\n",
      "|    preferences/entropy                 | 0.198    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.773    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.181    |\n",
      "|    reward/epoch-0/train/loss           | 0.477    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.788    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.186    |\n",
      "|    reward/epoch-1/train/loss           | 0.456    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.807    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.181    |\n",
      "|    reward/epoch-2/train/loss           | 0.441    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.807    |\n",
      "|    final/train/gt_reward_loss          | 0.181    |\n",
      "|    final/train/loss                    | 0.441    |\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'reward_loss': 0.4405572656542062, 'reward_accuracy': 0.8070312514901161}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pref_comparisons.train(\n",
    "    total_timesteps=50_000,\n",
    "    total_comparisons=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we trained the reward network using the preference comparisons algorithm, we can wrap our environment with that learned reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.rewards.reward_wrapper import RewardVecEnvWrapper\n",
    "\n",
    "learned_reward_venv = RewardVecEnvWrapper(venv, reward_net.predict_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train an agent that sees only the shaped, learned reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x71616d968c10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner = PPO(\n",
    "    seed=0,\n",
    "    policy=FeedForward32Policy,\n",
    "    policy_kwargs=dict(\n",
    "        features_extractor_class=NormalizeFeaturesExtractor,\n",
    "        features_extractor_kwargs=dict(normalize_class=RunningNorm),\n",
    "    ),\n",
    "    env=learned_reward_venv,\n",
    "    batch_size=64,\n",
    "    ent_coef=0.01,\n",
    "    n_epochs=10,\n",
    "    n_steps=2048 // learned_reward_venv.num_envs,\n",
    "    clip_range=0.1,\n",
    "    gae_lambda=0.95,\n",
    "    gamma=0.97,\n",
    "    learning_rate=2e-3,\n",
    ")\n",
    "learner.learn(100_000)  # Note: set to 100_000 to train a proficient expert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can evaluate it using the original reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: -8 +/- 0\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "n_eval_episodes = 100\n",
    "reward_mean, reward_std = evaluate_policy(learner.policy, venv, n_eval_episodes)\n",
    "reward_stderr = reward_std / np.sqrt(n_eval_episodes)\n",
    "print(f\"Reward: {reward_mean:.0f} +/- {reward_stderr:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('imitation_ppo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/lib/python3.8/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /imitation/docs/tutorials/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-0.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-0.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-1.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-1.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-2.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-2.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-2.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-3.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-3.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-3.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-4.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-4.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-4.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-5.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-5.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-5.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-6.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-6.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-6.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-7.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-7.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-7.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-8.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-8.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-8.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-9.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-9.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-9.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-10.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-10.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-10.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-11.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-11.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-11.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-12.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-12.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-12.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-13.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-13.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-13.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-14.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-14.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-14.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-15.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-15.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-15.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-16.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-16.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-16.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-17.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-17.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-17.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-18.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-18.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-18.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-19.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-19.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-19.mp4\n",
      "Moviepy - Building video /imitation/docs/tutorials/videos/training-episode-20.mp4.\n",
      "Moviepy - Writing video /imitation/docs/tutorials/videos/training-episode-20.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /imitation/docs/tutorials/videos/training-episode-20.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"Reacher-v4\", render_mode='rgb_array')\n",
    "env = RecordVideo(env, './videos', name_prefix=\"training\", episode_trigger=lambda x: x % 1 == 0) \n",
    "\n",
    "# Run the model in the environment\n",
    "obs, info = env.reset()\n",
    "for _ in range(1000):\n",
    "        action, _states = learner.predict(obs, deterministic=True)\n",
    "        obs, reward, _ ,done, info = env.step(action)\n",
    "        if done:\n",
    "            obs, info = env.reset()\n",
    "            \n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "439158cd89905785fcc749928062ade7bfccc3f087fab145e5671f895c635937"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
